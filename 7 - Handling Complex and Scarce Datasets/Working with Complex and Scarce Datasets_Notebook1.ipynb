{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Complex and Scrace Datasets [Notebook 1]\n",
    "\n",
    "## Introduction\n",
    "\n",
    "When it comes to deep learning applications and its development, it is certain that Data is the most essential component of the process. By and of itself, the training data should flow into the networks unobstructed. It should contain meaningful and impactful information for the network/model. Prior to entering the network, the dataset should be prepared by various transformations and such. \n",
    "\n",
    "Today, datasets can be obtained from several complex structures or that are stored on heterogenous devices, this inherently also increases the complexity in its handling as well. While this assumes that the data is readily avaible, on the other hand, there cases where the relevant training data (images or annotations) can be unavailable or scarce. \n",
    "\n",
    "This project will venture into dealing with these kinds of cases where it also explores the framework that is available with TensorFlow to set up optimised data pipelines (tf.data API).\n",
    "\n",
    "\n",
    "## Breakdown of this Project:\n",
    "- Building efficient input pipelines with \"tf.data\" for extracting and processing data samples of all kinds. (Notebook 1 & 2)\n",
    "- Augment and render images to help compensate for scarcity of training data. (Notebook 3 & 4)\n",
    "- Different types of Doamin Adaptation methods and how it helps to train more robust models. (Notebook 5 & 6)\n",
    "- Create or generate novel images with generative models such as Variational AutoEncoders (VAEs) and Generative Adversarial Networks (GANs). (Notebook 7 & 8)\n",
    "\n",
    "## Requirements:\n",
    "1. Vispy\n",
    "2. Tensorflow 2.++\n",
    "3. Numpy\n",
    "4. OS\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "The dataset can be obtain from the link: https://www.cityscapes-dataset.com/dataset-overview/.\n",
    "\n",
    "Quoted from the website: \"The Cityscapes Dataset focuses on semantic understanding of urban street scenes.\" It consists of >5,000 images with fine-grained semantic labels, 20,000 images with coarser annotations that were shot from the view point of driving a car around different cities in Germany.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Run on GPU:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" \n",
    "\n",
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42\n",
    "\n",
    "# Set to run or not run the code block: for code examples only. (0 = run code, and 1 = dont run code)\n",
    "dont_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - TensorFlow Data Pipelines and its Structure: \n",
    "\n",
    "__Extract, Transform, Load (ETL)__ is an existing paradigm for data processing in general, where for computer vision tasks, the __ETL Pipelines__ are used to process the raw data into training data before being fed into the models/networks. \n",
    "\n",
    "##### The following diagram below demonstrates the overall ETL process for computer vision tasks:\n",
    "\n",
    "<img src=\"Description Images/ETL_ComputerVision.png\" width=\"750\">\n",
    "\n",
    "Image Ref -> self-made.\n",
    "\n",
    "In more detail, each stage can be described as follows:\n",
    "- For __Extract__: This stage consists of selecting the desired data sources and proceeding to extract its contents. Sources can be varied such as CSV files with filenames for the images, images already in a folder and so on. Note that sources can also be stored on different types of devices like on a local or remote machine/storage device. Part of the extractor's task is to list these sources for each of the originating content/data. \n",
    "\n",
    "- For __Transform__: Once the data have been fetched from their respective sources, the next stage would be to transform the data. These transformations can include parsing of the extracted data into a common format for the task. Example: Parsing bytes of images (JPEG or PNG) into a matrix representation (tensors). Additional transformations can also be applied such as cropping/scaling of the images or augmentations with some operations. Similarly these can be performed to the annotations for supervised learning. the final transformation format would usually be tensors (if not only in tensors) after parsing so that it can be compatible with computation of the loss function for model training.\n",
    "\n",
    "- For __Load__: After the previous stage, the Data would then be \"loaded\" into the target structure. In terms of machine learning, this would be in batch samples where it is sent to the device that will run the model (like GPUs). After this process, the dataset can be cached or saved.\n",
    "\n",
    "#### API Information:\n",
    "\n",
    "Link: https://www.tensorflow.org/guide/data and https://www.tensorflow.org/api_docs/python/tf/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Description of the API Methods:\n",
    "\n",
    "This section will go through some of the methods that are frequently used for the ETL process.\n",
    "\n",
    "### 2.1 - Extract: from tensors, text files, TFRecord files and so on.\n",
    "\n",
    "The __1st Stage__ of the ETL process is Extract the files needed and process them.\n",
    "\n",
    "#### 2.1.1 - From NumPy and TensorFlow data:\n",
    "\n",
    "With the dataset source already in the format of either NumPy or TensorFlow, these can be directly passed into \"tf.data\". The methods that can be used are:\n",
    "- tf.data.Dataset.from_tensor()\n",
    "- tf.data.Dataset.from_tensor_slices()\n",
    "\n",
    "#### 2.1.2 - From Files:\n",
    "\n",
    "When the files of interest to the task resides within folders, these files can be read by firstly listing them with \".list_files()\" which also allows makes an iterable object. The individual files can the be opened with:\n",
    "- tf.io.read_file()\n",
    "\n",
    "Additionally, the API also accounts for binary or text files, CSV files, or image/label information in text files (from public datasets). The following can be used to iterate and read them:\n",
    "- tf.data.TextLineDataset()\n",
    "- tf.data.experimental.CsvDataset()\n",
    "\n",
    "#### 2.1.3 - From other Input Sources: (generators, SQL Database, range and so on)\n",
    "\n",
    "Note that \"tf.data.Dataset\" is quite the comprehensive API package, where it also account for a range of input sources. Where for example, it can be used to iterate over numbers with:\n",
    "- .range()\n",
    "\n",
    "Or work with Python Generators like:\n",
    "- .from_generator()\n",
    "\n",
    "Another is data that is sotred in a SQL database, where TensorFlow does have experimental tools to interact with them, such as:\n",
    "- tf.data.experimental.SqlDataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Transform the Data Samples: with parsing, augmenting and so on.\n",
    "\n",
    "The __2nd Stage__ of the ETL pipeline is to Transform the files that was extracted from the source. Here, the transformations can be split into two ways:\n",
    "1. Performing the transformation on the data samples individually.\n",
    "2. Performing the transformation on the entire dataset as a whole.\n",
    "\n",
    "The following will describe more on (1).\n",
    "\n",
    "#### 2.2.1 - Transform by Parsing Images and Labels:\n",
    "\n",
    "Similarly to the function \"tf.io.read_file()\" for text files, images can be read and converted to image tensors with:\n",
    "- tf.io.decode_png()\n",
    "- tf.io.decode_jpeg()\n",
    "- tf.io.decode_gif()\n",
    "\n",
    "To deal with the labels for computer vision tasks, if the labels are images for image segmentation or for editting, the above functions can also be directly applied. When the labels are text files, the following can be used to create an iteratble object:\n",
    "- TextLineDataset\n",
    "- FixedLengthRecordDataset\n",
    "\n",
    "Then to parse the lines or records from the text file, the following can be applied:\n",
    "- tf.strings\n",
    "- tf.strings.split(line, sep=',') for CSV files\n",
    "\n",
    "#### 2.2.2 - Transform by Parsing TFRecord Files:\n",
    "\n",
    "A more efficient way of inputting the data into the pipeline would be using the \"TFRecord\" file format, where it will store large number of images together into a binary file and make them directly accessible for read-from-disk operations. This is because the process of iterating through the image files, by the methods mentioned in the above section, inefficient. \n",
    "\n",
    "In more detail, TFRecord files are binary files where it will aggregate the data samples such as labels, images, and metadata. It can be serialised with a \"tf.train.Example\" instance, where it is a dictionaries that names each of the data elements (features). For example:\n",
    "- {'img'; image1, 'label': label_1, ...}\n",
    "\n",
    "Each of these element or feature that a sample contains would be an instance of \"tf.train.Feature\" or of its subclasses. These types of objects will be stored as lists of bytes, floats or integers.\n",
    "To utilise TFRecord files as part of the input pipelines, the record/data can be passed with:\n",
    "- tf.data.TFRecordDataset(filename)\n",
    "\n",
    "More examples can be found in the lin: https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset.\n",
    "\n",
    "#### 2.2.3 - Editing the data samples:\n",
    "\n",
    "To edit the samples like cropping, resizing or one-hot encoding, the \".map()\" function plays a large role in the pipeline. The optional operations suchh as augmentations can be built as a function and be wrap and passed to the \".map()\" function and performed on the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Transform the Datasets: with shuffling, zipping, parllelism and so on.\n",
    "\n",
    "The tf.data API does provide several functions that can transform one dataset into another whereit adpats the structure or it can be used for merging the data sources.\n",
    "\n",
    "#### 2.3.1 - Structuring the Dataset:\n",
    "\n",
    "The API offers operations to perform filtering, shuffling, stacking of the data into batches. The following are a few methods:\n",
    "- .batch(batch_size, ...), and tf.data.experimental.unbatch() is the opposite operation.\n",
    "- if .map() is called after .batch(), then the mapping function will have batched data as the input.\n",
    "- .repeat(count=None) \n",
    "- .shuffle(buffer_size, seed, ...), note that the larger the buffer_size the more processing required.\n",
    "- .filter(predicate), will keep or remove elements based on the Boolean output of the \"predicate\" function.\n",
    "- .take(count), returning the first few \"count\" of the dataset elements.\n",
    "- .skip(count), returning the dataset without the first \"count\" of the elements.\n",
    "\n",
    "More can be found in the API documentation.\n",
    "\n",
    "#### 2.3.2 - Merging the Dataset:\n",
    "\n",
    "The two most common techniques to merge datasets together would be:\n",
    "- .concatenate(dataset), this will concatenate the data samples of the dataset with the current one.\n",
    "- .zip(datasets), this will combine the dataset's elements into tuples.\n",
    "\n",
    "Another method (https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave) is the:\n",
    "- .interleave(map_func, cycle_length, block_length, ...)\n",
    "\n",
    "Where it will apply the \"map_func\" function to the elements and interleaves the results. For example, if there are several text files and the goal is to combine all of their images into a single dataset, then the interleave function can be applied. Example:\n",
    "\n",
    "filenames = ['path_to_file1','path_to_file2', ...] \\\n",
    "d = tf.data.Dataset.from_tensor_slices(filesnames) \\\n",
    "d = d.interleave(lambda f: tf.data.TextLineDataset(f).map(some_parse_function), cycle_length=2, block_length=5)\n",
    "\n",
    "The \"cycle_length\" is the parameter to fix the number of elements to be processed concurrently.\n",
    "The \"block_length\" is the parameter that controls the number of consecutive samples that are returned for every element.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Loading:\n",
    "\n",
    "The __3rd and final stage__ of the ETL process is the Loading stage. The advantage here is that \"tf.data\" operations are registered in TensorFlow operational graph and the data is process to be returned aas Tensor instances. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Demonstrating and preparing Input Pipelines for Tasks:\n",
    "\n",
    "With the introduction of ETL above, this stage will venture into the step by step process. The example here will be the Cityscapes dataset that was used in the semantic and instance segmentation of the previous project.\n",
    "\n",
    "### 3.1 - Setting up the Global Variables and Working Directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PC:\n",
    "# Run once, then comment it, so it does not execute the code again:\n",
    "currentDirectory = os.getcwd()\n",
    "os.environ[\"CITYSCAPES_DATASET\"] = currentDirectory + \"/Dataset/cityscapes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the utility files:\n",
    "from dataset_utilities import (CITYSCAPES_FOLDER,\n",
    "                               CITYSCAPES_LABELS,\n",
    "                               CITYSCAPES_IGNORE_VALUE,\n",
    "                               cityscapes_input_func,\n",
    "                               extract_cityscapes_file_pairs,\n",
    "                               postprocess_to_show_images)\n",
    "\n",
    "from plotting_utilities import plot_images_inGrid\n",
    "\n",
    "# Global Variables:\n",
    "batch_size     = 32\n",
    "nb_epochs      = 90\n",
    "image_size     = [256, 256]\n",
    "nb_channels    = 3\n",
    "nb_classes     = len(CITYSCAPES_LABELS)\n",
    "nb_show        = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Extract the Data (Stage 1):\n",
    "\n",
    "The first step would be to __extract__ the data samples that will later be processed and used as the input to train the model. \n",
    "\n",
    "This will mean parsing or listing the files that contains the images and the corresponding labels for both the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cityscapes - Training Dataset : 2975 images ; 2975 GTs.\n",
      "Cityscapes - Validation Dataset : 500 images ; 500 GTs.\n"
     ]
    }
   ],
   "source": [
    "# For the Training Data:\n",
    "train_input_files , train_gt_files = extract_cityscapes_file_pairs(split='train', \n",
    "                                                                   type='leftImg8bit_blurred')\n",
    "\n",
    "# For the Validation Data:\n",
    "val_input_files, val_gt_files = extract_cityscapes_file_pairs(split='val', \n",
    "                                                              type='leftImg8bit_blurred')\n",
    "\n",
    "# Print out the summary:\n",
    "print(\"Cityscapes - Training Dataset : {} images ; {} GTs.\".format(len(train_input_files), \n",
    "                                                                   len(train_gt_files)))\n",
    "\n",
    "print(\"Cityscapes - Validation Dataset : {} images ; {} GTs.\".format(len(val_input_files), \n",
    "                                                                     len(val_gt_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is to instantiate \"tf.data.Dataset\" objects for the purpose of iterating (pair by pair) over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RepeatDataset shapes: {image: (), label: ()}, types: {image: tf.string, label: tf.string}>\n"
     ]
    }
   ],
   "source": [
    "# Here, taking an example of a single sample:\n",
    "random_img_idx = np.random.choice(a=len(val_input_files))\n",
    "\n",
    "# Select and grab the image files:\n",
    "image_files = tf.constant(value=[val_input_files[random_img_idx]])\n",
    "gt_files = tf.constant(value=[val_gt_files[random_img_idx]])\n",
    "\n",
    "# Create the dataset from slices of the tensor:\n",
    "dataset_file = tf.data.Dataset.from_tensor_slices(tensors= {'image': image_files, \n",
    "                                                            'label': gt_files})\n",
    "\n",
    "# An optional setting, where TensorFlow can keep on iterating over the whole dataset:\n",
    "dataset_file = dataset_file.repeat()\n",
    "\n",
    "print(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-5-6aaf1f276005>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-6aaf1f276005>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"Description Images/.png\" width=\"750\">\n",
    "\n",
    "Image Ref -> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
