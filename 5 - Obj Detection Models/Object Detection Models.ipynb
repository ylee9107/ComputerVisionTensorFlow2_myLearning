{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "For this project, the aim would be to go through the techniques that are used for object detection in a scene or an image. The techniques that are explored here are the __You Only Look Once (YOLO)__ and __Regions with Convolutional Neural Networks (R-CNN)__. \n",
    "\n",
    "The process of detecting ojects in an image or video stream coupled with their bounding boxes is what object detection. Object detection is also called object locatlisation. A bounding box is a small rectangle that surrounds the object in question/interest. Here, the input for the algorithm is usually an image and the output would be a list of bounding boces and the object classes/labels. For each of the bounding boxes, the model should be able to output the corresponding predicted class/label and its confidence that the guess it correct. \n",
    "\n",
    "Object detection in general are widely used in industry. For example, these models can be used in the following:\n",
    "1. Self driving car - for perceiving vehicles and pedestrians.\n",
    "2. Content moderation - to locate forbidden objects in the scene and its respectiv size.\n",
    "3. Healthcare - detecting tumors or dangerous unwanted tissues from radiographs.\n",
    "4. Manufacturing - used in assembly robots of the manufacturing chain to put together or repair products.\n",
    "5. Security - to detect threats, threspasses, or count people.\n",
    "6. Wildlife Conservation - to monitor the population of animals.\n",
    "\n",
    "## Breakdown of this Notebook:\n",
    "- History of the object detection techniques.\n",
    "- The main approaches in object detection.\n",
    "- Implementing the YOLO Architecture for fast object detection task.\n",
    "- Improving upon YOLO with the Faster R-CNN architecture.\n",
    "- Utilising the Faster R-CNN with the TensorFlow Object Detection API.\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "\n",
    "\n",
    "## \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42\n",
    "\n",
    "# Set to run or not run the code block: for code examples only. (0 = run code, and 1 = dont run code)\n",
    "dont_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 5331551965398771464),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 6586313605, 1946315773326376630)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR TENSORFLOW ONLY\n",
    "\n",
    "Automatic Mixed Precision Training in TF. Requires NVIDIA DOCKER of TensorFlow.\n",
    "\n",
    "Sources:\n",
    "- https://developer.nvidia.com/automatic-mixed-precision\n",
    "- https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#framework\n",
    "\n",
    "When enabled, automatic mixed precision will do two things:\n",
    "\n",
    "- Insert the appropriate cast operations into your TensorFlow graph to use float16 execution and storage where appropriate(this enables the use of Tensor Cores along with memory storage and bandwidth savings). \n",
    "- Turn on automatic loss scaling inside the training Optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE CODE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph-based example:\n",
    "# opt = tf.train.AdamOptimizer()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# train_op = opt.miminize(loss)\n",
    "\n",
    "# # Keras-based example:\n",
    "# opt = tf.keras.optimizers.Adam()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# model.compile(loss=loss, optimizer=opt)\n",
    "# model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR KERAS API\n",
    "\n",
    "Source:\n",
    "- https://www.tensorflow.org/guide/keras/mixed_precision\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Set for MIXED PRECISION:\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - History of Object Detection:\n",
    "\n",
    "In the past, classical computer vision techniques for object detection uses image descriptors, this is where to detect an object like a bike would require several images of this object. The term descriptors refers to the bike object that would be extracted from the image and that these descriptors represents different parts of the bike. As the algorithm looks for the object (bike), it will try to find the descriptors in the target images. \n",
    "\n",
    "The most common technique at the time was the usage of the floating window. It is where small rectangular areas of the images were examined one by one, where the part that matches the descriptors the most would be considered to contain the object of interest. This technique have a few advantage at the time, where it was robust to rotation and colour changes in the images, it also does not require a lot training data samples and overall, it worked for most objects. The drawback was that the level of accuracy was not good enough. Soon Neural Networks outpaced this tradiational technique.\n",
    "\n",
    "Modern algorithms can be seen to have better performance, where this refers to the following:\n",
    "1. Bounding Box Precision - it provides the correct bounding box where it is not too large or narrow.\n",
    "2. Recall - it is able to find all the objects.\n",
    "3. Class Prediction - it is able to output the correct class/label for each of the found object.\n",
    "4. Speed of the algorithm - it is where the models are getting faster and faster at computing the results so that it can be used in real time. (real time = 5fps for computer vision tasks.)\n",
    "\n",
    "## 2 - Evaluating the Object Detection model's Performance:\n",
    "\n",
    "Before diving into YOLO's architecture or going further, it is important to cover some basics in model evaluation that are related to the YOLO model. Evaluating an object detection model will require more than accuracy of the prediction against the ground truth, these extra metrics to be included here are called __Precision__ and __Recall__. These will serve as the basis to compute other metrics that are important to object detection. \n",
    "\n",
    "## 2.1 - Precision and Recall Metrics:\n",
    "\n",
    "To begin with, here are the formulas for these metrics:\n",
    "\n",
    "$$ precision = \\frac{TP}{TP - FP} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ recall = \\frac{TP}{TP - FN} $$\n",
    "\n",
    "where here,\n",
    "\n",
    "- TP = Number of True Positives (like how many prediction are matching the ground truth of the same class) \n",
    "- FP = Number of False Positives (like how many prediction that do not match the ground truth for the same class)\n",
    "- FN = Number of False Negatives (like how many ground truths do not have a matching prediction)\n",
    "\n",
    "Additionally, which is not included in the formulas above, but are used in other areas/topics:\n",
    "- TN = Number of True Negatives (like how many ground truths are actualy a negative example)\n",
    "\n",
    "If the predictions are exactly matching the ground truthss, both FP and FN won't exist. In this case following the formulas, both precision and recall would equal to 1, leading to a perfect score. \n",
    "\n",
    "When a model's predictions comes from non-robust features during its training, the precision of the overall model will decrease because there will be a rise in false positives. In contrast, if the model is too strict where it detects only under precise conditions are met, the recall will decrease due to a rise in false negatives.\n",
    "\n",
    "## 2.2 - Precision and Recall Curve:\n",
    "\n",
    "Precision-Recall curves are used to summarise the trade off between the TP rate and the posistive predictive calue for a model by using different probability thresholds. The ideal model should have both high precision and high recall, however, most models often have a trade off between the two. One way to tell how well the model is performing is to compare the area under the curve (AUC) of the chart, where if the area is larger then it performs better or is a better classifier. Below shows a chart that compares two models where the BLUE line model is a better model compared to the GREEN line model. \n",
    "\n",
    "Source: \n",
    "- https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "- https://www.geeksforgeeks.org/precision-recall-curve-ml/\n",
    "\n",
    "<img src=\"Description Images/Precision Recall Curve.png\" width=\"350\">\n",
    "\n",
    "Image Ref -> https://www.geeksforgeeks.org/precision-recall-curve-ml/\n",
    "\n",
    "Since the general idea is to visualise the performance (precision and recall) at each threshold of confidence, where for every output bounding box, the model will also output a confidence (between 0 and 1, where 1 is more confident). In practice it is a good idea to remove less confident predictions and this is done by setting a certain threshold (T = 0.48 or 0.4) so that any predictions below this number will be removed. \n",
    "\n",
    "Changing the value of the threshold will impact the model's precision and recall, where for:\n",
    "1. __If T = close to 1__: The Precision will be high and the Recall will be low. This means that many objects will be filtered out and the model will miss it as only the confident predictions are kept.\n",
    "2. __If T = close to 0__: The Recall will be high and the Precision will be low. This means that most of the predictions will be kept and there will not be any false negatives. This directly contributes to the rise of false positive results where the model is less confident in its predictions. \n",
    "\n",
    "The choice of this threshold is also high reliant on the problem at hand (classification task). For example, for a model to detect pedestrians, the ideal model would have a high recall rate so not to misss any passers-by, even if the car have to stop for no reason. For a model to detect investment opportunities, the ideal model will have a high precision rate to make sure that it doesn't miss good opportunities and avoids the wrong opportunities.\n",
    "\n",
    "## 2.3 - Average Precision and Mean Average Precision:\n",
    "\n",
    "For convenience in practice, often a single number to describe what is happening is very useful. In these cases there are\n",
    "two terms to consider using: \n",
    "\n",
    "1. __Average Precision (AP)__: is the area under th curve and it is always between 0 and 1. This was meantioned earlier. It gives a good indicator of the model performance for a __Single__ class.\n",
    "2. __Mean Average Precision (mAP)__: is the mean of the average precision for __each__ of the classes. It provides an overall __global scor__. \n",
    "\n",
    "## 2.4 - Average Precision Threshold: Jaccard Index (or Intersection over Union, IoU)\n",
    "\n",
    "The __Jaccard Index__ or __Intersection over Union (IoU)__ is the common metric that is used to measure when a prediction and ground truth are matching. This also defines the TP and FP metrics earlier. The following equation defines the IoU a:\n",
    "\n",
    "$$ IoU(A, B) =  \\frac{\\lvert{A \\cap B}\\rvert}{\\lvert{A \\cup B}\\rvert} = \\frac {\\lvert{A \\cap B}\\rvert}{\\lvert{A}\\rvert - \\lvert{B}\\rvert - \\lvert{A \\cap B}\\rvert} $$\n",
    "\n",
    "Where here,\n",
    "- $\\lvert{A}\\rvert$, is the cardinality of set A. The number of elements A contains.\n",
    "- $\\lvert{B}\\rvert$, is the cardinality of set B. The number of elements B contains.\n",
    "- $\\lvert{A \\cap B}\\rvert$, is the numerator for number of elements that are in common between A and B. Here $A \\cap B$ is the intersection between the two sets.\n",
    "- $\\lvert{A \\cup B}\\rvert$, is the demoninator for total number of elements that A and B sets covers. Here $A \\cup B$ is the union of the sets.\n",
    "\n",
    "#### The following diagram shows Precision, Recall and IoU:\n",
    "\n",
    "<img src=\"Description Images/IoU intersection.PNG\" width=\"600\">\n",
    "\n",
    "Image Ref -> http://www.gabormelli.com/RKB/Bounding_Box_Intersection_over_Union_(IoU)_Measure\n",
    "\n",
    "Generally, the IoU are used in the computation rather than just the intersections, because the intersections value is absolute but not relative, meaning that two big boxes have more overlapping pixels than two smaller boxes, so the ratio is used instead as it is relative. IoU will always be between 0 and , where 0 if the two boxes does not overlap and 1 if the boxes overlaps completely.\n",
    "\n",
    "## 3 - YOLO Detection Algorithm: \n",
    "\n",
    "YOLO which stands for You Only Look Once algorithm, is one of the fasted object detection algorithm that are available. One a decent GPU, it will be able to achieve >100 FPS (frames per second) for an image sized at 256 by 256 pixels. YOLO is not only fast but accurate too, it has several iterations over the last few years from 2015 to 2018, where the current version is v3. \n",
    "\n",
    "The main creator for the YOLO paper maintains the network framework (Called Darknet) here: https://github.com/pjreddie/darknet.\n",
    "\n",
    "## 3.1 - Advantages and Disadvantages of YOLO:\n",
    "\n",
    "As mentioned before, the YOLO model is praised for its speed, however, recently this model was outperformed by Fast R-CNN because it performs better than YOLO on detection of smaller objects. Another disadvantage is that the model struggles to classify objects that deviate from the original training dataset. \n",
    "\n",
    "## 3.2 - Main Concepts of YOLO:\n",
    "\n",
    "From their paper (link below), they tackled the problem differently by \"reframing object detection as a single regression problem\". This essentially means that rather than using a sliding window or other techniques, the input image is divided into a width x height grid. This can be seen in the diagram below:\n",
    "\n",
    "<img src=\"Description Images/YOLO Grid.PNG\" width=\"350\">\n",
    "\n",
    "Image Ref -> https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/\n",
    "\n",
    "As it can be seen in each part of the grid, the bounding boxes are defined. This is then followed by predicting the (i) entre of the box, (ii) the width and height of the box, (iii) probability that there is an object within the box, and (iv) lastly the class of the object itself. It is because these predictions are computed as numbers, the problem can be transformed into a regression problem as stated. This covers roughly the basics of the YOLO's concept in detecting objects, there are also more complex issues such as, more than one object in the grid, overlapping objects in several parts of the grid, or how to train the loss for this model? \n",
    "\n",
    "Source: \n",
    "- https://arxiv.org/pdf/1506.02640v5.pdf\n",
    "- https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006\n",
    "\n",
    "For the purpose of simplicity the model will be broken down into two parts, begining with the __Inference__ and then the __Training__ later on.\n",
    "\n",
    "## 3.3 - YOLO Architecture: Part 1 Inference.\n",
    "\n",
    "The YOLO architecture itself is based on a __Backbone model__ like most other CNNs, where the backbone model is the feature extractor that will extract meaningful features from the input images that will be used by the final layers later on. The feature extractor architecture here can be anything, but the official YOLO feature extractor uses its own custom backbone. Changing or altering the feature extractor can lead to different performance numbers. Below is a diagram that shows YOLO's model architecture:\n",
    "\n",
    "<img src=\"Description Images/YOLO Architecture.PNG\" width=\"850\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/how-to-peform-yolo-object-detection-using-keras/\n",
    "\n",
    "The feature extractor's final layer will output a feature volume of size width x height x depth (w x h x D), where the width x height is the size of the grid and D is the depth of the feature volume. The size of the grid (w x h) will depend on the following two factors:\n",
    "\n",
    "- Stride of the complete feature extractor.\n",
    "- Size of the input image (smaller the input image, the smaller the grid).\n",
    "\n",
    "The final layers of the model will take the feature volume as its input. The final layers are composed of 1x1 convolutional filters, which changes the depth of the feature volume without affecting its spatial structure.\n",
    "\n",
    "## 3.4 - Outputs:\n",
    "\n",
    "The final outputs from the model is sized as width x height x M (w x h x M). The (w x h) is the grid size and $M = B * (C + 5)$. Where here, B is the number of boinding boxes per grid cell, C is the number of classes. Notice that there is a number 5 added to the number of classes. This is because, for each of the bounding boxes there is a need to predict (C + 5) of the following numbers:\n",
    "\n",
    "1. $t_{x}$ and $t_{y}$ - is to compute the coordinates of the centre of the bounding box.\n",
    "2. $t_{w}$ and $t_{h}$ - is to compute the width and height of the bounding box.\n",
    "3. c - is the confidence of the object in the box.\n",
    "4. $p1, p2, ..., $ and $pC$ - are  the probability that the bounding box has the object of class 1, 2, ..., C (where C = 20 for the  dataset classes).\n",
    "\n",
    "Below the diagram summarises the output matrix:\n",
    "\n",
    "<img src=\"Description Images/YOLO final matrix output.PNG\" width=\"850\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/deep-learning-bounding-boxes/ and https://www.cnblogs.com/ranjiewen/p/7896398.html\n",
    "\n",
    "## 3.5 - Anchor Boxes:\n",
    "\n",
    "Prior to computing the final bounding boxes as mentioned above, the YOLO architecture also employs the concept of __Anchor Boxes__. This technique was because in version 1 of YOLO, many errors (because the objects varried in sizes) cropped up by directly computing the bounding box coordinates: $t_{x}, t_{y}, t_{w}, t_{h}$. Most of the objects in the training set were to big which resulted in larger bounding boxes, and the model would fail to draw these boxes around smaller objects. YOLO version 2 utilised Anchor Boxes to fix this issue.\n",
    "\n",
    "__Anchor Boxes (a.k.a Priors) are a set of bounding boxes__ that were decided before training the network, and consists of 3 to 25 different sizes. The network will then refine these anchor boxes to find the closest matching ones for the object in the image. For example, in the image below, a set of 5 bounding boxes were picked to detect the car. As these boxes do not exactly match the object, the network will be required to refine the closest matching anchor box. \n",
    "\n",
    "<img src=\"Description Images/YOLO Anchor Box Example 1.PNG\" width=\"350\">\n",
    "\n",
    "Image Ref -> https://www.jeremyjordan.me/object-detection-one-stage/\n",
    "\n",
    "In this example, the network will need to correct the height and a small portion of the width of the anchor box. These are what the coordinates, $t_{x}, t_{y}, t_{w}, t_{h}$, previously mentioned above, they are the __corrections to the anchor box__ on the object. For YOLO version 2, the creators realised that sizes of anchor boxes are different for each dataset, therefore it was recommended that the data is to be analysed first to pick the size of the anchor boxes.\n",
    "\n",
    "## 3.6 - Refining the Anchor Boxes:\n",
    "\n",
    "The refinement process of these anchor boxes in practice are computed with the following equations:\n",
    "\n",
    "$$ b_{x} = sigmoid(t_{x} + c_{x}) $$\n",
    "$$ b_{y} = sigmoid(t_{y} + c_{y}) $$\n",
    "\n",
    "$$ b_{w} = p_{w}exp(t_{w}) $$\n",
    "$$ b_{h} = p_{h}exp(t_{h}) $$\n",
    "\n",
    "Where here,\n",
    "- $t_{x}, t_{y}, t_{w}, t_{h}$ are the outputs from the last layer.\n",
    "- $b_{x}, b_{y}, b_{w}, b_{h}$ are the position and size of the predicted bounding box.\n",
    "- $p_{w}$ and $p_{h}$ are the original size of the anchor box.\n",
    "- $c_{x}$ and $p_{y}$ are the coordinates of the current grid cell, like (0, 0) for the top left box, (w-1, ) for the top right box, and (0, h-1) for the bottom left box.\n",
    "- $exp$ is the exponential function.\n",
    "- $sigmoid$ is the sigmoid function.\n",
    "\n",
    "Below is a diagram to show how these equations are use:\n",
    "\n",
    "<img src=\"Description Images/YOLO Anchor Boxes refinement.PNG\" width= \"400\">\n",
    "\n",
    "Image Ref -> https://arxiv.org/pdf/1612.08242v1.pdf\n",
    "\n",
    "The output matrix of the neural network would need to be transformed into a list of boudning boxes, it also needs to be ran for every inference for the computation of the bounding boxes for an image, and below is a simplified version of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the numbers of an output matrix from Neural Network to a list of bounding boxes:\n",
    "if dont_run == 1:\n",
    "    \n",
    "    boxes = []\n",
    "    for row in range(grid_height):\n",
    "        for col in range(grid_width):\n",
    "            for b in range(nb_box):\n",
    "                tx, ty, tw, th = network_output[row, col, b, :4]\n",
    "                box_confidence = network_output[row, col, b, 4]\n",
    "                class_scores = network_output[row, col, b, 5:]\n",
    "                \n",
    "                # Bounding Boxes\n",
    "                bx = signmoid(tx) + col\n",
    "                by = sigmoid(ty) + row\n",
    "                \n",
    "                # Anchor boxes are a list of dict consisting of the size of each anchor:\n",
    "                bw = anchor_boxes[b]['w'] * np.exp(tw)\n",
    "                bh = anchor_boxes[b]['h'] * np.exp(th)\n",
    "                \n",
    "                boxes.append( (bx, by, bw, bh, box_confidence, class_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 - Post-processing the Bounding Boxes:\n",
    "\n",
    "Before the boxes can be displayed, it will require a futher post-processing. From the above, the list consists of the cordinates, size of the predicted boxes, confidence and the class probabilities. However, right now, the class probabilities also includes predictions that are less confident or wrong, so these will need to be excluded. Utilising these, the confidence and the class probabilities are then multiplied firstly, then a threshold is used to include only those that are of greater probabilities.\n",
    "\n",
    "Below is the code to show this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    \n",
    "    # The confidence is a float variable, Classes are an array of the size equivalent to the nb_classes:\n",
    "    final_scores = box_confidence * class_scores\n",
    "    \n",
    "    # Set the Threshold:\n",
    "    OBJECT_THRESHOLD = 0.3\n",
    "    \n",
    "    # Filter out the lower probabilities and keep the ones that are greater than the threshold:\n",
    "    filter_classes = class_scores >= OBJECT_THRESHOLD\n",
    "    \n",
    "    filtered_scores = class_scores * filter_classes\n",
    "    \n",
    "    # If the filtered scores contains non-null values, it means that at least one class is above the threshold and this is kept:\n",
    "    class_id = np.argmax(filtered_scores)\n",
    "    class_label = CLASS_LABELS[class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the filtering/post-processing to all of the bounding boxes in the grid, all the information can be used to draw the predictions. The following shows the output from this stag:\n",
    "\n",
    "<img src=\"Description Images/YOLO ouput bounding boxes before NMS.png\" width=\"350\">\n",
    "\n",
    "Image Ref -> https://towardsdatascience.com/guide-to-car-detection-using-yolo-48caac8e4ded\n",
    "\n",
    "As it can be seen, there are numerous bounding boxes that are overlapping the objects in the grid. This is an outcome of the object occupying multiple grid boxes and therefore will have multiple bounding boxes overlapping each other. In order to correct this and clean up the image, the last stage of the post-processing pipeline is the __Non-Maximum Supression (NMS)__.\n",
    "\n",
    "## 3.8 - Non-Maximum Supression (NMS):\n",
    "\n",
    "The NMS post-processing procedure is to remove all the other unwanted bounding boxes (with the lowest probability) and keep only the bounding box with the highest probability. This is why it is call __Non-Maximum__ as it removes the boxes with lower probabilities. This can be done by firstly sorting out all the boxes by its probability, then taking the boxes with the highest probability. Next, for each of the box chosen, proceed to compute the __Intersection over Union (IoU)__ with all the other boxes.\n",
    "\n",
    "Once the IoU(s) are computed between a box and the other boxes, the ones with an IoU value that is above a certain threshold (typically, 0.5 to 0.9) will be removed.\n",
    "\n",
    "The following shows an example code for the NMS:\n",
    "\n",
    "NOTE: In practice, TensorFlow has its own implementation of NMS called \"tf.image.non_max_supression(boxes, ...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    \n",
    "    # Sort the bounding boxes:\n",
    "    sorted_boxes = sort_boxes_by_confidence(boxes)\n",
    "    ids_to_suppress = []\n",
    "    \n",
    "    # Compute the IoU:\n",
    "    for maximum_box in sorted_boxes:\n",
    "        for idx, box, in enumerate(boxes):\n",
    "            iou = compute_iou(maximum_box, box)\n",
    "            \n",
    "            if iou > iou_threshold:\n",
    "                ids_to_suppress.append(idx)\n",
    "                \n",
    "    # Delete the unwanted boxes:\n",
    "    processed_boxes = np.delete(boxes, ids_to_suppress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After post-processing with NMS, the object now has only one bounding box surrounding it. This can be seen in the image below:\n",
    "\n",
    "<img src=\"Description Images/YOLO ouput bounding boxes after NMS.png\" width=\"600\">\n",
    "\n",
    "Image Ref -> https://towardsdatascience.com/guide-to-car-detection-using-yolo-48caac8e4ded\n",
    "\n",
    "## 3.9 - Summary of YOLO Inference:\n",
    "\n",
    "Before diving into YOLO training phase, this section will summarise all the steps taken. \n",
    "\n",
    "1. Input image enters the CNN backbone to compute the feature volumes.\n",
    "2. Using the convolutional layer to compute the anchor box corrections, objectness scores, and class probabilities.\n",
    "3. Using the output matrix to compute the coordinates of the bounding boxes.\n",
    "4. Filter out the lower threshold boxes and post-process them with NMS to obtain the highest probability bounding box over the object.\n",
    "\n",
    "It should be noted that this whole process is composed of convolutions and filtering operations throughout, which means that the model can ingest images of any size and ratios, giving the property of flexibility.\n",
    "\n",
    "## 4 - YOLO Architecture: Part 2 YOLO Training.\n",
    "\n",
    "The YOLO model is composed of two parts, where the first is the backbone and the second is the YOLO head. As for the backbone architectures, many types can be used. Typically before training the full YOLO model, the backbone would be trained first, on a classsification task with pretrained weights of ImageNet as this speeds up the computation time. \n",
    "\n",
    "Keras can be employed to train the backbone based on the pretrained wweights of ImageNet, the following shows an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    \n",
    "    # Define the input image shape:\n",
    "    input_image = Input(shape = (IMAGE_H, IMAGE_W, 3))\n",
    "    \n",
    "    # Define the True Box for the objects:\n",
    "    true_boxes = Input(shape = (1, 1, 1, TRUE_BOX_BUFFER, 4))\n",
    "    \n",
    "    # Instantiate the Backbone CNN model: Inception model v.\n",
    "    inception = InceptionV3(input_shape = (IMAGE_H, IMAGE_W, 3),\n",
    "                            weights = 'imagenet',\n",
    "                            include_top = False)\n",
    "    \n",
    "    # Get the output feature volumes from the inception model:\n",
    "    features = inception(input_image)\n",
    "    \n",
    "    # Get the out shape:\n",
    "    GRID_H, GRID_W = inception.get_output_shape_at(-1)[1:3]\n",
    "    \n",
    "    # Get the output with the bounding box:\n",
    "    output = Conv2D(BOX * (4 + 1 + CLASS), \n",
    "                    (1, 1),\n",
    "                    strides = (1, 1),\n",
    "                    padding = 'same',\n",
    "                    name = 'DetectionLayer',\n",
    "                    kernel_initializer = 'lecun_normal')(features)\n",
    "    \n",
    "    # Reshape the output:\n",
    "    output = Reshape( (GRID_H, GRID_W, BOX, 4 + 1 + CLASS) )(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - YOLO Loss:\n",
    "\n",
    "Immediately it can be seen that the output of the last layer is unusual compared to model that were implemented in previous notebooks, therefore the corresponding loss here will also be unusual. Note that the YOLO loss is quite complex and will be required to be broken down into several parts. Each of these corresponding loss outputs will be returned by the last layer.\n",
    "\n",
    "The YOLO network will predict the following kinds of information:\n",
    "- Bounding box coordinates and its size. (__Bounding Box Loss__)\n",
    "- Cofidence that the object of interest is in the bounding box. (__Object Confidence Loss__)\n",
    "- Scores of the classes. (__Classification Loss__)\n",
    "- Full YOLO Loss.\n",
    "\n",
    "In general, the loss computed here should be high when the error is high and the loss will be used to penalise the incorrect values. However, with YOLO, the loss should only penalise when it makes sense to do so, such as if the bounding box contains no objects, there is no need to penalise its coordinates as it won't be utilised anyway. \n",
    "\n",
    "NOTE: As the source paper does not include how the loss is computed, it is better to check other sources of their loss implementation as there are several that works.\n",
    "\n",
    "\n",
    "\n",
    "### 4.1.1 - Boudning Box Loss:\n",
    "\n",
    "This first loss aids the model to learn the weights so that it can be used to predict the bounding box coordinates and its size. Key to this loss equation would be the __indicator function__, where the computed coordinates will only be correct when the box is responsible in detecting an object. The key part of this loss computation is the indicator function, where the coordinates will be correct if the box is responsible for detecting an object. Taking the instance of YOLO v1, it had difficulty in determining the correct bounding box to use, and in YOLO v2, it was resolved by using the highest IoU with the detected object. The goal here is to ensure that each objectof interest will have its own anchor boxes.\n",
    "\n",
    "The following equation shows the loss at this stage:\n",
    "\n",
    "$$ \\lambda_{coord} \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} 1_{ij}^{obj} [(x_{i} -\\hat{x_{i}})^{2}] + \\lambda_{coord} \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} 1_{ij}^{obj} [(\\sqrt{w_{i}} - \\sqrt{\\hat{w_{i}}})^{2} + (\\sqrt{h_{i}} - \\sqrt{\\hat {h_{i}}})^{2}] $$\n",
    "\n",
    "Where here,\n",
    "- $ \\lambda_{coord} $ is the weighting of the loss, it determines how much importance is given to the bounding box coordinates during training.\n",
    "- $ \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} $ is the sum for each part of the grid (from i = 0 to i = $S^{2}$) and for each box in this part of the grid (from 0 to B).\n",
    "- $ 1_{ij}^{obj} $ is the indicator function for objects, it is equal to 1 when the $i^{th}$ part of the grid and the $j^{th}$ bounding box are responsible for an object.\n",
    "- $ x_{i}, y_{i}, w_{i}m h_{i}$ is the bounding box size and coordinates. The difference between the predicted value (hat^) from the ouput of the network and the target value (ground truth). \n",
    "- $ Square Root $ ensures that the values are positive.\n",
    "- By taking the square root of $w_{i}$ and $h_{i}$, it ensures that the errors for small bounding boxes are penalised more heavily compared to larger bounding boxes.\n",
    "\n",
    "### 4.1.2 - Object Confidence Loss:\n",
    "\n",
    "The next loss computation will have the network learn the weights that predicts if the bounding box contains an object.\n",
    "\n",
    "The following equation shows the loss equation:\n",
    "\n",
    "$$ \\lambda_{obj} \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} 1_{ij}^{obj} [(C_{ij} -\\hat{C_{ij}})^{2}] + \\lambda_{noobj} \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} 1_{ij}^{noobj} [(C_{ij} -\\hat{C_{ij}})^{2}] $$\n",
    "\n",
    "Where here,\n",
    "- $C_{ij}$ is the confidence that the box (j) in the part (i) of the grid does contain an object.\n",
    "- $1_{ij}^{noobj}$ is the indicator function for no object, it will equal to 1 when the $i^{th}$ part of the gird and the $j^{th}$ bounding box are not responsible for an object.\n",
    "\n",
    "In order to ensure that the objectness score is not penalised when there are other good candidates that also fits the object of interest, the $1_{ij}^{noobj}$ can be defined as seen below:\n",
    "\n",
    "$$ 1_{ij}^{noobj} == \\{^{1 (box is not responsible for any object) and (box not overlapping too much with any object bounding box)}_{0 (otherwise)}$$\n",
    "\n",
    "Note that in practice, for each of the bounding box (i, j), the IoU w.r.t the ground truth boxes is computed. If the IoU is over the threshold (e.g. 0.6), $1_{ij}^{noobj}$ would be set to 0. This ensures to avoid penalising the boxes that contain objects but are not responsible for the object of interest.\n",
    "\n",
    "### 4.1.3 - Classification Loss:\n",
    "\n",
    "The last part of the loss computation is the classification loss which have the network learning to predict the proper class for each of the bounding box. The following shows the loss equation:\n",
    "\n",
    "$$ \\sum^{S^{2}}_{i=0} 1_{i}^{obj} \\sum_{c \\epsilon classes} (p_{i}(c) - \\hat{p_{i}}(c))^{2} $$\n",
    "\n",
    "NOTE: the original YOLO paper utilised L2 loss while many other implementations were using cross-entropy.\n",
    "\n",
    "### 4.1.4 - Full YOLO Loss:\n",
    "\n",
    "With the above three loss equations stated, the final overall loss is computed by summing the three losses together. This means that the combination computes the loss penalises the error of the bounding box coordinate refinement, objectness scores and the class prediction. By backpropagating this error, the YOLO model can be trained properly to predict the correct bounding boxes for the correct objects.\n",
    "\n",
    "## 5 - Techniques used for Training:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: \n",
    "- https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/\n",
    "- https://manalelaidouni.github.io/manalelaidouni.github.io/Understanding%20YOLO%20and%20YOLOv2.html\n",
    "- https://appsilon.com/object-detection-yolo-algorithm/\n",
    "- http://datahacker.rs/how-to-peform-yolo-object-detection-using-keras/\n",
    "- http://datahacker.rs/tensorflow2-0-yolov3/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Description Images/YOLO Bounding Boxes.PNG\" width=\"350\">\n",
    "\n",
    "Image Ref -> https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
