{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "For this project, the aim would be to go through the techniques that are used for object detection in a scene or an image. The techniques that are explored here are the __You Only Look Once (YOLO)__ and __Regions with Convolutional Neural Networks (R-CNN)__. \n",
    "\n",
    "The process of detecting ojects in an image or video stream coupled with their bounding boxes is what object detection. Object detection is also called object locatlisation. A bounding box is a small rectangle that surrounds the object in question/interest. Here, the input for the algorithm is usually an image and the output would be a list of bounding boces and the object classes/labels. For each of the bounding boxes, the model should be able to output the corresponding predicted class/label and its confidence that the guess it correct. \n",
    "\n",
    "Object detection in general are widely used in industry. For example, these models can be used in the following:\n",
    "1. Self driving car - for perceiving vehicles and pedestrians.\n",
    "2. Content moderation - to locate forbidden objects in the scene and its respectiv size.\n",
    "3. Healthcare - detecting tumors or dangerous unwanted tissues from radiographs.\n",
    "4. Manufacturing - used in assembly robots of the manufacturing chain to put together or repair products.\n",
    "5. Security - to detect threats, threspasses, or count people.\n",
    "6. Wildlife Conservation - to monitor the population of animals.\n",
    "\n",
    "## Breakdown of this Notebook:\n",
    "- History of the object detection techniques.\n",
    "- The main approaches in object detection.\n",
    "- Implementing the YOLO Architecture for fast object detection task.\n",
    "- Improving upon YOLO with the Faster R-CNN architecture.\n",
    "- Utilising the Faster R-CNN with the TensorFlow Object Detection API.\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "\n",
    "\n",
    "## \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42\n",
    "\n",
    "# Set to run or not run the code block: for code examples only. (0 = run code, and 1 = dont run code)\n",
    "dont_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 1474424523630474660),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 6586313605, 13280329909406021941)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR TENSORFLOW ONLY\n",
    "\n",
    "Automatic Mixed Precision Training in TF. Requires NVIDIA DOCKER of TensorFlow.\n",
    "\n",
    "Sources:\n",
    "- https://developer.nvidia.com/automatic-mixed-precision\n",
    "- https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#framework\n",
    "\n",
    "When enabled, automatic mixed precision will do two things:\n",
    "\n",
    "- Insert the appropriate cast operations into your TensorFlow graph to use float16 execution and storage where appropriate(this enables the use of Tensor Cores along with memory storage and bandwidth savings). \n",
    "- Turn on automatic loss scaling inside the training Optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE CODE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph-based example:\n",
    "# opt = tf.train.AdamOptimizer()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# train_op = opt.miminize(loss)\n",
    "\n",
    "# # Keras-based example:\n",
    "# opt = tf.keras.optimizers.Adam()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# model.compile(loss=loss, optimizer=opt)\n",
    "# model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR KERAS API\n",
    "\n",
    "Source:\n",
    "- https://www.tensorflow.org/guide/keras/mixed_precision\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Set for MIXED PRECISION:\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - History of Object Detection:\n",
    "\n",
    "In the past, classical computer vision techniques for object detection uses image descriptors, this is where to detect an object like a bike would require several images of this object. The term descriptors refers to the bike object that would be extracted from the image and that these descriptors represents different parts of the bike. As the algorithm looks for the object (bike), it will try to find the descriptors in the target images. \n",
    "\n",
    "The most common technique at the time was the usage of the floating window. It is where small rectangular areas of the images were examined one by one, where the part that matches the descriptors the most would be considered to contain the object of interest. This technique have a few advantage at the time, where it was robust to rotation and colour changes in the images, it also does not require a lot training data samples and overall, it worked for most objects. The drawback was that the level of accuracy was not good enough. Soon Neural Networks outpaced this tradiational technique.\n",
    "\n",
    "Modern algorithms can be seen to have better performance, where this refers to the following:\n",
    "1. Bounding Box Precision - it provides the correct bounding box where it is not too large or narrow.\n",
    "2. Recall - it is able to find all the objects.\n",
    "3. Class Prediction - it is able to output the correct class/label for each of the found object.\n",
    "4. Speed of the algorithm - it is where the models are getting faster and faster at computing the results so that it can be used in real time. (real time = 5fps for computer vision tasks.)\n",
    "\n",
    "## 2 - Evaluating the Object Detection model's Performance:\n",
    "\n",
    "Before diving into YOLO's architecture or going further, it is important to cover some basics in model evaluation that are related to the YOLO model. Evaluating an object detection model will require more than accuracy of the prediction against the ground truth, these extra metrics to be included here are called __Precision__ and __Recall__. These will serve as the basis to compute other metrics that are important to object detection. \n",
    "\n",
    "## 2.1 - Precision and Recall Metrics:\n",
    "\n",
    "To begin with, here are the formulas for these metrics:\n",
    "\n",
    "$$ precision = \\frac{TP}{TP - FP} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ recall = \\frac{TP}{TP - FN} $$\n",
    "\n",
    "where here,\n",
    "\n",
    "- TP = Number of True Positives (like how many prediction are matching the ground truth of the same class) \n",
    "- FP = Number of False Positives (like how many prediction that do not match the ground truth for the same class)\n",
    "- FN = Number of False Negatives (like how many ground truths do not have a matching prediction)\n",
    "\n",
    "Additionally, which is not included in the formulas above, but are used in other areas/topics:\n",
    "- TN = Number of True Negatives (like how many ground truths are actualy a negative example)\n",
    "\n",
    "If the predictions are exactly matching the ground truthss, both FP and FN won't exist. In this case following the formulas, both precision and recall would equal to 1, leading to a perfect score. \n",
    "\n",
    "When a model's predictions comes from non-robust features during its training, the precision of the overall model will decrease because there will be a rise in false positives. In contrast, if the model is too strict where it detects only under precise conditions are met, the recall will decrease due to a rise in false negatives.\n",
    "\n",
    "## 2.2 - Precision and Recall Curve:\n",
    "\n",
    "Precision-Recall curves are used to summarise the trade off between the TP rate and the posistive predictive calue for a model by using different probability thresholds. The ideal model should have both high precision and high recall, however, most models often have a trade off between the two. One way to tell how well the model is performing is to compare the area under the curve (AUC) of the chart, where if the area is larger then it performs better or is a better classifier. Below shows a chart that compares two models where the BLUE line model is a better model compared to the GREEN line model. \n",
    "\n",
    "Source: \n",
    "- https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "- https://www.geeksforgeeks.org/precision-recall-curve-ml/\n",
    "\n",
    "<img src=\"Description Images/Precision Recall Curve.png\" width=\"350\">\n",
    "\n",
    "Image Ref -> https://www.geeksforgeeks.org/precision-recall-curve-ml/\n",
    "\n",
    "Since the general idea is to visualise the performance (precision and recall) at each threshold of confidence, where for every output bounding box, the model will also output a confidence (between 0 and 1, where 1 is more confident). In practice it is a good idea to remove less confident predictions and this is done by setting a certain threshold (T = 0.48 or 0.4) so that any predictions below this number will be removed. \n",
    "\n",
    "Changing the value of the threshold will impact the model's precision and recall, where for:\n",
    "1. __If T = close to 1__: The Precision will be high and the Recall will be low. This means that many objects will be filtered out and the model will miss it as only the confident predictions are kept.\n",
    "2. __If T = close to 0__: The Recall will be high and the Precision will be low. This means that most of the predictions will be kept and there will not be any false negatives. This directly contributes to the rise of false positive results where the model is less confident in its predictions. \n",
    "\n",
    "The choice of this threshold is also high reliant on the problem at hand (classification task). For example, for a model to detect pedestrians, the ideal model would have a high recall rate so not to misss any passers-by, even if the car have to stop for no reason. For a model to detect investment opportunities, the ideal model will have a high precision rate to make sure that it doesn't miss good opportunities and avoids the wrong opportunities.\n",
    "\n",
    "## 2.3 - Average Precision and Mean Average Precision:\n",
    "\n",
    "For convenience in practice, often a single number to describe what is happening is very useful. In these cases there are\n",
    "two terms to consider using: \n",
    "\n",
    "1. __Average Precision (AP)__: is the area under th curve and it is always between 0 and 1. This was meantioned earlier. It gives a good indicator of the model performance for a __Single__ class.\n",
    "2. __Mean Average Precision (mAP)__: is the mean of the average precision for __each__ of the classes. It provides an overall __global scor__. \n",
    "\n",
    "## 2.4 - Average Precision Threshold: Jaccard Index (or Intersection over Union, IoU)\n",
    "\n",
    "The __Jaccard Index__ or __Intersection over Union (IoU)__ is the common metric that is used to measure when a prediction and ground truth are matching. This also defines the TP and FP metrics earlier. The following equation defines the IoU a:\n",
    "\n",
    "$$ IoU(A, B) =  \\frac{\\lvert{A \\cap B}\\rvert}{\\lvert{A \\cup B}\\rvert} = \\frac {\\lvert{A \\cap B}\\rvert}{\\lvert{A}\\rvert - \\lvert{B}\\rvert - \\lvert{A \\cap B}\\rvert} $$\n",
    "\n",
    "Where here,\n",
    "- $\\lvert{A}\\rvert$, is the cardinality of set A. The number of elements A contains.\n",
    "- $\\lvert{B}\\rvert$, is the cardinality of set B. The number of elements B contains.\n",
    "- $\\lvert{A \\cap B}\\rvert$, is the numerator for number of elements that are in common between A and B. Here $A \\cap B$ is the intersection between the two sets.\n",
    "- $\\lvert{A \\cup B}\\rvert$, is the demoninator for total number of elements that A and B sets covers. Here $A \\cup B$ is the union of the sets.\n",
    "\n",
    "#### The following diagram shows Precision, Recall and IoU:\n",
    "\n",
    "<img src=\"Description Images/IoU intersection.PNG\" width=\"600\">\n",
    "\n",
    "Image Ref -> http://www.gabormelli.com/RKB/Bounding_Box_Intersection_over_Union_(IoU)_Measure\n",
    "\n",
    "Generally, the IoU are used in the computation rather than just the intersections, because the intersections value is absolute but not relative, meaning that two big boxes have more overlapping pixels than two smaller boxes, so the ratio is used instead as it is relative. IoU will always be between 0 and , where 0 if the two boxes does not overlap and 1 if the boxes overlaps completely.\n",
    "\n",
    "## 3 - YOLO Detection Algorithm: \n",
    "\n",
    "YOLO which stands for You Only Look Once algorithm, is one of the fasted object detection algorithm that are available. One a decent GPU, it will be able to achieve >100 FPS (frames per second) for an image sized at 256 by 256 pixels. YOLO is not only fast but accurate too, it has several iterations over the last few years from 2015 to 2018, where the current version is v3. \n",
    "\n",
    "The main creator for the YOLO paper maintains the network framework (Called Darknet) here: https://github.com/pjreddie/darknet.\n",
    "\n",
    "## 3.1 - Advantages and Disadvantages of YOLO:\n",
    "\n",
    "As mentioned before, the YOLO model is praised for its speed, however, recently this model was outperformed by Fast R-CNN because it performs better than YOLO on detection of smaller objects. Another disadvantage is that the model struggles to classify objects that deviate from the original training dataset. \n",
    "\n",
    "## 3.2 - Main Concepts of YOLO:\n",
    "\n",
    "From their paper (link below), they tackled the problem differently by \"reframing object detection as a single regression problem\". This essentially means that rather than using a sliding window or other techniques, the input image is divided into a width x height grid. This can be seen in the diagram below:\n",
    "\n",
    "<img src=\"Description Images/YOLO Grid.PNG\" width=\"350\">\n",
    "\n",
    "Image Ref -> https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/\n",
    "\n",
    "As it can be seen in each part of the grid, the bounding boxes are defined. This is then followed by predicting the (i) entre of the box, (ii) the width and height of the box, (iii) probability that there is an object within the box, and (iv) lastly the class of the object itself. It is because these predictions are computed as numbers, the problem can be transformed into a regression problem as stated. This covers roughly the basics of the YOLO's concept in detecting objects, there are also more complex issues such as, more than one object in the grid, overlapping objects in several parts of the grid, or how to train the loss for this model? \n",
    "\n",
    "Source: \n",
    "- https://arxiv.org/pdf/1506.02640v5.pdf\n",
    "- https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006\n",
    "\n",
    "For the purpose of simplicity the model will be broken down into two parts, begining with the __Inference__ and then the __Training__ later on.\n",
    "\n",
    "## 3.3 - YOLO Architecture: Part 1 Inference.\n",
    "\n",
    "The YOLO architecture itself is based on a __Backbone model__ like most other CNNs, where the backbone model is the feature extractor that will extract meaningful features from the input images that will be used by the final layers later on. The feature extractor architecture here can be anything, but the official YOLO feature extractor uses its own custom backbone. Changing or altering the feature extractor can lead to different performance numbers. Below is a diagram that shows YOLO's model architecture:\n",
    "\n",
    "<img src=\"Description Images/YOLO Architecture.PNG\" width=\"850\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/how-to-peform-yolo-object-detection-using-keras/\n",
    "\n",
    "The feature extractor's final layer will output a feature volume of size width x height x depth (w x h x D), where the width x height is the size of the grid and D is the depth of the feature volume. The size of the grid (w x h) will depend on the following two factors:\n",
    "\n",
    "- Stride of the complete feature extractor.\n",
    "- Size of the input image (smaller the input image, the smaller the grid).\n",
    "\n",
    "The final layers of the model will take the feature volume as its input. The final layers are composed of 1x1 convolutional filters, which changes the depth of the feature volume without affecting its spatial structure.\n",
    "\n",
    "## 3.4 - Outputs:\n",
    "\n",
    "The final outputs from the model is sized as width x height x M (or w x h x M). The (w x h) is the grid size and $M = B * (C + 5)$. Where here, B is the number of bounding boxes per grid cell, C is the number of classes. Notice that there is a number 5 added to the number of classes. This is because, for each of the bounding boxes there is a need to predict (C + 5) of the following numbers:\n",
    "\n",
    "1. $t_{x}$ and $t_{y}$ - is to compute the coordinates of the centre of the bounding box.\n",
    "2. $t_{w}$ and $t_{h}$ - is to compute the width and height of the bounding box.\n",
    "3. c - is the confidence of the object in the box.\n",
    "4. $p1, p2, ..., $ and $pC$ - are  the probability that the bounding box has the object of class 1, 2, ..., C (where C = 20 for the  dataset classes).\n",
    "\n",
    "Alternatively, $M = B * (C + 5)$ can be broken down further as $M = B * (C + 4 + 1)$. Where here, B is the number of bounding boxes per grid cell, C is the number of classes, 4 is for the bounding boxes and 1 is the objectness prediction.\n",
    "\n",
    "Below the diagram summarises the output matrix:\n",
    "\n",
    "<img src=\"Description Images/YOLO final matrix output.PNG\" width=\"850\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/deep-learning-bounding-boxes/ and https://www.cnblogs.com/ranjiewen/p/7896398.html\n",
    "\n",
    "## 3.5 - Anchor Boxes:\n",
    "\n",
    "Prior to computing the final bounding boxes as mentioned above, the YOLO architecture also employs the concept of __Anchor Boxes__. This technique was because in version 1 of YOLO, many errors (because the objects varried in sizes) cropped up by directly computing the bounding box coordinates: $t_{x}, t_{y}, t_{w}, t_{h}$. Most of the objects in the training set were to big which resulted in larger bounding boxes, and the model would fail to draw these boxes around smaller objects. YOLO version 2 utilised Anchor Boxes to fix this issue.\n",
    "\n",
    "__Anchor Boxes (a.k.a Priors) are a set of bounding boxes__ that were decided before training the network, and consists of 3 to 25 different sizes. The network will then refine these anchor boxes to find the closest matching ones for the object in the image. For example, in the image below, a set of 5 bounding boxes were picked to detect the car. As these boxes do not exactly match the object, the network will be required to refine the closest matching anchor box. \n",
    "\n",
    "<img src=\"Description Images/YOLO Anchor Box Example 1.PNG\" width=\"350\">\n",
    "\n",
    "Image Ref -> https://www.jeremyjordan.me/object-detection-one-stage/\n",
    "\n",
    "In this example, the network will need to correct the height and a small portion of the width of the anchor box. These are what the coordinates, $t_{x}, t_{y}, t_{w}, t_{h}$, previously mentioned above, they are the __corrections to the anchor box__ on the object. For YOLO version 2, the creators realised that sizes of anchor boxes are different for each dataset, therefore it was recommended that the data is to be analysed first to pick the size of the anchor boxes.\n",
    "\n",
    "## 3.6 - Refining the Anchor Boxes:\n",
    "\n",
    "The refinement process of these anchor boxes in practice are computed with the following equations:\n",
    "\n",
    "$$ b_{x} = sigmoid(t_{x} + c_{x}) $$\n",
    "$$ b_{y} = sigmoid(t_{y} + c_{y}) $$\n",
    "\n",
    "$$ b_{w} = p_{w}exp(t_{w}) $$\n",
    "$$ b_{h} = p_{h}exp(t_{h}) $$\n",
    "\n",
    "Where here,\n",
    "- $t_{x}, t_{y}, t_{w}, t_{h}$ are the outputs from the last layer.\n",
    "- $b_{x}, b_{y}, b_{w}, b_{h}$ are the position and size of the predicted bounding box.\n",
    "- $p_{w}$ and $p_{h}$ are the original size of the anchor box.\n",
    "- $c_{x}$ and $p_{y}$ are the coordinates of the current grid cell, like (0, 0) for the top left box, (w-1, ) for the top right box, and (0, h-1) for the bottom left box.\n",
    "- $exp$ is the exponential function.\n",
    "- $sigmoid$ is the sigmoid function.\n",
    "\n",
    "Below is a diagram to show how these equations are use:\n",
    "\n",
    "<img src=\"Description Images/YOLO Anchor Boxes refinement.PNG\" width= \"400\">\n",
    "\n",
    "Image Ref -> https://arxiv.org/pdf/1612.08242v1.pdf\n",
    "\n",
    "The output matrix of the neural network would need to be transformed into a list of boudning boxes, it also needs to be ran for every inference for the computation of the bounding boxes for an image, and below is a simplified version of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the numbers of an output matrix from Neural Network to a list of bounding boxes:\n",
    "if dont_run == 1:\n",
    "    \n",
    "    boxes = []\n",
    "    for row in range(grid_height):\n",
    "        for col in range(grid_width):\n",
    "            for b in range(nb_box):\n",
    "                tx, ty, tw, th = network_output[row, col, b, :4]\n",
    "                box_confidence = network_output[row, col, b, 4]\n",
    "                class_scores = network_output[row, col, b, 5:]\n",
    "                \n",
    "                # Bounding Boxes\n",
    "                bx = signmoid(tx) + col\n",
    "                by = sigmoid(ty) + row\n",
    "                \n",
    "                # Anchor boxes are a list of dict consisting of the size of each anchor:\n",
    "                bw = anchor_boxes[b]['w'] * np.exp(tw)\n",
    "                bh = anchor_boxes[b]['h'] * np.exp(th)\n",
    "                \n",
    "                boxes.append( (bx, by, bw, bh, box_confidence, class_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 - Post-processing the Bounding Boxes:\n",
    "\n",
    "Before the boxes can be displayed, it will require a futher post-processing. From the above, the list consists of the cordinates, size of the predicted boxes, confidence and the class probabilities. However, right now, the class probabilities also includes predictions that are less confident or wrong, so these will need to be excluded. Utilising these, the confidence and the class probabilities are then multiplied firstly, then a threshold is used to include only those that are of greater probabilities.\n",
    "\n",
    "Below is the code to show this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    \n",
    "    # The confidence is a float variable, Classes are an array of the size equivalent to the nb_classes:\n",
    "    final_scores = box_confidence * class_scores\n",
    "    \n",
    "    # Set the Threshold:\n",
    "    OBJECT_THRESHOLD = 0.3\n",
    "    \n",
    "    # Filter out the lower probabilities and keep the ones that are greater than the threshold:\n",
    "    filter_classes = class_scores >= OBJECT_THRESHOLD\n",
    "    \n",
    "    filtered_scores = class_scores * filter_classes\n",
    "    \n",
    "    # If the filtered scores contains non-null values, it means that at least one class is above the threshold and this is kept:\n",
    "    class_id = np.argmax(filtered_scores)\n",
    "    class_label = CLASS_LABELS[class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the filtering/post-processing to all of the bounding boxes in the grid, all the information can be used to draw the predictions. The following shows the output from this stag:\n",
    "\n",
    "<img src=\"Description Images/YOLO ouput bounding boxes before NMS.png\" width=\"350\">\n",
    "\n",
    "Image Ref -> https://towardsdatascience.com/guide-to-car-detection-using-yolo-48caac8e4ded\n",
    "\n",
    "As it can be seen, there are numerous bounding boxes that are overlapping the objects in the grid. This is an outcome of the object occupying multiple grid boxes and therefore will have multiple bounding boxes overlapping each other. In order to correct this and clean up the image, the last stage of the post-processing pipeline is the __Non-Maximum Supression (NMS)__.\n",
    "\n",
    "## 3.8 - Non-Maximum Supression (NMS):\n",
    "\n",
    "The NMS post-processing procedure is to remove all the other unwanted bounding boxes (with the lowest probability) and keep only the bounding box with the highest probability. This is why it is call __Non-Maximum__ as it removes the boxes with lower probabilities. This can be done by firstly sorting out all the boxes by its probability, then taking the boxes with the highest probability. Next, for each of the box chosen, proceed to compute the __Intersection over Union (IoU)__ with all the other boxes.\n",
    "\n",
    "Once the IoU(s) are computed between a box and the other boxes, the ones with an IoU value that is above a certain threshold (typically, 0.5 to 0.9) will be removed.\n",
    "\n",
    "The following shows an example code for the NMS:\n",
    "\n",
    "NOTE: In practice, TensorFlow has its own implementation of NMS called \"tf.image.non_max_supression(boxes, ...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    \n",
    "    # Sort the bounding boxes:\n",
    "    sorted_boxes = sort_boxes_by_confidence(boxes)\n",
    "    ids_to_suppress = []\n",
    "    \n",
    "    # Compute the IoU:\n",
    "    for maximum_box in sorted_boxes:\n",
    "        for idx, box, in enumerate(boxes):\n",
    "            iou = compute_iou(maximum_box, box)\n",
    "            \n",
    "            if iou > iou_threshold:\n",
    "                ids_to_suppress.append(idx)\n",
    "                \n",
    "    # Delete the unwanted boxes:\n",
    "    processed_boxes = np.delete(boxes, ids_to_suppress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After post-processing with NMS, the object now has only one bounding box surrounding it. This can be seen in the image below:\n",
    "\n",
    "<img src=\"Description Images/YOLO ouput bounding boxes after NMS.png\" width=\"600\">\n",
    "\n",
    "Image Ref -> https://towardsdatascience.com/guide-to-car-detection-using-yolo-48caac8e4ded\n",
    "\n",
    "## 3.9 - Summary of YOLO Inference:\n",
    "\n",
    "Before diving into YOLO training phase, this section will summarise all the steps taken. \n",
    "\n",
    "1. Input image enters the CNN backbone to compute the feature volumes.\n",
    "2. Using the convolutional layer to compute the anchor box corrections, objectness scores, and class probabilities.\n",
    "3. Using the output matrix to compute the coordinates of the bounding boxes.\n",
    "4. Filter out the lower threshold boxes and post-process them with NMS to obtain the highest probability bounding box over the object.\n",
    "\n",
    "It should be noted that this whole process is composed of convolutions and filtering operations throughout, which means that the model can ingest images of any size and ratios, giving the property of flexibility.\n",
    "\n",
    "## 4 - YOLO Architecture: Part 2 YOLO Training.\n",
    "\n",
    "The YOLO model is composed of two parts, where the first is the backbone and the second is the YOLO head. As for the backbone architectures, many types can be used. Typically before training the full YOLO model, the backbone would be trained first, on a classsification task with pretrained weights of ImageNet as this speeds up the computation time. \n",
    "\n",
    "Keras can be employed to train the backbone based on the pretrained wweights of ImageNet, the following shows an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    \n",
    "    # Define the input image shape:\n",
    "    input_image = Input(shape = (IMAGE_H, IMAGE_W, 3))\n",
    "    \n",
    "    # Define the True Box for the objects:\n",
    "    true_boxes = Input(shape = (1, 1, 1, TRUE_BOX_BUFFER, 4))\n",
    "    \n",
    "    # Instantiate the Backbone CNN model: Inception model v.\n",
    "    inception = InceptionV3(input_shape = (IMAGE_H, IMAGE_W, 3),\n",
    "                            weights = 'imagenet',\n",
    "                            include_top = False)\n",
    "    \n",
    "    # Get the output feature volumes from the inception model:\n",
    "    features = inception(input_image)\n",
    "    \n",
    "    # Get the out shape:\n",
    "    GRID_H, GRID_W = inception.get_output_shape_at(-1)[1:3]\n",
    "    \n",
    "    # Get the output with the bounding box:\n",
    "    output = Conv2D(BOX * (4 + 1 + CLASS), \n",
    "                    (1, 1),\n",
    "                    strides = (1, 1),\n",
    "                    padding = 'same',\n",
    "                    name = 'DetectionLayer',\n",
    "                    kernel_initializer = 'lecun_normal')(features)\n",
    "    \n",
    "    # Reshape the output:\n",
    "    output = Reshape( (GRID_H, GRID_W, BOX, 4 + 1 + CLASS) )(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - YOLO Loss:\n",
    "\n",
    "Immediately it can be seen that the output of the last layer is unusual compared to model that were implemented in previous notebooks, therefore the corresponding loss here will also be unusual. Note that the YOLO loss is quite complex and will be required to be broken down into several parts. Each of these corresponding loss outputs will be returned by the last layer.\n",
    "\n",
    "The YOLO network will predict the following kinds of information:\n",
    "- Bounding box coordinates and its size. (__Bounding Box Loss__)\n",
    "- Cofidence that the object of interest is in the bounding box. (__Object Confidence Loss__)\n",
    "- Scores of the classes. (__Classification Loss__)\n",
    "- Full YOLO Loss.\n",
    "\n",
    "In general, the loss computed here should be high when the error is high and the loss will be used to penalise the incorrect values. However, with YOLO, the loss should only penalise when it makes sense to do so, such as if the bounding box contains no objects, there is no need to penalise its coordinates as it won't be utilised anyway. \n",
    "\n",
    "NOTE: As the source paper does not include how the loss is computed, it is better to check other sources of their loss implementation as there are several that works.\n",
    "\n",
    "### 4.1.1 - Boudning Box Loss:\n",
    "\n",
    "This first loss aids the model to learn the weights so that it can be used to predict the bounding box coordinates and its size. Key to this loss equation would be the __indicator function__, where the computed coordinates will only be correct when the box is responsible in detecting an object. The key part of this loss computation is the indicator function, where the coordinates will be correct if the box is responsible for detecting an object. Taking the instance of YOLO v1, it had difficulty in determining the correct bounding box to use, and in YOLO v2, it was resolved by using the highest IoU with the detected object. The goal here is to ensure that each objectof interest will have its own anchor boxes.\n",
    "\n",
    "The following equation shows the loss at this stage:\n",
    "\n",
    "$$ \\lambda_{coord} \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} 1_{ij}^{obj} [(x_{i} -\\hat{x_{i}})^{2}] + \\lambda_{coord} \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} 1_{ij}^{obj} [(\\sqrt{w_{i}} - \\sqrt{\\hat{w_{i}}})^{2} + (\\sqrt{h_{i}} - \\sqrt{\\hat {h_{i}}})^{2}] $$\n",
    "\n",
    "Where here,\n",
    "- $ \\lambda_{coord} $ is the weighting of the loss, it determines how much importance is given to the bounding box coordinates during training.\n",
    "- $ \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} $ is the sum for each part of the grid (from i = 0 to i = $S^{2}$) and for each box in this part of the grid (from 0 to B).\n",
    "- $ 1_{ij}^{obj} $ is the indicator function for objects, it is equal to 1 when the $i^{th}$ part of the grid and the $j^{th}$ bounding box are responsible for an object.\n",
    "- $ x_{i}, y_{i}, w_{i}m h_{i}$ is the bounding box size and coordinates. The difference between the predicted value (hat^) from the ouput of the network and the target value (ground truth). \n",
    "- $ Square Root $ ensures that the values are positive.\n",
    "- By taking the square root of $w_{i}$ and $h_{i}$, it ensures that the errors for small bounding boxes are penalised more heavily compared to larger bounding boxes.\n",
    "\n",
    "### 4.1.2 - Object Confidence Loss:\n",
    "\n",
    "The next loss computation will have the network learn the weights that predicts if the bounding box contains an object.\n",
    "\n",
    "The following equation shows the loss equation:\n",
    "\n",
    "$$ \\lambda_{obj} \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} 1_{ij}^{obj} [(C_{ij} -\\hat{C_{ij}})^{2}] + \\lambda_{noobj} \\sum^{S^{2}}_{i=0} \\sum^{B}_{j=0} 1_{ij}^{noobj} [(C_{ij} -\\hat{C_{ij}})^{2}] $$\n",
    "\n",
    "Where here,\n",
    "- $C_{ij}$ is the confidence that the box (j) in the part (i) of the grid does contain an object.\n",
    "- $1_{ij}^{noobj}$ is the indicator function for no object, it will equal to 1 when the $i^{th}$ part of the gird and the $j^{th}$ bounding box are not responsible for an object.\n",
    "\n",
    "In order to ensure that the objectness score is not penalised when there are other good candidates that also fits the object of interest, the $1_{ij}^{noobj}$ can be defined as seen below:\n",
    "\n",
    "$$ 1_{ij}^{noobj} == \\{^{1 (box is not responsible for any object) and (box not overlapping too much with any object bounding box)}_{0 (otherwise)}$$\n",
    "\n",
    "Note that in practice, for each of the bounding box (i, j), the IoU w.r.t the ground truth boxes is computed. If the IoU is over the threshold (e.g. 0.6), $1_{ij}^{noobj}$ would be set to 0. This ensures to avoid penalising the boxes that contain objects but are not responsible for the object of interest.\n",
    "\n",
    "### 4.1.3 - Classification Loss:\n",
    "\n",
    "The last part of the loss computation is the classification loss which have the network learning to predict the proper class for each of the bounding box. The following shows the loss equation:\n",
    "\n",
    "$$ \\sum^{S^{2}}_{i=0} 1_{i}^{obj} \\sum_{c \\epsilon classes} (p_{i}(c) - \\hat{p_{i}}(c))^{2} $$\n",
    "\n",
    "NOTE: the original YOLO paper utilised L2 loss while many other implementations were using cross-entropy.\n",
    "\n",
    "### 4.1.4 - Full YOLO Loss:\n",
    "\n",
    "With the above three loss equations stated, the final overall loss is computed by summing the three losses together. This means that the combination computes the loss penalises the error of the bounding box coordinate refinement, objectness scores and the class prediction. By backpropagating this error, the YOLO model can be trained properly to predict the correct bounding boxes for the correct objects.\n",
    "\n",
    "## 5 - Techniques used for Training:\n",
    "\n",
    "Using the loss, the YOLO model can be trained through backpropagation. To ensure that the loss does not diverge away and that good performance were to be obtained, techniques such as:\n",
    "\n",
    "1. Augmentation, is used to prevent overfitting the dataset.\n",
    "2. Multi-scaling training, where for every \"n\" batches, the network's input is changed to a different siz, which ensures the model learns to predict accurately across several input dimensions.\n",
    "3. Pretraining the YOLO model on image classification task.\n",
    "4. Burn-in, where the learning rate is reduced at the beginning of the training process to avoid a loss explosion. \n",
    "\n",
    "## 6 - Further Reading:\n",
    "\n",
    "It is recommended to have a go at these three YOLO papers:\n",
    "1. https://arxiv.org/pdf/1506.02640.pdf\n",
    "2. https://arxiv.org/pdf/1612.08242.pdf\n",
    "3. https://pjreddie.com/media/files/papers/YOLOv3.pdf\n",
    "\n",
    "## 7 - Building the YOLO model in TensorFlow:\n",
    "\n",
    "This section will now go through the implemenetation of the YOLO v3 model in TensorFlow 2.0++. Before getting started, download and save the \"yolov3.weights\" from pjreddie.com by using the following command:\n",
    "\n",
    "wget https://pjreddie.com/media/files/yolov3.weights\n",
    "\n",
    "Note that if the above command does not work, you can manually go to the website and download it. Link: https://github.com/AlexeyAB/darknet\n",
    "\n",
    "Instructions for manual download:\n",
    "\n",
    "__Step 1__:\n",
    "\n",
    "Navigate through the Github Readme file and find the following:\n",
    "\n",
    "<img src=\"Description Images/Manual Download of YOLO weights_1.PNG\" width=\"600\">\n",
    "\n",
    "Image Ref -> https://github.com/AlexeyAB/darknet\n",
    "\n",
    "__Step 2__:\n",
    "\n",
    "Expand and Download the weights file.\n",
    "\n",
    "<img src=\"Description Images/Manual Download of YOLO weights_2.PNG\" width=\"600\">\n",
    "\n",
    "Image Ref -> https://github.com/AlexeyAB/darknet\n",
    "\n",
    "__Step 3__:\n",
    "\n",
    "Create your own folder inyour working directory for the weights so that it can be found later.\n",
    "\n",
    "### 7.1 - Import the required Libaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from absl import logging\n",
    "from itertools import repeat\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (Add, Concatenate, Lambda, \n",
    "                                     Conv2D, Input, LeakyReLU, \n",
    "                                     MaxPool2D, UpSampling2D, ZeroPadding2D)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import (binary_crossentropy, \n",
    "                                     sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - Define the Global Variables:\n",
    "\n",
    "Note that, the paths to the weights file (.weights) and its checkpoints (.tf) file will be placed in a .py utility file as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Threshold for YOLO:\n",
    "yolo_iou_threshold = 0.6\n",
    "\n",
    "# Define the Threshold Score:\n",
    "yolo_score_threshold = 0.6\n",
    "\n",
    "# Define the path directory to the weigths file:\n",
    "weightsyolov3 = 'yolov3.weights'\n",
    "\n",
    "# Define the path directory to the checkpoints file:\n",
    "weights = 'checkpoints/yolov3.tf'\n",
    "\n",
    "# Resize the image size:\n",
    "img_size = 416\n",
    "\n",
    "# Define the checkpoint file:\n",
    "checkpoints = 'checkpoints/yolov3.tf'\n",
    "\n",
    "# Define the number of classes in the model:\n",
    "nb_classes = 80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 - List the Layers of the Yolo v3 model's Fully Convolutional Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLOV3_LAYER_LIST = ['yolo_darknet', \n",
    "                     'yolo_conv_0',\n",
    "                     'yolo_output_0',\n",
    "                     'yolo_conv_1',\n",
    "                     'yolo_output_1',\n",
    "                     'yolo_conv_2',\n",
    "                     'yolo_output_2',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 - Load in the pretrained weights:\n",
    "\n",
    "The YOLO v3 implementation utilised here is a fully convolutional network and is done without any training, but in order to predict an output with the model, pretrained weights are needed. This was the reason the weights were downloaded and saved locally first. Therefore, the pretrained weights will need to be loaded in and to do this, a function is need.\n",
    "\n",
    "To aid the model in speeding up its computations, batch normalisation will be employed to normalise the outputs from the layers. Note that \"tf.keras.layers.BatchNormalization\" would not work here as this whole implementation utilises Transfer Learning. The fix is also included in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_darknet_weights(model, weights_file):\n",
    "    wf = open(weights_file, 'rb')\n",
    "    major, minor, revision, seen, _ = np.fromfile(wf, dtype = np.int32, count = 5)\n",
    "    layers = YOLOV3_LAYER_LIST\n",
    "    \n",
    "    for layer_name in layers:\n",
    "        sub_model = model.get_layer(layer_name)\n",
    "        \n",
    "        for i, layer in enumerate(sub_model.layers):\n",
    "        \n",
    "            # If the layer name does not start with 'conv2d', skip it and move to the next layer:\n",
    "            # As this is the convolutional layer, there is no batch_norm set.\n",
    "            if not layer.name.startswith('conv2d'):\n",
    "                continue\n",
    "            batch_norm = None\n",
    "            \n",
    "            # For all the layers that consist of Batch Norm within the model, if the layer starts with \"batch_norm\"\n",
    "            # set it as the batch_norm variable:\n",
    "            if i + 1 < len(sub_model.layers) and sub_model.layers[i + 1].name.startswith('batch_norm'):\n",
    "                batch_norm = sub_model.layers[i + 1]\n",
    "            \n",
    "            # Log the layers layout for each sub_model:\n",
    "            logging.ifo(\"{}/{} {}\".format(sub_model.name, layer.name, 'bn' if batch_norm else 'bias'))\n",
    "            \n",
    "            # Define the filters:\n",
    "            filters = layer.filters\n",
    "        \n",
    "            # Define the filter size:\n",
    "            size = layer.kernel_size[0]\n",
    "            \n",
    "            # Define the input dimensions:\n",
    "            input_dim = layer.input_shape[-1]\n",
    "            \n",
    "            # When batch_norm is None, set it to convolutional bias:\n",
    "            if batch_norm is None:\n",
    "                conv_bias = np.fromfile(wf, dtype = np.float32, count = filters)\n",
    "            else:\n",
    "                # darknet [beta, gamma, mean, variance]:\n",
    "                bn_weights = np.fromfile(wf, dtype=np.float32, count = 4 * filters)\n",
    "                \n",
    "                # tf [gamma, beta, mean, variance]:\n",
    "                bn_weights = bn_weights.reshape( (4, filters) )[[1, 0, 2, 3]]\n",
    "            \n",
    "            # darknet shape (out_dim, in_dim, height, width):\n",
    "            conv_shape = (filters, input_dim, size, size)\n",
    "            \n",
    "            conv_weights = np.fromfile(wf, dtype = np.float32, count = np.product(conv_shape))\n",
    "            \n",
    "            # tf shape (height, width, in_dim, out_dim):\n",
    "            conv_weights = conv_weights.reshape(conv_shape).transpose( [2, 3, 1, 0] )\n",
    "            \n",
    "            # Set the weights for the Convolutional layers and the bias:\n",
    "            if batch_norm is None:\n",
    "                layer.set_weights( [conv_weights, conv_bias] )\n",
    "            else:\n",
    "                # Set the weights for the batch norm:\n",
    "                layer.set_weights( [conv_weights] )\n",
    "                batch_norm.set_weights(bn_weights)\n",
    "                \n",
    "    assert len(wf.read()) == 0, 'failed to read the weights.'\n",
    "    \n",
    "    wf.close()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalisation(tf.keras.layers.BatchNormalization):\n",
    "    \"\"\" This builds the Batch Normalisation function for YOLOv3 to be compatible with Transfer Learning Methods.\n",
    "        It also inherits the properties from tf.keras.layers.BatchNormalization.\n",
    "    \n",
    "    Parameters: within the \"call\" method.\n",
    "        - Make trainable = False freeze BN for real.\n",
    "    \"\"\"\n",
    "    def call(self, x, training = False):\n",
    "        if training is None:\n",
    "            training = tf.constant(False)\n",
    "            \n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "            \n",
    "        return super().call(x, training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 - Define the function computing the Intersection over Union (IoU):\n",
    "\n",
    "The following two functions will compute the \"intersection\" and the \"IuO\" as seen in the diagram below:\n",
    "\n",
    "<img src=\"Description Images/IoU intersection_2.PNG\" width=\"350\">\n",
    "\n",
    "Image Ref -> http://www.gabormelli.com/RKB/Bounding_Box_Intersection_over_Union_(IoU)_Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_overlap(interval_1, interval_2):\n",
    "    x1, x2 = interval_1\n",
    "    x3, x4 = interval_2\n",
    "    \n",
    "    if x3 < x1:\n",
    "        return 0 if x4 < x1 else (min(x2, x4) - x1)\n",
    "    else:\n",
    "        return 0 if x2 < x3 else (mi(x2, x4) - x3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersectionOverUnion(box1, box2):\n",
    "    # Compute the intersect width and height:\n",
    "    intersect_width = interval_overlap( [box1.xmin, box1.xmax], [box2.xmin, box2.xmax] )\n",
    "    intersect_height = interval_overlap( [box1.ymin, box1.ymax], [box2.ymin, box2.ymax] )\n",
    "    \n",
    "    # Compute the intersection area:\n",
    "    intersect_area = intersect_width * intersect_height\n",
    "    \n",
    "    # Get the length of height and width for both boxes:\n",
    "    w1 = box1.xmax - box1.xmin\n",
    "    h1 = box1.ymax - box1.ymin\n",
    "    \n",
    "    w2 = box2.xmax - box2.xmin\n",
    "    h2 = box2.ymax - box2.ymin\n",
    "    \n",
    "    # Compute the Area of Union:\n",
    "    union_area = (w1 * h1) + (w2 * h2) - intersect_area\n",
    "    \n",
    "    return float(intersect_area) / union_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 - Anchor Boxes:\n",
    "\n",
    "For each of the scales (13x13 scale, 26x26 scale and 52x52 scale), the 3 anchor boxes will be defined for each grid.\n",
    "\n",
    "The mask are:\n",
    "- 0, 1, 2 are the masks to use for the first three anchor boxes.\n",
    "- 3, 4, 5 are the masks to use for the fourth, fifth and sixth boxes.\n",
    "- 6, 7, 8 are the masks to use for the seventh, eighth and ninth boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Yolo Anchor boxes:\n",
    "yolo_anchors = np.array( [(10, 13), (16, 30), (33, 23), \n",
    "                          (30, 61), (62, 45), (59, 119), \n",
    "                          (116, 90), (156, 198), (373, 326)], np.float32 ) / 416\n",
    "\n",
    "# Define the Yolo Anchor Masks:\n",
    "yolo_anchor_masks = np.array( [[6, 7, 8], [3, 4, 5], [0, 1, 2]] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 - Function to draw the Bounding Boxes, Class Name and Probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_outputs(img, outputs, class_names):\n",
    "    boxes, score, classes, nums = outputs\n",
    "    boxes, score, classes, nums = boxes[0], score[0], classes[0], nums[0]\n",
    "    \n",
    "    wh = np.flip(img.shape[0:2])\n",
    "    \n",
    "    for i in range(nums):\n",
    "        x1y1 = tuple( (np.array(boxes[i][0:2]) *  wh).astype(np.int32) )\n",
    "        x2y2 = tuple( (np.array(boxes[i][2:4]) *  wh).astype(np.int32) )\n",
    "        \n",
    "        img = cv2.rectangle(img, x1y1, x2y2, (255, 0, 0), 2)\n",
    "        \n",
    "        img = cv2.putText(img = img, \n",
    "                          text = '{} {:.3f}'.format(class_names[int(classes[i])], score[i]), \n",
    "                          org = x1y1, \n",
    "                          fontFace = cv2.FONT_HERSHEY_COMPLEX_SMALL, \n",
    "                          fontScale = 1,\n",
    "                          color = (0, 0, 255), \n",
    "                          thickness = 2)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 - Implement YOLOv3 Network Architecture:\n",
    "\n",
    "As seen in the diagram below, the Fully Convolutional layers consists of 53 layers. The Functional API would be used to implement these layers effectively and these layers can be altered by simply passing the stated overall parameters into the model functions. It also means that the branches such as the Residual Block can be defined as well as easily implemented amongst the layers. The model architecture uses Residual Blocks for learning the features of the input images. \n",
    "\n",
    "<img src=\"Description Images/YOLO Architecture_2.PNG\" width=\"700\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/tensorflow2-0-yolov3/\n",
    "\n",
    "Below shows the Residual Block:\n",
    "\n",
    "<img src=\"Description Images/Residual Block.PNG\" width=\"500\">\n",
    "\n",
    "Image Ref -> https://towardsdatascience.com/architecture-comparison-of-alexnet-vggnet-resnet-inception-densenet-beb8b116866d\n",
    "\n",
    "#### For DarknetConv block:\n",
    "\n",
    "The code design follows accordingly:\n",
    "\n",
    "<img src=\"Description Images/DarknetConv.PNG\" width=\"400\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/tensorflow2-0-yolov3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DarknetConv(x, filters, size, strides = 1, batch_norm = True):\n",
    "    \"\"\" This builds the Convolutions Filters over the input image.\n",
    "    Parameters:\n",
    "        - x, is the input image tensor.\n",
    "        - filters, is the number of filters/kernels.\n",
    "        - size, is the size of the kernels.\n",
    "        - strides, is the convoluitional stride.\n",
    "        - batch_norm, is a Flas to use or not the batch normalisation in this convolutional layer.\n",
    "    Returns:\n",
    "        - returns the processed (convolved) x output.\n",
    "    \"\"\"\n",
    "    # Set the padding to ouput the same dimension/shape as the input if stride = 1:\n",
    "    if strides == 1:\n",
    "        padding = 'same'\n",
    "    else:\n",
    "        # Padding applied to only the top left of the input tensor:\n",
    "        x = ZeroPadding2D( padding=((1, 0), (1, 0)) )(x)\n",
    "        x = Conv2D(filters = filters,\n",
    "                   kernel_size = size,\n",
    "                   strides = strides,\n",
    "                   padding= padding,\n",
    "                   use_bias = not batch_norm,\n",
    "                   kernel_regularizer = l2(l = 0.0005)\n",
    "                  )(x)\n",
    "        \n",
    "    # If this layer uses batch_norm, set the following:\n",
    "    if batch_norm:\n",
    "        x = BatchNormalisation()(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For DarknetResidual block:\n",
    "\n",
    "The code desgin follows accordingly:\n",
    "\n",
    "<img src=\"Description Images/DarknetResidual.PNG\" width=\"400\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/tensorflow2-0-yolov3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DarknetResidual(x, filters):\n",
    "    \"\"\" This builds the Residual block.\n",
    "    Parameters:\n",
    "        - x, is the input tensor.\n",
    "        - filters, is the number of filters/kernels\n",
    "    Returns:\n",
    "        - returns the processed x output.\n",
    "    \"\"\"\n",
    "    # Save an instance of x to be used for merging later:\n",
    "    previous_x = x\n",
    "    \n",
    "    # 1st DarknetConv_BatchNorm_LeakyReLU (DBL) block:\n",
    "    x = DarknetConv(x = x,\n",
    "                    filters = filters // 2,\n",
    "                    size = 1)\n",
    "    # 2nd DarknetConv_BatchNorm_LeakyReLU (DBL) block:\n",
    "    x = DarknetConv(x = x,\n",
    "                    filters = filters,\n",
    "                    size = 3)\n",
    "    \n",
    "    # Merge the Residual and DBL:\n",
    "    x = Add()( [previous_x, x] )\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the Residual Body Block:\n",
    "\n",
    "The code desgin follows accordingly:\n",
    "\n",
    "<img src=\"Description Images/DarknetResidual_Body.PNG\" width=\"400\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/tensorflow2-0-yolov3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DarknetBlock(x , filters, blocks):\n",
    "    \"\"\" This builds the Resblock_body block.\n",
    "    Parameters:\n",
    "        - x, is the input tensor.\n",
    "        - filters, is the number of kernels/filters.\n",
    "        - blocks, is the number of repeated blocks to be set.\n",
    "    Returns:\n",
    "        - returns the processed x output of this ResN block.\n",
    "    \"\"\"\n",
    "    # 1st block with zero padding and DarknetConv_BatchNorm_LeakyReLU (DBL) block:\n",
    "    x = DarknetConv(x = x,\n",
    "                    filters = filters,\n",
    "                    size = 3, \n",
    "                    strides=2)\n",
    "    \n",
    "    # Repeated ResN blocks (or Redisdual blocks):\n",
    "    for _ in repeat(None, blocks):\n",
    "        x = DarknetResidual(x = x, \n",
    "                            filters = filters)   \n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For 53-Layer Darknet Fully Convolutional Network:\n",
    "\n",
    "The code desgin follows accordingly:\n",
    "\n",
    "<img src=\"Description Images/Darknet_53.PNG\" width=\"400\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/tensorflow2-0-yolov3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Darknet(name = None):\n",
    "    \"\"\" This builds the 53-Layer Darknet model.\n",
    "    Parameters:\n",
    "        - name, is the Name Suffix of this layer.\n",
    "    Returns:\n",
    "        - returns the model.\n",
    "    \"\"\"\n",
    "    # Input:\n",
    "    x = inputs = Input( [None, None, 3] )\n",
    "       \n",
    "    # 1st layer after input: DarknetConv_BatchNorm_LeakyReLU (DBL) block.\n",
    "    x = DarknetConv(x = x, filters = 32, size = 3)\n",
    "        \n",
    "    # 2nd Block: res1 block.\n",
    "    x = DarknetBlock(x = x, filters = 64, blocks = 1)\n",
    "    \n",
    "    # 3rd Block: res2 block.\n",
    "    x = DarknetBlock(x = x, filters = 128, blocks = 2)\n",
    "    \n",
    "    # 4th Block: res8 block.\n",
    "    x = x_36 = DarknetBlock(x = x, filters = 256, blocks = 8)\n",
    "    \n",
    "    # 5th Block: res8 block.\n",
    "    x = x_61 = DarknetBlock(x = x, filters = 512, blocks = 8)\n",
    "    \n",
    "    # 6th Block: res4 block.\n",
    "    x = DarknetBlock(x = x, filters = 1024, blocks = 4)\n",
    "    \n",
    "    return tf.keras.Model(inputs, (x_36, x_61, x), name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Yolo Convolutions:\n",
    "\n",
    "The code desgin follows accordingly:\n",
    "\n",
    "<img src=\"Description Images/Yolo_conv.PNG\" width=\"700\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/tensorflow2-0-yolov3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YoloConv(filters, name = None):\n",
    "    \"\"\" This builds the Yolo Convolutions (DBL * 5) and the (DBL + Up-Sampling).\n",
    "    Parameters:\n",
    "        - filters, is the number of filters.\n",
    "        - name, is the Name Suffix of this layer.\n",
    "    Returns:\n",
    "        - returns a callable yolo_conv layer.\n",
    "    \"\"\"\n",
    "    def yolo_conv(x_in):\n",
    "        \n",
    "        # For DBL with Up-sampling:\n",
    "        if isinstance(x_in, tuple):\n",
    "            inputs = Input(x_i[0].shape[1:]), Input(x_in[1].shape[1:])\n",
    "            \n",
    "            x, x_skip = inputs\n",
    "            \n",
    "            x = DarknetConv(x = x, filters = filters, size = 1)\n",
    "            x = UpSampling2D(2)(x)\n",
    "            x = Concatenate()( [x, x_skip] )\n",
    "        \n",
    "        else:\n",
    "            x = inputs = Input(x_in.shape[1:])\n",
    "            \n",
    "        # For DBL * 5 Layer:\n",
    "        x = DarknetConv(x = x, filters = filters, size = 1)\n",
    "        x = DarknetConv(x = x, filters = filters * 2, size = 3)\n",
    "        x = DarknetConv(x = x, filters = filters, size = 1)\n",
    "        x = DarknetConv(x = x, filters = filters * 2, size = 3)\n",
    "        x = DarknetConv(x = x, filters = filters, size = 1)\n",
    "        \n",
    "        return Model(inputs, x, name = name)(x_in)\n",
    "    return yolo_conv          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Yolo Outputs, Yolo Boxes and Non Maximum Suppression:\n",
    "\n",
    "The code desgin follows accordingly:\n",
    "\n",
    "<img src=\"Description Images/Yolo_output.PNG\" width=\"700\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/tensorflow2-0-yolov3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YoloOutput(filters, anchors, classes, name = None):\n",
    "    \"\"\" This builds the Yolo Output blocks, where it can be configured for different channels.\n",
    "    Parameters:\n",
    "        - filters, is the number of filters/kernels.\n",
    "        - anchors, is the defined number of anchor boxes (masks).\n",
    "        - classes, is the number of classes.\n",
    "        - name, is the Name Suffix of this layer.\n",
    "    Returns:\n",
    "        - returns a callable yolo output layer.\n",
    "    \"\"\"\n",
    "    def yolo_output(x_in):\n",
    "        x = inputs = Input( x_in.shape[1:] )\n",
    "        x = DarknetConv(x = x, filters = filters * 2, size = 3)\n",
    "        x = DarknetConv(x = x, filters = anchors * (classes + 5), size = 1, batch_norm = False)\n",
    "        x = Lambda(lambda x: tf.reshape(tensor = x, \n",
    "                                        shape = (-1, tf.shape(x)[1], tf.shape(x)[2], anchors, classes + 5)\n",
    "                                        )\n",
    "                  )(x)\n",
    "        \n",
    "        return tf.keras.Model(inputs, x, name = name)(x_in)\n",
    "    return yolo_output       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_boxes(pred, anchors, classes):\n",
    "    \"\"\" This builds the YOLO model output prediction boxes.\n",
    "    Parameters:\n",
    "        - pred, is the input predictions from the model.\n",
    "        - anchors, is the defined number of anchor boxes (masks).\n",
    "        - classes, is the number of classes.\n",
    "    Returns:\n",
    "        - returns the bounding boxes (bbox), score, class_probs, and pred_box.\n",
    "    \"\"\"\n",
    "    # Define the Grid Size:\n",
    "    grid_size = tf.shape(pred)[1]\n",
    "    \n",
    "    # Define the box x-y, w-h, score and class_prob:\n",
    "    box_xy, box_wh, score, class_probs = tf.split(pred, (2, 2, 1, classes), axis=-1)\n",
    "    \n",
    "    # Refine the Acnhor boxes (part 1):\n",
    "    box_xy = tf.sigmoid(box_xy)\n",
    "    score = tf.sigmoid(score)\n",
    "    class_probs = tf.sigmoid(class_probs)\n",
    "    pred_box = tf.concat( (box_xy, box_wh), axis = -1)\n",
    "    \n",
    "    # Build the Grid:\n",
    "    grid = tf.meshgrid(tf.range(grid_size),\n",
    "                       tf.range(grid_size))\n",
    "    grid = tf.expand_dims(tf.stack(grid, axis = -1), \n",
    "                          axis = 2)\n",
    "    \n",
    "    # Refine the Anchor Boxes (part 2):\n",
    "    box_xy = (box_xy + tf.cast(grid, tf.float32)) / tf.cast(grid_size, tf.float32)\n",
    "    box_wh = tf.exp(box_wh) * anchors\n",
    "    \n",
    "    box_x1y1 = box_xy - box_wh / 2\n",
    "    box_x2y2 = box_xy + box_wh / 2\n",
    "    \n",
    "    bbox = tf.concat( [box_x1y1, box_x2y2], axis = -1)\n",
    "    \n",
    "    return bbox, score, class_probs, pred_box  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonMaximumSuppression(outputs, anchors, masks, classes):\n",
    "    \"\"\" This builds the NMS, where it remove all the other unwanted bounding boxes (with the lowest probability).\n",
    "    Parameters:\n",
    "        - outputs, is the output tensor from the previous layer.\n",
    "        - anchors, is the number of anchors.\n",
    "        - masks, is the anchor masks.\n",
    "        - classes, is the number of classes.\n",
    "    Returns:\n",
    "        - returns boxes, scores, classes, valid_detections.\n",
    "    \"\"\"\n",
    "    # Define empty arrays:\n",
    "    boxes, conf, out_type = [], [], []\n",
    "    \n",
    "    # Append the empty arrays:\n",
    "    for output in outputs:\n",
    "        boxes.append(tf.reshape(tensor = output[0],\n",
    "                                shape = (tf.shape(output[0])[0], -1, tf.shape(output[0])[-1])\n",
    "                               )\n",
    "                    )\n",
    "        conf.append(tf.reshape(tensor = output[1],\n",
    "                                shape = (tf.shape(output[1])[0], -1, tf.shape(output[1])[-1])\n",
    "                               )\n",
    "                    )\n",
    "        out_type.append(tf.reshape(tensor = output[2],\n",
    "                                   shape = (tf.shape(output[2])[0], -1, tf.shape(output[2])[-1])\n",
    "                                  )\n",
    "                       )\n",
    "        \n",
    "    # Define the bounding box, confidence and class_probs: axis=1 means concat to the right side, not below.\n",
    "    bbox = tf.concat(values = boxes, axis = 1)\n",
    "    confidence = tf.concat(values = conf, axis = 1)\n",
    "    class_probs = tf.concat(values = out_type, axis = 1)\n",
    "    \n",
    "    # Compute the scores:\n",
    "    scores = confidence * class_probs\n",
    "    \n",
    "    # Apply TensorFlow NMS:\n",
    "    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(boxes = tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),\n",
    "                                                                                     scores = tf.reshape(scores, (tf.shape(scores)[0], -1, tf.shape(scores)[-1])),\n",
    "                                                                                     max_output_size_per_class = 100,\n",
    "                                                                                     max_total_size = 100,\n",
    "                                                                                     iou_threshold = yolo_iou_threshold,\n",
    "                                                                                     score_threshold = yolo_score_threshold\n",
    "                                                                                    )\n",
    "    \n",
    "    return boxes, scores, classes, valid_detections\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Entire YOLOv3 Network model:\n",
    "\n",
    "The code desgin follows accordingly:\n",
    "\n",
    "<img src=\"Description Images/YOLO Architecture_2.PNG\" width=\"600\">\n",
    "\n",
    "Image Ref -> http://datahacker.rs/tensorflow2-0-yolov3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YoloV3(size = None, channels = 3, anchors = yolo_anchors, masks = yolo_anchor_masks, classes = 80, training = False):\n",
    "    \"\"\" This builds the YoloV3 model, by combining all the blocks.\n",
    "    Parameters:\n",
    "        - size, is the kernels/filers size.\n",
    "        - channels, is the number of RGB =3 colour channels.\n",
    "        - anchors, is the yolo anchors.\n",
    "        - masks, is the yolo anchors masks.\n",
    "        - classes, is the number of classes.\n",
    "        - training, is a Flag to state whether the model instantiation will will to be trained or not.\n",
    "    Returns:\n",
    "        - returns the YoloV3 model.\n",
    "    \"\"\"\n",
    "    # Input layer:\n",
    "    x = inputs = Input( [size, size, channels] )\n",
    "    \n",
    "    # Instantiate the Darknet-53 without FC layer:\n",
    "    x_36, x_61, x = Darknet(name = 'yolo_darknet')(x)\n",
    "    \n",
    "    # Define the yolo_head blocks for each type of (512, 256, 128) or (1024, 512, 256) chanel: \n",
    "    # For masks -> [6, 7, 8]\n",
    "    x = YoloConv(filters = 512, nam = 'yolo_conv_0')(x)\n",
    "    output_0 = YoloOutput(filters = 512, anchors = len(masks[0]), classes = classes, name = 'yolo_output_1')(x)\n",
    "    \n",
    "    # For masks -> [3, 4, 5]\n",
    "    x = YoloConv(filters = 256, nam = 'yolo_conv1')(x, x_61)\n",
    "    output_1 = YoloOutput(filters = 256, anchors = len(masks[1]), classes = classes, name = 'yolo_output_2')(x)\n",
    "    \n",
    "    # For masks -> [0, 1, 2]\n",
    "    x = YoloConv(filters = 128, nam = 'yolo_conv_2')(x, x_36)\n",
    "    output_2 = YoloOutput(filters = 128, anchors = len(masks[2]), classes = classes, name = 'yolo_output_2')(x)\n",
    "    \n",
    "    \n",
    "    # If Flag 'training' = True:\n",
    "    if training:\n",
    "        return Model(inputs, (output_0, output_1, output_2), name = 'yolov3')\n",
    "    \n",
    "    # Compute the bounding boxes:\n",
    "    boxes_0 = Lambda(lambda x: yolo_boxes(pred = x, \n",
    "                                          anchors = anchors[masks[0]], \n",
    "                                          classes = classes), name = 'yolo_boxes_0')(output_0)\n",
    "    boxes_1 = Lambda(lambda x: yolo_boxes(pred = x, \n",
    "                                          anchors = anchors[masks[1]], \n",
    "                                          classes = classes), name = 'yolo_boxes_1')(output_1)\n",
    "    boxes_2 = Lambda(lambda x: yolo_boxes(pred = x, \n",
    "                                          anchors = anchors[masks[2]], \n",
    "                                          classes = classes), name = 'yolo_boxes_2')(output_2)\n",
    "    \n",
    "    # Apply NMS on the candidate bounding boxes:\n",
    "    outputs = Lambda(lambda x: nonMaximumSuppression(outputs = x, \n",
    "                                                     anchors = anchors, \n",
    "                                                     masks = masks, \n",
    "                                                     classes = classes), \n",
    "                     name = 'nonMaximumSuppression')( (boxes_0[:3], boxes_1[:3], boxes_2[:3]) )\n",
    "    \n",
    "    \n",
    "    return MOdel(inputs, outputs, name = 'yolov3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the model loss computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YoloLoss(anchors, classes = 80, ignore_threshold = 0.5):\n",
    "    \"\"\" This builds for the compute of the model losses.\n",
    "    Parameters:\n",
    "        - anchors, is the yolo_anchors or anchor boxes.\n",
    "        - classes, is the number of classes.\n",
    "        - ignore_threshold, is when not specified, the threshold will be default at 0.5.\n",
    "    Returns:\n",
    "        - returns yolo_loss, where = xy_loss + wh_loss + obj_loss + class_loss.\n",
    "    \"\"\"\n",
    "    def yolo_loss(y_true, y_pred):\n",
    "        \n",
    "        # Part 1 - Transform all pred outputs:\n",
    "        # y_pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...cls))\n",
    "        pred_box, pred_obj, pred_class, pred_xywh = yolo_boxes(pred = y_pred, anchors = anchors, classes = classes)\n",
    "        \n",
    "        # Split the predicted box shape lines:\n",
    "        pred_xy = pred_xywh[..., 0:2]\n",
    "        pred_wh = pred_xywh[..., 2:4]\n",
    "        \n",
    "        # Part 2 - Transform all true outputs\n",
    "        # y_true: (batch_size, grid, grid, anchors, (x1, y1, x2, y2, obj, cls))\n",
    "        true_box, true_obj, true_class_idx = tf.split(value = y_true, num_or_size_splits = (4, 1, 1), axis = -1)\n",
    "        \n",
    "        # Split the Ground truth box shape lines:\n",
    "        true_xy = (true_box[..., 0:2] + true_box[..., 2:4]) / 2\n",
    "        true_wh = true_box[..., 2:4] - true_box[..., 0:2]\n",
    "        \n",
    "        # Allow for higher weights to small boxes:\n",
    "        box_loss_scale = 2 - true_wh[..., 0] * true_wh[..., 1]\n",
    "        \n",
    "        # Part 3 - Inverting the pred box equations:\n",
    "        grid_size = tf.shape(y_true)[1]\n",
    "        grid = tf.meshgrid(tf.range(grid_size, tf.range(grid_size)))\n",
    "        grid = tf.expand_dims(tf.stack(grid, axis = -1), axis = 2)\n",
    "        \n",
    "        # Convert true_xy to match grid size:\n",
    "        true_xy = true_xy * tf.cast(grid_size, tf.float32) - tf.cast(grid, tf.float32)\n",
    "        true_wh = tf.math.log(true_wh, anchors)\n",
    "        true_wh = tf.where(tf.math.is_inf(true_wh), tf.zeros_like(true_wh), true_wh)\n",
    "        \n",
    "        # Part 4 - Compute all masks:\n",
    "        obj_mask = tf.squeeze(input = true_obj, axis = -1)\n",
    "        \n",
    "        # Ignore when IoU is over the threshold stated:\n",
    "        true_box_flat = tf.boolean_mask(tensor = true_box, mask = tf.casst(obj_mask, tf.bool))\n",
    "        \n",
    "        # Find the Best IoU:\n",
    "        best_iou = tf.reduce_max(intersectionOverUnion(box1 = pred_box, box2 = true_box_flat), axis = -1)\n",
    "        \n",
    "        # Ignore IoU:\n",
    "        ignore_mask = tf.cast(best_iou < ignore_threshold, tf.float32)\n",
    "        \n",
    "        # Part 5 - Compute all losses:\n",
    "        xy_loss = obj_mask * box_loss_scale * tf.reduce_sum(input_tensor = tf.square(true_xy - pred_xy), axis = -1)\n",
    "        wh_loss = obj_mask * box_loss_scale * tf.reduce_sum(input_tensor = tf.square(true_wh - pred_wh), axis = -1)\n",
    "        \n",
    "        # Using binary_crossentropy for the object loss:\n",
    "        obj_loss = binary_crossentropy(y_true = true_obj, y_pred = pred_obj)\n",
    "        obj_loss = obj_mask * obj_loss + (1 - obj_mask) * ignore_mask * obj_loss\n",
    "        \n",
    "        # Using sparse_categorical_crossentropy instead for the classes loss:\n",
    "        class_loss= obj_mask * sparse_categorical_crossentropy(y_true = true_class_idx, y_pred = pred_class)\n",
    "        \n",
    "        # Part 6 - Sum over (batch, gridx, gridy, anchors) => (batch, 1)\n",
    "        xy_loss = tf.reduce_sum(xy_loss, axis = (1, 2, 3))\n",
    "        wh_loss = tf.reduce_sum(wh_loss, axis = (1, 2, 3))\n",
    "        obj_loss= tf.reduce_sum(obj_loss, axis = (1, 2, 3))\n",
    "        class_loss = tf.reduce_sum(class_loss, axis = (1, 2, 3))\n",
    "        \n",
    "        return xy_loss + wh_loss + obj_loss + class_loss\n",
    "    \n",
    "    return yolo_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Tansforming targets:\n",
    "\n",
    "Here, the funtion will transform targets outputs tuple of shape:\n",
    "\n",
    "( \\\n",
    "  [N, 13, 13, 3, 6],\\\n",
    "  [N, 26, 26, 3, 6],\\\n",
    "  [N, 52, 52, 3, 6] \\\n",
    ")\n",
    "\n",
    "Where here,\n",
    "\n",
    "- N is the number of labels in the batch.\n",
    "- 6 is [x, y, w, h, obj, class] of the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def transform_targets_for_output(y_true, grid_size, anchor_idxs, classes):\n",
    "    \"\"\" This builds the function to transform targets outputs tuple of shape.\n",
    "        (\n",
    "            [N, 13, 13, 3, 6],\n",
    "            [N, 26, 26, 3, 6],\n",
    "            [N, 52, 52, 3, 6]\n",
    "        )\n",
    "        \n",
    "    Parameters:\n",
    "        - y_true, is the labels.\n",
    "        - grid_size, is the size of the grid.\n",
    "        - anchor_idxs, is the anchor indexes.\n",
    "        - classes, is the number of classes.\n",
    "    Returns:\n",
    "        - returns an updated scattered tensor.\n",
    "    \n",
    "    \"\"\"\n",
    "    # For y_true, shape is (N, boxes, (x1, y1, x2, y2, class, best_anchor))\n",
    "    N = tf.shape(y_true)[0]\n",
    "\n",
    "    # For y_true_out, shape is (N, grid, grid, anchors, [x, y, w, h, obj, class])\n",
    "    y_true_out = tf.zeros( (N, grid_size, grid_size, tf.shape(anchor_idxs)[0], 6) )\n",
    "    anchor_idxs = tf.cast(anchor_idxs, tf.int32)\n",
    "\n",
    "    # Define array for indexes and updates:\n",
    "    indexes = tf.TensorArray(tf.int32, size = 1, dynamic_size = True )\n",
    "    updates = tf.TensorArray(tf.float32, size = 1, dynamic_size = True)\n",
    "\n",
    "    # Loop over labels to get anchor boxes:\n",
    "    idx = 0\n",
    "    for i in tf.range(N):\n",
    "        for j in tf.range(tf.shape(y_true)[1]):\n",
    "            if tf.equal(y_true[i][j][2], 0):\n",
    "                continue\n",
    "            anchor_eq = tf.euqal(anchor_idxs, tf.cast(y_true[i][j][5], tf.int32))\n",
    "            \n",
    "            if tf.reduce_any(anchor_eq):\n",
    "                box = y_true[i][j][0:4]\n",
    "                box_xy = (y_true[i][j][0:2] + y_true[i][j][2:4]) / 2\n",
    "                \n",
    "                anchor_idx = tf.cast(tf.where(anchor_eq), tf.int32)\n",
    "                grid_xy = tf.cast(box_xy // (1 / grid_size), tf.int32)\n",
    "                \n",
    "                indexes = indexes.write(idx, [i, grid_xy[1], grid_xy[0], anchor_idx[0][0]])\n",
    "                updates = updates.write(idx, [box[0], box[1], box[2], box[3], 1, y_true[i][j][4]])\n",
    "                idx += 1\n",
    "                \n",
    "    return tf.tensor_scatter_nd_update(y_true_out, indexes.stack(), updates.stack())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_targets(y_train, anchors, anchor_mask, classes):\n",
    "    \"\"\" This transforms the Outputs -> [x, y, w, h, object, class].\n",
    "    Parameters:\n",
    "        - y_train, are the labels from the training set.\n",
    "        - anchors, are the anchor boxes.\n",
    "        - anchor_masks, are the anchor boxes masks.\n",
    "        - classes, is the number of classses.\n",
    "    Returns:\n",
    "        - returns a tuple of the output -> [x, y, w, h, obj, class]\n",
    "    \"\"\"\n",
    "    # Define the array for the outputs:\n",
    "    outputs = []\n",
    "    \n",
    "    # Define the Grid size:\n",
    "    grid_size = 13\n",
    "    \n",
    "    # Compute the anchor index for true boxes:\n",
    "    anchors = ttf.cast(anchors, tf.float32)\n",
    "    anchor_area = anchors[..., 0] * anchors[..., 1]\n",
    "    \n",
    "    box_wh = y_train[..., 2:4] - y_train[..., 0:2]\n",
    "    box_wh = tf.tile(input = tf.expand_dims(input = box_wh, axis = -2),\n",
    "                     multiples = (1, 1, tf.shape(anchors)[0], 1))\n",
    "    \n",
    "    box_area = box_wh[..., 0] * box_wh[..., 1]\n",
    "    \n",
    "    intersection = tf.minimum(x = box_wh[..., 0], y = anchors[..., 0]) * tf.minimum(x = box_wh[..., 1], y = anchors[..., 1])\n",
    "    iou = intersection / (box_area + anchor_area - intersection)\n",
    "    \n",
    "    anchor_idx = tf.cast(tf.argmax(iou, axis = -1), tf.float32)\n",
    "    anchor_idx = tf.expand_dims(input = anchor_idx, axis = -1)\n",
    "    \n",
    "    y_train = tf.concat( [y_train, anchor_idx], axis = -1 )\n",
    "    \n",
    "    for anchor_i in anchor_masks:\n",
    "        outputs.append(transform_targets_for_output(y_true = y_train, \n",
    "                                                    grid_size = grid_size,\n",
    "                                                    anchor_idxs = anchor_i, \n",
    "                                                    classes = classes))\n",
    "        grid_size *= 2\n",
    "    \n",
    "    return tuple(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(x_train, size):\n",
    "    return (tf.image.resize(x_train, (size, size))) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Run the Yolo v3 model:\n",
    "\n",
    "First, instantiate the Yolov3 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "\n",
    "Source: \n",
    "- https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/\n",
    "- https://manalelaidouni.github.io/manalelaidouni.github.io/Understanding%20YOLO%20and%20YOLOv2.html\n",
    "- https://appsilon.com/object-detection-yolo-algorithm/\n",
    "- http://datahacker.rs/how-to-peform-yolo-object-detection-using-keras/\n",
    "- http://datahacker.rs/tensorflow2-0-yolov3/\n",
    "- https://github.com/zzh8829/yolov3-tf2\n",
    "- https://github.com/AlexeyAB/darknet\n",
    "\n",
    "## Summary:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
