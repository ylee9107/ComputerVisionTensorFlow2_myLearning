{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Neural Networks - Notebook 4\n",
    "\n",
    "This notebook will continue on from Notebook 3 of Modern Neural Networks and dive into the OOP-style development of CNN (LeNet-5) with TensorFlow 2 along with an experiment on different Regularisation methods.\n",
    "\n",
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Adding Regularisation to the Model:\n",
    "\n",
    "## 5.1 - What is Regularisation?\n",
    "\n",
    "Previously covered sections mainly trains the models to minimise the loss function to update the network weights in order to obtain better accuracies over time. Regularly, the model may need further improvements to be applied to prevent the model from overfitting the data. Overfitting is undesired because when the trained model recevies new unknown data to classify, it won't be able to perform with the same accuracy. The idea here, is to employ techniques to prevent overfitting so that the model can generalise well. \n",
    "\n",
    "Methods without regularisation can be:\n",
    "1) Training the model on a Rich dataset, this provides enough variability from the data to improve the model's performance during testing scenarios. \\\n",
    "2) Change the model architecture with experiments, ensuring that the model is not too shallow to avoid underfitting or too deep to prevent overfitting. \n",
    "\n",
    "Regularistion techniques are:\n",
    "1) Early Stopping. \\\n",
    "2) L1 and L2 Regularisation. \\\n",
    "3) Dropout. \\\n",
    "4) Batch Normalisation.\n",
    "\n",
    "The following section will discuss more about these techniques.\n",
    "\n",
    "## 5.2 - Early Stopping:\n",
    "\n",
    "This straightforward technique essentially stops the model during training at a certain point (traininng epoch), this is to prevent overfitting as the model iterates over the dataset too many times, where ususally more common when the dataset has less training samples. The stopping point should be low enough to stop overfitting and large enough to ensure that the model can learn all that is needed. \n",
    "\n",
    "Cross-validation is the key to deciding on the early stopping point. This is done with providing a validation dataset to test the model on and through this validation, the network is able to measure if the training process should be continued or not.\n",
    "\n",
    "Note: This can be automatically implemented with Keras Callbakcs, \"tf.keras.callbacks.EarlyStopping\".\n",
    "\n",
    "## 5.3 - L1 and L2 Regularisation:\n",
    "\n",
    "Generally, using regularisation in Machine Learning penalises the coefficients of the fitting function while in Deep Learning, the weight matrices of the nodes are the ones being penalised.\n",
    "\n",
    "Mathematically, the regularisation term $R(P)$ is added to the loss function before training. This can be represened as the following:\n",
    "\n",
    "$$ L(y, y^{true}) + \\lambda R(P) $$ with $$ y = f(x, P) $$\n",
    "\n",
    "Where, \n",
    "- $\\lambda$ is the controlling factor for the strength of the regularisation term. \n",
    "- $y$ is the output of the function $f$ that is parameterised by $P$ for the input data $x$.\n",
    "\n",
    "Next, the L1 and L2 regularisation terms can be defined as:\n",
    "\n",
    "__For L1 Regularisation (a.k.a LASSO)__:\n",
    "\n",
    "$$ R_{L1}(P) = \\left\\lvert \\left\\lvert P \\right\\rvert \\right\\rvert _{1} = \\sum_{k} \\left\\lvert P_{k} \\right\\rvert$$\n",
    "\n",
    "In more detail, the L1 Regulariser (LASSO, Least Absolute Shrinkage and Selection Operator) makes the network minimise the sum of its absolute parameter values. The larger weights are not penalised by the squaring factor, where instead it shrinks the parameters that are linked to the less important feature towards zero. Essentially, the network will ignore the less meaningful features, adopting sparse parameters. This technique is also useful when being applied to models that needs to run on mobile applications.\n",
    "\n",
    "__For L2 Regularisaiton (a.k.a RIDGE)__:\n",
    "\n",
    "$$ R_{L2}(P) = \\frac{1}{2} \\left\\lvert \\left\\lvert P \\right\\rvert \\right\\rvert _{1}^{2} = \\sum_{k} \\frac{1}{2}  \\left\\lvert P_{k}^{2} \\right\\rvert$$\n",
    "\n",
    "In more detail, the L2 Regulariser (RIDGE) makes the network minimise the sum of its squared parameter values. This technique will decay all of the parameter values but it does so more strongly on large parameters (because of the squared term). Essentially, the network will keep its parameter values low and therefore it will be more homogeneously distributed. This technique prevents the network from developing a small set of parameters that has large values which influences the predictions.\n",
    "\n",
    "The Code implementations for these will be shown in the sections below.\n",
    "\n",
    "## 5.4 - Dropout:\n",
    "\n",
    "Larger neural networks with greater number of parameters inherently have the problem of overfitting and with the increase of architecture complexity, the model will be slower to compute. Dropout can be implemented to address these issues. The main concept is to randomly drop units from the neural network by temporarily removing it and its incoming and outgoing connections in the training phase. Applying this technique on the network essentially mean it amounts to sampling a thinned-out network, where it consists of only the units that has survived dropout. \n",
    "\n",
    "This method takes in a hyperparameter ratio of $\\rho$ which is the probability of that neuron being switched off at each of the training step. This value is typically set betweem 0.1 to 0.5.\n",
    "\n",
    "Note: In TF code implementation, \"tf.nn.dropout()\" and in Keras API implementation, \"tf.kears.layers.Dropout()\".\n",
    "\n",
    "#### The diagram below briefly describes the dropout being applied to the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout: \n",
    "display(Image(image_path + 'Dropout.png', width=600, unconfined=True))\n",
    "print('Image ref -> https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 - Batch Normalisation:\n",
    "\n",
    "Batch normalization is a technique used in deep learning to tackle the problem known as internal covariate shift. For each layers output becomes the input for the next layer in the hidden layers. As the model updates after each training iteration using gradient descent, the distribution of the activations changes as well, hence this slows down the training process because each layer has to adapt to these new changes.\n",
    "\n",
    "The batch normalisation operation normalises the output results the from the previous layer and normalises it by subtracting the batch mean and then divides it by the batch standard deviation.\n",
    "As the batches in Stochastic Gradient descent are randomly sampled, this also means that the data won't be normalised the same way twice, therefore, the network learns to deal with these fluctuations making it more robust and generalise better.\n",
    "\n",
    "Note: In TF code implementation, \"tf.nn.batch_normalization()\" and in Keras API implementation, \"tf.kears.layers.BatchNormalization()\".\n",
    "\n",
    "## 5.6 - TF and Keras implementation: Adding Regularisation to the model:\n",
    "\n",
    "### 5.6.1 - Load in the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes:\n",
    "nb_classes = 10\n",
    "\n",
    "# Define the image dimensions:\n",
    "img_rows, img_cols, img_chnls = 28, 28, 1\n",
    "\n",
    "# Define the input shape:\n",
    "input_shape = (img_rows, img_cols, img_chnls)\n",
    "\n",
    "# Load in the dataset:\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 - Data Preprocessing:\n",
    "\n",
    "NOTE: the \"*input_shape\" where the single \" * \" notation in the code means a to create a tuple from this argument. where two \" * \" like \" ** \" is to create a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalise the data:\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Inspect:\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the inputs:\n",
    "x_train = x_train.reshape(x_train.shape[0], *input_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *input_shape)\n",
    "\n",
    "# Inspect:\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Summary:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
