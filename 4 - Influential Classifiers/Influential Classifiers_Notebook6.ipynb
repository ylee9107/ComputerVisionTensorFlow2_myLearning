{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Influential Classification Models (and Tools)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Notebook (6) will continue on from the previous sections. This notebook will go through the process of using __Transfer Learning and Applying it__. Previously, the notebook focused on implementing the Inception model and MobieNet from TensorFlow Hub, here, the intention is to utilise transfer learning with Keras. Utilising models from Keras Applications that are pre-trained on richer datasets on new tasks. The focus here would be to fetch the parameters of pre-trained weights of the models that was trained on the ImageNet dataset, test different types of transfer learning such as freezing and fine-tuning of the feature_extractor layers.\n",
    "\n",
    "## Supporting Utilities .py files:\n",
    "\n",
    "In this Notebook, there will be a requirement to import the code/utilities from the following files (.py files):\n",
    "- DataPrepCIFAR_utility.py\n",
    "- customCallbacks_Keras.py\n",
    "-\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "For this part of the project, the CIFAR-100 dataset will be used, it is a collection of of 60,000 32x32 images that have 100 classes. CIFAR-100 was originally collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. It iss also a subset of the 80 million tiny images dataset. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses.\n",
    "\n",
    "Source: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Further, the TensorFlow team offers a python package called \"tensorflow_datasets\" that provides the helper function to download tthis dataset as well as other more common ones. For the purposes of this project, the CIFAR-100 dataset will be download with this package.\n",
    "\n",
    "Source: https://www.tensorflow.org/datasets/catalog/cifar100\n",
    "\n",
    "## Requirements:\n",
    "- Tensorflow 2.0 (GPU is better)\n",
    "- Tensorflow-Hub\n",
    "- Keras (GPU is better)\n",
    "\n",
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from IPython.display import display, Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42\n",
    "\n",
    "# Paramneter to run code block or not: 0 = to run code, 1 = ignore.\n",
    "dont_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 8046729624521184429),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 6586313605, 7930735724641623814)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Transfer Learning:\n",
    "\n",
    "Humans are able to learn things and apply it to new problems. For example, one part of our lifetime learning experiences are delivered in classrooms where there are teachers that explains the concepts of different topics. This guidance allows us to learn and develop without having to go through the early stages of trial and error until we find the correct path/answer. This behaviour is central to human intelligence. Essentially, this is what transfer learning is and it can be very powerful in the development of machine learning and deep learning. By applying guidance from pre-trained models and applying to new problems or a similar problem, is a way to develop a more proficient system(s) without having to relearn everything from scratch.\n",
    "\n",
    "Typically, most machine learning systems are designed for a single task, where if these systems were to be applied on a different dataset, it would yield very poor results (like MNIST digits vs. ImageNet pictures and so on). As CNNs are trained to interpret some features of the dataset, it does make sense that the model should be able to partially be resued on a different but similar dataset like, classifying digits to classifying texts. The goal of transfer learning is to apply knowledge either from one task to another or to be adapted onto other domains.\n",
    "\n",
    "## 2 - Transfering the Knowledge:\n",
    "\n",
    "This section looks into how it is possible to transfer the knowledge gathered from one model to another? As with digital systems like Machine/Deep Learning, the data/weights can be easily stored and duplicated. \n",
    "\n",
    "Transfer learning for CNNs relies on conditioned instantiation, meaning it consists of reusing either the complete or partial architectures and weights of a past performant model and be instantiated as a new model for a new task. At this point, the model would be fine tuned for the new task/domain.\n",
    "\n",
    "The 1st convolutional layer - extract low level features such as lines, edges or colour gradients, whereas the final convolutional layer - extracts shapes and patterns. The Dense layers at the end of the model are used to process these high level feature maps to make predictions. \n",
    "\n",
    "#### Typical setup for Transfer Learning models:\n",
    "\n",
    "There are various strategies for the usage of pretrained CNNs. One of them is where the final prediction layers are removed, it would be used as an efficient __feature extractor__. If the model would be used for a simlar task where these feature extractors have trained for, it can be used to output pertinent features and then be processed by one or two dense layers which are trained to output predictions. \n",
    "\n",
    "The layers are often __FROZEN__ during the training phase to preserve the quality of the extracted feautres, meaning that the parameters will not be updated during the gradient descent phase. However, in other cases wher the tasks/domain differs, these layers will require __FINE TUNING__. This means that the feature extracting layers would be trained with the new prediction dense layers on the task. The next section will detail other use cases.\n",
    "\n",
    "## 3 - Use Cases of Transfer learning:\n",
    "\n",
    "Aside from the feature extractor case above, the following also details other use cases. Here, the questions asked are which pretrained model requires reusing or to be fine tuned or frozen?\n",
    "\n",
    "List of cases:  \n",
    "1) Limited training data for similar tasks. \\\n",
    "2) Abundant training data for similar tasks. \\\n",
    "3) Limited training data for dissimilar tasks. \\\n",
    "4) Abundant training data for dissimilar tasks.\n",
    "\n",
    "### 3.1 - Limited training data for similar task:\n",
    "\n",
    "When encountering a particular task that do not have enough training samples for the model to learn from, transfer learning is a solution where the model was previously trained on a larger but similar dataset. The pre-trained model can be used here by firstly removing the final layers and be replaced with new final layers. These layers can be trained on the new targeted task. \n",
    "\n",
    "For example, for the purpose of distinguishing bess and wasps, the ImageNet dataset has these two classes but does not contain enough data samples to produce an efficient CNN without overfitting. Therefore, the model can firstly be trained on the entire ImageNet dataset for the 1,000 classes, then its final dense layers are removed and replace with the output prediction layers specifically for 2 classes that are the bees and wasps. \n",
    "\n",
    "By fixing these parameters of the feature extractor layers (trained on the larger dataset), the network is able to retain its expressiveness that was previously developed on the richer dataset. \n",
    "\n",
    "### 3.2 - Abundant training data for similar tasks.:\n",
    "\n",
    "One of reasons why networks tend to overfitting on the dataset is because it has less data samples. The bigger the dataset, the less chance the network has to overfitting it. In the case of larger training data, it is more common to unfreeze the latest layers of feature extractor for the purpose of fine tuning. This allows the network to extract the relevant features for the new task, this essentially translate to better learning during training and better prediction performance. Further, as the model is already close to convergence, it is common practice to use smaller learning rate in the fine tuning phase.\n",
    "\n",
    "### 3.3 - Limited training data for dissimilar tasks:\n",
    "\n",
    "Typically, the use of transfer learning is more advantageous if the task at hand are similar, but does not benefit a model that is used for visual recognition but was trained for audio tasks. For tasks that differs but however presents with a large enough dataset, it may not make sense to use an existing pretrained model. However, it has been demonstrated through experiments that the use of pretrained models performs much better with its pretrained weights than with random initialisation. \n",
    "\n",
    "### 3.4 - Abundant training data for dissimilar tasks:\n",
    "\n",
    "This scenario would be the least ideal for a model to train on and can happen from time to time. In this case, careful considerations are required. For example, it would be crucial to reconsider the usage of deep models as training such a model can lead to overfitting, and that a deep pretrained model would consist of irrelevant features for the task. Interestingly, the solution here would be to utilise the first layers of the CNNs, as it tends to extract the low-level features from the data. This means that more layers other than the final dense layers would require removal, resulting in a shallow classifier. A shallow classifier can be adopted as the top half of the new dense layer for prediction, where this new model can be fine tuned for the specific task.\n",
    "\n",
    "## 4 - Transfer Learning with TensorFlwo and Keras Examples: Model Surgery.\n",
    "\n",
    "It is also common to utilise non-standard models/networks such as state-of-the-art CNNs or custom models by experts in their field/domain. This section will briefly go through the code implementations for the examples above, before a followed up section that applies transfer learning to an actual dataset. This section will also cover model surgery and selective training. \n",
    "\n",
    "Types of Model Surgery:\n",
    "1) Removing Layers. \\\n",
    "2) Grafting Layers.\n",
    "\n",
    "Types of Selective Training:\n",
    "1) Restoring pretrained parameters. \\\n",
    "2) Freezing layers.\n",
    "\n",
    "## Model Surgergy:\n",
    "\n",
    "### 4.1 - Removing Layers:\n",
    "\n",
    "Typically, when using pretrained models, one of the first tasks would be to remove the final prediction layers from the model and transforming it to feature extractors. \n",
    "\n",
    "In Keras, this can be done with the Sequential API where for \"Sequential\" models, the list of layers can be accessed with \"model.layers\" attribute. There is a \"pop()\" method that removes the last layer of the model. This can be done by knowing the number of layers to remove and specifying it in the parameter of the method.\n",
    "\n",
    "For example in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If statement to check if this code block should be executed.\n",
    "if dont_run == 1:\n",
    "    \n",
    "    # Code example below:\n",
    "    for i in range(nb_layers_to_remove):\n",
    "        model.layers.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The removal of layers in TenserFlow is not recommended as it can be highly complex to edit operational graph that supports the model. Note that unused graph operations are not executed during runtime, this means that having old layers incorporated in the compiled graph does not effect the pcompute performance of the new model. Therefore in TensorFlow, the layers are \"removed\" by pinpointing the last layer/operation of the pretrained model that is to be kept rather than outright removing them. \n",
    "\n",
    "If the corresponding Python object was lost as there are a lot of things to keep track, and that the name of the object is known, it can be found with TensorBoard. The represenative tensor can be recovered through a for-loop over the model layers and checking its name.\n",
    "\n",
    "For example in Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If statement to check if this code block should be executed.\n",
    "if dont_run == 1:\n",
    "    \n",
    "    # Code example below:\n",
    "    for layer in model.layers:\n",
    "        if layer.name == name_of_lastLayer_to_keep:\n",
    "            bottleneck_feaatures = layer.output\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all things Keras, it proveds the convience of simplicity to code design and thus this process, as compared to the one example above. Knowing the name of the last layer to be kept, after checking \"model.summary()\" for the layer name, a feature extractor can be instantiated in a couple of lines ans would be ready for use.\n",
    "\n",
    "For example in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If statement to check if this code block should be executed.\n",
    "if dont_run == 1:\n",
    "    \n",
    "    # Code example below:\n",
    "    bottleneck_features = model.get_layer(last_layer_name).output\n",
    "    feature_extractor = Model(inputs = model.input,\n",
    "                              outputs = bottleneck_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Grafting Layers:\n",
    "\n",
    "Grafting is where new prediction layers are added to the pretrained model (on top of the feature extractor). This process is straightforward and can be done easily.\n",
    "\n",
    "For example with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If statement to check if this code block should be executed.\n",
    "if dont_run == 1:\n",
    "    \n",
    "    # Code example below:\n",
    "    dense1 = Dense(...)(feature_extractor.ouput)\n",
    "    new_model = Model(model.input, dense1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective Training:\n",
    "\n",
    "The training phase with transfer learning can present to be complex, as the pretrained layers must be restored firstly and to define which of these layers are to be frozen.\n",
    "\n",
    "### 4.3 - Restoring Pretrained Parameters:\n",
    "\n",
    "For example with TensorFlow, there are utility functions to initialise ssome of the layers with pretrained weights. The following will show the saved parameters of a pretrained estimator to be used with a new model with layers of the same name.\n",
    "\n",
    "The \"warmStartSettings\" is an initialiser that takes an optional argument \"vars_to_warm_start\" to provide the names of the specific variables (as a list or regex form) to be restored from the checkpoint files (ckpt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If statement to check if this code block should be executed.\n",
    "if dont_run == 1:\n",
    "    \n",
    "    # Code example below:\n",
    "    def model_func():\n",
    "        # Define the new model, where it resuses pretrained weights/parameters as a feature extractor.\n",
    "        \n",
    "        # Remove \"None\" below when using this code.\n",
    "        return None\n",
    "    \n",
    "    ckpt_path = '/path/to/pretrained/estimator/model.ckpt'\n",
    "    ws = tf.estimator.WarmStartSettings(skpt_path)\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(model_func, warm_start_from=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example with Keras, the restoration of the pretrained mdoel is done before its transformation for the new task. Note that although this is not the most optimal way to restore the complete model prior to removing some of the unwanted layers, the code is however, concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If statement to check if this code block should be executed.\n",
    "if dont_run == 1:\n",
    "    \n",
    "    # Code example below:\n",
    "    # Assumes pretrained model was saved with the method \"model.save()\":\n",
    "    model.tf.keras.models.load_model('path/to/pretrained/model.h5')\n",
    "    \n",
    "    # Next, is to \"pop\" or \"add\" layers for the creation of the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Freezing Layers:\n",
    "\n",
    "For the Tensorflow example, it presents with the most versatile way of freezing layers. This is done by removing the \"tf.Variable\" attributes from the list of variables to be passed into the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If statement to check if this code block should be executed.\n",
    "if dont_run == 1:\n",
    "    \n",
    "    # Code example below:\n",
    "    # For this case, there is a need to freeze the model's layers with \"conv\" in the name:\n",
    "    vars_to_train = model.trainable_varaibles\n",
    "    vars_to_train = [ v for v in vars_to_train if \"conv\" in v.name]\n",
    "    \n",
    "    # Apply the optimiser to the remaining model's variables:\n",
    "    optimizer.apply_gradietns( zip(gradient, vars_to_train) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Keras example, the layers have a \" .trainable \" attribute that allows to be set as \"False\" to freeze the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If statement to check if this code block should be executed.\n",
    "if dont_run == 1:\n",
    "    \n",
    "    # Code example below:\n",
    "    for layer in feature_extractor.layers:\n",
    "        layer.trainable = False # this freezes the complete extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Applying Transfer Learning on the CIFAR-100 Dataset:\n",
    "\n",
    "Like previous noteboooks, the results gathered here are used to compare with the previously implemented models. This section will then apply transfer learning discussed above, utilising the ResNet model on the CIFAR-100 dataset.\n",
    "\n",
    "## 5.1 - Hyperparamters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Expected input and output shape for the model:\n",
    "input_shape = [224, 224, 3]\n",
    "\n",
    "batch_size = 64\n",
    "nb_epochs = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Data Preparation and Defining the Input Pipeline:\n",
    "\n",
    "To use the TensorFlow package, please ensure to install it: \"pip install tensorflow-datasets\" or use the Anaconda Navigator.\n",
    "\n",
    "### 5.2.1 - Download the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import DataPrepCIFAR_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='cifar100',\n",
      "    version=1.3.1,\n",
      "    description='This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).',\n",
      "    urls=['https://www.cs.toronto.edu/~kriz/cifar.html'],\n",
      "    features=FeaturesDict({\n",
      "        'coarse_label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=100),\n",
      "    }),\n",
      "    total_num_examples=60000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 50000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the Dataset Information:\n",
    "cifarData_info = DataPrepCIFAR_utility.get_info()\n",
    "print(cifarData_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - Set up the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of classes:\n",
    "nb_classes = cifarData_info.features['label'].num_classes\n",
    "\n",
    "# Define the number of images for training and validation:\n",
    "nb_train_imgs = cifarData_info.splits['train'].num_examples\n",
    "nb_valid_imgs = cifarData_info.splits['test'].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 - Set the \"steps_per_epoch\" for Training and Validation Set:\n",
    "\n",
    "From the previous code block, the dataset is compatible with Keras methods liike \"model.fit()\", but will also require the \"steps_per_epoch\" to be specified. This is the number of batches per epoch of the Dataset objects that has to be specified for the Keras method so that it can work properly together. This is done for the training batches (\"steps_per_epoch\") and validation batches (\"validation_steps\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = math.ceil(nb_train_imgs / batch_size)\n",
    "\n",
    "valid_steps_per_epoch = math.ceil(nb_valid_imgs / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 - Load in the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n",
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# Training Dataset:\n",
    "train_set_cifar = DataPrepCIFAR_utility.get_dataset(phase='train',\n",
    "                                                    batch_size= batch_size,\n",
    "                                                    nb_epochs= nb_epochs,\n",
    "                                                    shuffle=True,\n",
    "                                                    input_shape= input_shape,\n",
    "                                                    seed= Seed_nb)\n",
    "\n",
    "# Validation Dataset:\n",
    "val_set_cifar = DataPrepCIFAR_utility.get_dataset(phase='test',\n",
    "                                                  batch_size= batch_size,\n",
    "                                                  nb_epochs= 1,\n",
    "                                                  shuffle= False,\n",
    "                                                  input_shape= input_shape,\n",
    "                                                  seed= Seed_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the CIFAR-100 dataset has been prepared. The next stage is to call the ResNet model.\n",
    "\n",
    "## 5.3 - Freeze the ResNet Feature Extractor:\n",
    "\n",
    "This section will venture into instantiating a ResNet model from Keras Applications, where the model would have weights that were pretrained on the ImageNet dataset. The final layers of the model would be removed and therefore be a feature extractor. The new final dense layers will be incorporated as part of the model for the purpose of predictitons output for the CIFAR-100 dataset. \n",
    "\n",
    "Here in this section, the pretrained weights/parameters of the feature extraction layers of the ResNet will be frozen, thereby the only training will take place is the in the final dense layers for the CIFAR-100. The following will demonstrate how the pretrained weights are frozen.\n",
    "\n",
    "### 5.3.1 - Building the ResNet model:\n",
    "\n",
    "The ResNet-50 will be chosen here for the CIFAR-100 dataset. First, the model will be instantiated directly from Keras Applications with its pretrained weights without its final dense layers. This can be done by setting \"include_top\" parameter to be \"False\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Libraries:\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the ResNet-50 model:\n",
    "resnet50_featureExtractor = tf.keras.applications.resnet50.ResNet50(include_top=False, \n",
    "                                                                    weights='imagenet',  \n",
    "                                                                    input_shape=input_shape)\n",
    "\n",
    "# Model Summary:\n",
    "resnet50_featureExtractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 - Freeze the Feature Extractor Layers:\n",
    "\n",
    "To do this, means that the layers of the network will be \"non-trainable\" where it will maintain the learnt weights derived from the training process over the ImageNet dataset. Note that the feature extractor layers here also means the convolutional layers. \n",
    "\n",
    "It is wise to consider that not all the layers should be frozen, as the only layers that require freezing are just the convolutional ones. Noting the architecture of the model from previous notebooks, and to simplify which layers are to be frozen or not:\n",
    "- Convolutional layers = YES. (It is non-trainable).\n",
    "- Regularisation layers = NO. (It is trainable).\n",
    "- Batch-Normalisation layers = NO. (It is trainable).\n",
    " \n",
    "As these trainable layers are added after the convolutional layers, it does consist of some trainable parameters, in this case, these parameters have become too specific on the ImageNet dataset, therefore it is better to re-train them for the purpose of the CIFAR-100 dataset, so that they are able to adapt to the new task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Layers that require freezing or not:\n",
    "frozen_layers, trainable_layers = [], []\n",
    "\n",
    "# Loop over the layers of the model to update the list of layers above:\n",
    "for layer in resnet50_featureExtractor.layers:\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        layer.trainable = False\n",
    "        frozen_layers.append(layer.name)\n",
    "        \n",
    "    else:\n",
    "        if len(layer.trainable_weights) > 0:\n",
    "            trainable_layers.append(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging formats:\n",
    "log_begin_red, log_begin_blue, log_begin_green = '\\033[91m', '\\n\\033[94m', '\\033[92m'\n",
    "log_begin_bold, log_begin_underline = '\\033[1m', '\\033[4m'\n",
    "log_end_format = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mLayers that are Frozen:\u001b[0m ['conv1_conv', 'conv2_block1_1_conv', 'conv2_block1_2_conv', 'conv2_block1_0_conv', 'conv2_block1_3_conv', 'conv2_block2_1_conv', 'conv2_block2_2_conv', 'conv2_block2_3_conv', 'conv2_block3_1_conv', 'conv2_block3_2_conv', 'conv2_block3_3_conv', 'conv3_block1_1_conv', 'conv3_block1_2_conv', 'conv3_block1_0_conv', 'conv3_block1_3_conv', 'conv3_block2_1_conv', 'conv3_block2_2_conv', 'conv3_block2_3_conv', 'conv3_block3_1_conv', 'conv3_block3_2_conv', 'conv3_block3_3_conv', 'conv3_block4_1_conv', 'conv3_block4_2_conv', 'conv3_block4_3_conv', 'conv4_block1_1_conv', 'conv4_block1_2_conv', 'conv4_block1_0_conv', 'conv4_block1_3_conv', 'conv4_block2_1_conv', 'conv4_block2_2_conv', 'conv4_block2_3_conv', 'conv4_block3_1_conv', 'conv4_block3_2_conv', 'conv4_block3_3_conv', 'conv4_block4_1_conv', 'conv4_block4_2_conv', 'conv4_block4_3_conv', 'conv4_block5_1_conv', 'conv4_block5_2_conv', 'conv4_block5_3_conv', 'conv4_block6_1_conv', 'conv4_block6_2_conv', 'conv4_block6_3_conv', 'conv5_block1_1_conv', 'conv5_block1_2_conv', 'conv5_block1_0_conv', 'conv5_block1_3_conv', 'conv5_block2_1_conv', 'conv5_block2_2_conv', 'conv5_block2_3_conv', 'conv5_block3_1_conv', 'conv5_block3_2_conv', 'conv5_block3_3_conv'] (\u001b[1mtotal = 53\u001b[0m).\n",
      "\n",
      "\u001b[94mLayers that will be Fine-tuned:\u001b[0m ['conv1_bn', 'conv2_block1_1_bn', 'conv2_block1_2_bn', 'conv2_block1_0_bn', 'conv2_block1_3_bn', 'conv2_block2_1_bn', 'conv2_block2_2_bn', 'conv2_block2_3_bn', 'conv2_block3_1_bn', 'conv2_block3_2_bn', 'conv2_block3_3_bn', 'conv3_block1_1_bn', 'conv3_block1_2_bn', 'conv3_block1_0_bn', 'conv3_block1_3_bn', 'conv3_block2_1_bn', 'conv3_block2_2_bn', 'conv3_block2_3_bn', 'conv3_block3_1_bn', 'conv3_block3_2_bn', 'conv3_block3_3_bn', 'conv3_block4_1_bn', 'conv3_block4_2_bn', 'conv3_block4_3_bn', 'conv4_block1_1_bn', 'conv4_block1_2_bn', 'conv4_block1_0_bn', 'conv4_block1_3_bn', 'conv4_block2_1_bn', 'conv4_block2_2_bn', 'conv4_block2_3_bn', 'conv4_block3_1_bn', 'conv4_block3_2_bn', 'conv4_block3_3_bn', 'conv4_block4_1_bn', 'conv4_block4_2_bn', 'conv4_block4_3_bn', 'conv4_block5_1_bn', 'conv4_block5_2_bn', 'conv4_block5_3_bn', 'conv4_block6_1_bn', 'conv4_block6_2_bn', 'conv4_block6_3_bn', 'conv5_block1_1_bn', 'conv5_block1_2_bn', 'conv5_block1_0_bn', 'conv5_block1_3_bn', 'conv5_block2_1_bn', 'conv5_block2_2_bn', 'conv5_block2_3_bn', 'conv5_block3_1_bn', 'conv5_block3_2_bn', 'conv5_block3_3_bn'] (\u001b[1mtotal = 53\u001b[0m).\n"
     ]
    }
   ],
   "source": [
    "# Log the lists of both frozen and trainable alyers:\n",
    "\n",
    "# For FROZEN layers:\n",
    "print(\"{2}Layers that are Frozen:{4} {0} ({3}total = {1}{4}).\".format(\n",
    "    frozen_layers, len(frozen_layers), log_begin_red, log_begin_bold, log_end_format))\n",
    "\n",
    "# For TRAINABLE layers:\n",
    "print(\"{2}Layers that will be Fine-tuned:{4} {0} ({3}total = {1}{4}).\".format(\n",
    "    trainable_layers, len(trainable_layers), log_begin_blue, log_begin_bold, log_end_format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the above concludes that there are 53 layers in total for both trainable and frozen layers.\n",
    "\n",
    "#### Continue Building the model: \n",
    "\n",
    "Add the __\"trainable\" final dense layers__ to the ResNet model so that the model is able to output the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Output section of the ResNet model above:\n",
    "features = resnet50_featureExtractor.output\n",
    "\n",
    "# Apply the global average pooling:\n",
    "avg_pool = GlobalAveragePooling2D(data_format = 'channels_last')(features)\n",
    "\n",
    "# Add in the Final Dense Layers:\n",
    "predictions = Dense(nb_classes, \n",
    "                    activation='softmax',\n",
    "                   )(avg_pool)\n",
    "\n",
    "# Instantiate the COMPLETE MODEL:\n",
    "resnet50_freeze = Model(resnet50_featureExtractor.input, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 - Training the ResNet-50 Model:\n",
    "\n",
    "To do this, it is similar to previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Libaries:\n",
    "import collections\n",
    "import functools\n",
    "from customCallbacks_Keras import Simplified_LogCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation for Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model optimiser:\n",
    "optimiser = tf.keras.optimizers.SGD(momentum=0.9,\n",
    "                                    nesterov=True)\n",
    "\n",
    "# Define the accuracy metric:\n",
    "accuracy_metric = tf.metrics.SparseCategoricalAccuracy(name = 'acc')\n",
    "\n",
    "# Define the top-5 accuracy metric: a.k.a Top-K\n",
    "top5_acc_metric = tf.metrics.SparseTopKCategoricalAccuracy(k = 5,\n",
    "                                                           name = 'top5_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to log metrics at the end of each epoch: this saves space compared to Verbose = 1.\n",
    "metrics_to_print = collections.OrderedDict( [(\"loss\", \"loss\"), \n",
    "                                            (\"v-loss\", \"val_loss\"),\n",
    "                                            (\"acc\", \"acc\"), \n",
    "                                            (\"v-acc\", \"val_acc\"),\n",
    "                                            (\"top5-acc\", \"top5_acc\"), \n",
    "                                            (\"v-top5-acc\", \"val_top5_acc\")] \n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    }
   ],
   "source": [
    "# Define the model directtory:\n",
    "model_dir = '.\\models\\ResNet50_freezeAll_run1'\n",
    "\n",
    "# Define the Callbacks:\n",
    "callbacks = [\n",
    "    \n",
    "    # Callback to interrupt the training if the validation loss/metric stops imrpoving for some amount of epochs:\n",
    "    tf.keras.callbacks.EarlyStopping(monitor ='val_acc',\n",
    "                                    patience =8,\n",
    "                                    restore_best_weights = True\n",
    "                                    ),\n",
    "    \n",
    "    # Callback to log the graph, Losses and Metrics into TensorBoard:\n",
    "    tf.keras.callbacks.TensorBoard(log_dir = model_dir,\n",
    "                                    histogram_freq = 0,\n",
    "                                    write_graph = True\n",
    "                                  ),\n",
    "    \n",
    "    # Callback to save the model: at every 5 epochs and specifies the poch and val-loss in the filename.\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(model_dir, 'weights-epoch{epoch:02}-loss{val_loss:.2f}.h5'), period = 5\n",
    "                                    ), \n",
    "    \n",
    "    # Log the Callbacks:\n",
    "    Simplified_LogCallback(metrics_dict = metrics_to_print,\n",
    "                           nb_epochs = nb_epochs,\n",
    "                           log_frequency= 1\n",
    "                          )  \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model:\n",
    "resnet50_freeze.compile(optimizer = optimiser,\n",
    "                 loss = 'sparse_categorical_crossentropy',\n",
    "                 metrics = [accuracy_metric, top5_acc_metric]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the training process for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running time calculation\n",
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \u001b[91mstart\u001b[0m\n",
      "Epoch  0/300: \u001b[1mloss\u001b[0m = \u001b[94m1.990\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m4.854\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.483\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.143\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.765\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.376\u001b[0m\n",
      "Epoch  1/300: \u001b[1mloss\u001b[0m = \u001b[94m1.189\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.933\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.660\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.725\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.904\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.940\u001b[0m\n",
      "Epoch  2/300: \u001b[1mloss\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.770\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.709\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.768\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.931\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.958\u001b[0m\n",
      "Epoch  3/300: \u001b[1mloss\u001b[0m = \u001b[94m0.878\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.697\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.741\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.789\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.944\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.961\u001b[0m\n",
      "Epoch  4/300: \u001b[1mloss\u001b[0m = \u001b[94m0.792\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.645\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.763\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.803\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.954\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.968\u001b[0m\n",
      "Epoch  5/300: \u001b[1mloss\u001b[0m = \u001b[94m0.740\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.592\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.777\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.818\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.960\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.974\u001b[0m\n",
      "Epoch  6/300: \u001b[1mloss\u001b[0m = \u001b[94m0.679\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.546\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.794\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.830\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.966\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.977\u001b[0m\n",
      "Epoch  7/300: \u001b[1mloss\u001b[0m = \u001b[94m0.648\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.517\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.802\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.841\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.968\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.979\u001b[0m\n",
      "Epoch  8/300: \u001b[1mloss\u001b[0m = \u001b[94m0.614\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.498\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.811\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.849\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.972\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.980\u001b[0m\n",
      "Epoch  9/300: \u001b[1mloss\u001b[0m = \u001b[94m0.570\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.467\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.824\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.855\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.975\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.982\u001b[0m\n",
      "Epoch 10/300: \u001b[1mloss\u001b[0m = \u001b[94m0.560\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.451\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.827\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.860\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.976\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.985\u001b[0m\n",
      "Epoch 11/300: \u001b[1mloss\u001b[0m = \u001b[94m0.531\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.442\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.836\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.858\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.979\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.986\u001b[0m\n",
      "Epoch 12/300: \u001b[1mloss\u001b[0m = \u001b[94m0.515\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.393\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.840\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.876\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.980\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.989\u001b[0m\n",
      "Epoch 13/300: \u001b[1mloss\u001b[0m = \u001b[94m0.491\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.385\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.847\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.877\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.982\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.987\u001b[0m\n",
      "Epoch 14/300: \u001b[1mloss\u001b[0m = \u001b[94m0.477\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.368\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.851\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.883\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.982\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.990\u001b[0m\n",
      "Epoch 15/300: \u001b[1mloss\u001b[0m = \u001b[94m0.459\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.374\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.856\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.880\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.984\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.990\u001b[0m\n",
      "Epoch 16/300: \u001b[1mloss\u001b[0m = \u001b[94m0.448\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.343\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.859\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.893\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.985\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.991\u001b[0m\n",
      "Epoch 17/300: \u001b[1mloss\u001b[0m = \u001b[94m0.432\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.332\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.864\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.898\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.986\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.991\u001b[0m\n",
      "Epoch 18/300: \u001b[1mloss\u001b[0m = \u001b[94m0.415\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.314\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.869\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.905\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.987\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.994\u001b[0m\n",
      "Epoch 19/300: \u001b[1mloss\u001b[0m = \u001b[94m0.404\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.311\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.873\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.904\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.988\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.992\u001b[0m\n",
      "Epoch 20/300: \u001b[1mloss\u001b[0m = \u001b[94m0.394\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.304\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.877\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.901\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.989\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.994\u001b[0m\n",
      "Epoch 21/300: \u001b[1mloss\u001b[0m = \u001b[94m0.390\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.289\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.877\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.913\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.988\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.995\u001b[0m\n",
      "Epoch 22/300: \u001b[1mloss\u001b[0m = \u001b[94m0.377\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.280\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.880\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.914\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.990\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.993\u001b[0m\n",
      "Epoch 23/300: \u001b[1mloss\u001b[0m = \u001b[94m0.366\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.283\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.885\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.912\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.991\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.993\u001b[0m\n",
      "Epoch 24/300: \u001b[1mloss\u001b[0m = \u001b[94m0.354\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.271\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.888\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.916\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.991\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.994\u001b[0m\n",
      "Epoch 25/300: \u001b[1mloss\u001b[0m = \u001b[94m0.347\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.255\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.890\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.922\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.991\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.995\u001b[0m\n",
      "Epoch 26/300: \u001b[1mloss\u001b[0m = \u001b[94m0.339\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.252\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.892\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.920\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.991\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.995\u001b[0m\n",
      "Epoch 27/300: \u001b[1mloss\u001b[0m = \u001b[94m0.336\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.258\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.893\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.923\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.992\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.995\u001b[0m\n",
      "Epoch 28/300: \u001b[1mloss\u001b[0m = \u001b[94m0.323\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.243\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.897\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.927\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.992\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.996\u001b[0m\n",
      "Epoch 29/300: \u001b[1mloss\u001b[0m = \u001b[94m0.322\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.237\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.896\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.927\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.993\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.996\u001b[0m\n",
      "Epoch 30/300: \u001b[1mloss\u001b[0m = \u001b[94m0.319\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.228\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.898\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.931\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.992\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.997\u001b[0m\n",
      "Epoch 31/300: \u001b[1mloss\u001b[0m = \u001b[94m0.303\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.230\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.904\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.927\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.993\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.996\u001b[0m\n",
      "Epoch 32/300: \u001b[1mloss\u001b[0m = \u001b[94m0.301\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.224\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.905\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.931\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.993\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.996\u001b[0m\n",
      "Epoch 33/300: \u001b[1mloss\u001b[0m = \u001b[94m0.300\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.219\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.904\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.931\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.993\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.997\u001b[0m\n",
      "Epoch 34/300: \u001b[1mloss\u001b[0m = \u001b[94m0.287\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.218\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.909\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.932\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.994\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.997\u001b[0m\n",
      "Epoch 35/300: \u001b[1mloss\u001b[0m = \u001b[94m0.284\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.214\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.910\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.933\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.994\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 36/300: \u001b[1mloss\u001b[0m = \u001b[94m0.285\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.201\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.909\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.936\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.994\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 37/300: \u001b[1mloss\u001b[0m = \u001b[94m0.274\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.203\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.913\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.938\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.995\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.997\u001b[0m\n",
      "Epoch 38/300: \u001b[1mloss\u001b[0m = \u001b[94m0.274\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.191\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.914\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.942\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.995\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.997\u001b[0m\n",
      "Epoch 39/300: \u001b[1mloss\u001b[0m = \u001b[94m0.272\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.196\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.912\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.940\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.995\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.997\u001b[0m\n",
      "Epoch 40/300: \u001b[1mloss\u001b[0m = \u001b[94m0.264\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.194\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.916\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.941\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.995\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 41/300: \u001b[1mloss\u001b[0m = \u001b[94m0.262\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.195\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.917\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.940\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.997\u001b[0m\n",
      "Epoch 42/300: \u001b[1mloss\u001b[0m = \u001b[94m0.256\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.189\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.919\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.939\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.995\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.997\u001b[0m\n",
      "Epoch 43/300: \u001b[1mloss\u001b[0m = \u001b[94m0.252\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.176\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.920\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.945\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 44/300: \u001b[1mloss\u001b[0m = \u001b[94m0.249\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.171\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.920\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.946\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 45/300: \u001b[1mloss\u001b[0m = \u001b[94m0.246\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.172\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.921\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.945\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 46/300: \u001b[1mloss\u001b[0m = \u001b[94m0.243\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.171\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.921\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.948\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 47/300: \u001b[1mloss\u001b[0m = \u001b[94m0.236\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.171\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.925\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.947\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 48/300: \u001b[1mloss\u001b[0m = \u001b[94m0.231\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.160\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.927\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.952\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 49/300: \u001b[1mloss\u001b[0m = \u001b[94m0.235\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.162\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.926\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.950\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 50/300: \u001b[1mloss\u001b[0m = \u001b[94m0.229\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.161\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.927\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.951\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 51/300: \u001b[1mloss\u001b[0m = \u001b[94m0.226\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.168\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.929\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.950\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 52/300: \u001b[1mloss\u001b[0m = \u001b[94m0.224\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.159\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.928\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.952\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 53/300: \u001b[1mloss\u001b[0m = \u001b[94m0.222\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.153\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.930\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.953\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.996\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 54/300: \u001b[1mloss\u001b[0m = \u001b[94m0.220\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.158\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.930\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.950\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 55/300: \u001b[1mloss\u001b[0m = \u001b[94m0.217\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.147\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.932\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.955\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 56/300: \u001b[1mloss\u001b[0m = \u001b[94m0.215\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.140\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.932\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.957\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 57/300: \u001b[1mloss\u001b[0m = \u001b[94m0.206\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.146\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.935\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.956\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 58/300: \u001b[1mloss\u001b[0m = \u001b[94m0.209\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.140\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.934\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.959\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 59/300: \u001b[1mloss\u001b[0m = \u001b[94m0.202\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.150\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.937\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.956\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 60/300: \u001b[1mloss\u001b[0m = \u001b[94m0.203\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.134\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.936\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.960\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 61/300: \u001b[1mloss\u001b[0m = \u001b[94m0.197\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.141\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.937\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.957\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 62/300: \u001b[1mloss\u001b[0m = \u001b[94m0.201\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.136\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.937\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.958\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 63/300: \u001b[1mloss\u001b[0m = \u001b[94m0.195\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.134\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.938\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.959\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 64/300: \u001b[1mloss\u001b[0m = \u001b[94m0.190\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.131\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.939\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.961\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 65/300: \u001b[1mloss\u001b[0m = \u001b[94m0.196\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.134\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.939\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.961\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 66/300: \u001b[1mloss\u001b[0m = \u001b[94m0.188\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.133\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.941\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.961\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 67/300: \u001b[1mloss\u001b[0m = \u001b[94m0.189\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.131\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.940\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.961\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 68/300: \u001b[1mloss\u001b[0m = \u001b[94m0.183\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.127\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.942\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.964\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 69/300: \u001b[1mloss\u001b[0m = \u001b[94m0.190\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.131\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.940\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.959\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 70/300: \u001b[1mloss\u001b[0m = \u001b[94m0.185\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.122\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.942\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.966\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 71/300: \u001b[1mloss\u001b[0m = \u001b[94m0.181\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.117\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.944\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.965\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 72/300: \u001b[1mloss\u001b[0m = \u001b[94m0.180\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.132\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.943\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.959\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.998\u001b[0m\n",
      "Epoch 73/300: \u001b[1mloss\u001b[0m = \u001b[94m0.178\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.123\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.944\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.961\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 74/300: \u001b[1mloss\u001b[0m = \u001b[94m0.177\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.125\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.944\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.963\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 75/300: \u001b[1mloss\u001b[0m = \u001b[94m0.174\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.116\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.945\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.967\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.997\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 76/300: \u001b[1mloss\u001b[0m = \u001b[94m0.173\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.117\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.945\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.965\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 77/300: \u001b[1mloss\u001b[0m = \u001b[94m0.173\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.116\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.945\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.967\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 78/300: \u001b[1mloss\u001b[0m = \u001b[94m0.169\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.111\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.948\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.967\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 79/300: \u001b[1mloss\u001b[0m = \u001b[94m0.169\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.109\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.946\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.968\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 80/300: \u001b[1mloss\u001b[0m = \u001b[94m0.171\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.107\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.947\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.969\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 81/300: \u001b[1mloss\u001b[0m = \u001b[94m0.169\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.114\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.948\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.966\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 82/300: \u001b[1mloss\u001b[0m = \u001b[94m0.167\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.104\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.948\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.971\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 83/300: \u001b[1mloss\u001b[0m = \u001b[94m0.167\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.107\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.948\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.969\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 84/300: \u001b[1mloss\u001b[0m = \u001b[94m0.164\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.106\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.947\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.969\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 85/300: \u001b[1mloss\u001b[0m = \u001b[94m0.163\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.109\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.948\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.970\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 86/300: \u001b[1mloss\u001b[0m = \u001b[94m0.162\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.109\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.949\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.967\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 87/300: \u001b[1mloss\u001b[0m = \u001b[94m0.154\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.113\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.952\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.966\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 88/300: \u001b[1mloss\u001b[0m = \u001b[94m0.160\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.095\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.950\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.973\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 89/300: \u001b[1mloss\u001b[0m = \u001b[94m0.154\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.107\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.952\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.970\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 90/300: \u001b[1mloss\u001b[0m = \u001b[94m0.158\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.101\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.951\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.974\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 91/300: \u001b[1mloss\u001b[0m = \u001b[94m0.152\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.099\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.952\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.971\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 92/300: \u001b[1mloss\u001b[0m = \u001b[94m0.150\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.100\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.953\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.970\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 93/300: \u001b[1mloss\u001b[0m = \u001b[94m0.151\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.098\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.953\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.972\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 94/300: \u001b[1mloss\u001b[0m = \u001b[94m0.155\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.096\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.950\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.973\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 95/300: \u001b[1mloss\u001b[0m = \u001b[94m0.150\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.105\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.953\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.968\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 96/300: \u001b[1mloss\u001b[0m = \u001b[94m0.150\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.098\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.952\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.973\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 97/300: \u001b[1mloss\u001b[0m = \u001b[94m0.152\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.098\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.952\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.972\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m0.999\u001b[0m\n",
      "Epoch 98/300: \u001b[1mloss\u001b[0m = \u001b[94m0.150\u001b[0m; \u001b[1mv-loss\u001b[0m = \u001b[94m0.094\u001b[0m; \u001b[1macc\u001b[0m = \u001b[94m0.953\u001b[0m; \u001b[1mv-acc\u001b[0m = \u001b[94m0.972\u001b[0m; \u001b[1mtop5-acc\u001b[0m = \u001b[94m0.998\u001b[0m; \u001b[1mv-top5-acc\u001b[0m = \u001b[94m1.000\u001b[0m\n",
      "Training: \u001b[92mend\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "history_resnet50_freeze = resnet50_freeze.fit(train_set_cifar,\n",
    "                                              epochs = nb_epochs,\n",
    "                                              steps_per_epoch= train_steps_per_epoch,\n",
    "                                              validation_data= (val_set_cifar),\n",
    "                                              validation_steps= valid_steps_per_epoch,\n",
    "                                              verbose=0,\n",
    "                                              callbacks= callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 482.09 Minutes\n",
      "Time: 8.03 hours\n"
     ]
    }
   ],
   "source": [
    "# Stop the timer:\n",
    "stop = timeit.default_timer()\n",
    "print('Time: {} Minutes'.format(round((stop - start)/60, 2)))\n",
    "print('Time: {} hours'.format(round((stop - start)/3600, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "\n",
    "## 6 - Model Performance:\n",
    "\n",
    "## 6.1 - Inspect TensorBoard:\n",
    "\n",
    "These can also be found in TensorBoard with the input in terminal: \"tensorboard --logdir ./models\" or in this case, \"tensorboard --logdir .\\models\\ResNet50_freezeAll_run1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tensorboard screenshot - joint results](./Description\\ Images/ResNet50_freezeAll_run1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - Plot of the Model's Performance:\n",
    "\n",
    "NOTE: the \"history\" object returned from the \"model.fit()\" method also provides data for plotting the training metrics. The following till plot these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x262ff00be88>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 10), sharex='col') # add parameter `sharey='row'` for a more direct comparison\n",
    "ax[0, 0].set_title(\"loss\")\n",
    "ax[0, 1].set_title(\"val-loss\")\n",
    "ax[1, 0].set_title(\"acc\")\n",
    "ax[1, 1].set_title(\"val-acc\")\n",
    "ax[2, 0].set_title(\"top5-acc\")\n",
    "ax[2, 1].set_title(\"val-top5-acc\")\n",
    "\n",
    "ax[0, 0].plot(history_resnet50_freeze.history['loss'])\n",
    "ax[0, 1].plot(history_resnet50_freeze.history['val_loss'])\n",
    "ax[1, 0].plot(history_resnet50_freeze.history['acc'])\n",
    "ax[1, 1].plot(history_resnet50_freeze.history['val_acc'])\n",
    "ax[2, 0].plot(history_resnet50_freeze.history['top5_acc'])\n",
    "ax[2, 1].plot(history_resnet50_freeze.history['val_top5_acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 97.38%\n",
      "Best Validation Top 5 Accuracy: 99.96%\n"
     ]
    }
   ],
   "source": [
    "# Get the best performing scores:\n",
    "best_val_accuracy = max(history_resnet50_freeze.history['val_acc']) * 100\n",
    "best_val_top5accuracy = max(history_resnet50_freeze.history['val_top5_acc']) * 100\n",
    "\n",
    "# Print out the results:\n",
    "print(\"Best Validation Accuracy: {:2.2f}%\".format(best_val_accuracy))\n",
    "print(\"Best Validation Top 5 Accuracy: {:2.2f}%\".format(best_val_top5accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "## 7 - Fine-Tuning the Resnet Feature Extractor:\n",
    "\n",
    "As mentioned earlier in this notebook, fine-tuning is advantageous when the training dataset can be said to be big enough where the model won't overfit it. In this section, the ResNet-50 model and its pretrained weights/parameters (layers) will be instantiated again. However, a complete freezing of the feature extractor layers __will not__ be done for the purpose of fine tuning the deeper (latest) higher-level convolutional layers. \n",
    "\n",
    "## 7.1 - Selectively Freezing Layers of the model:\n",
    "\n",
    "With what was previously mentioned, the higher-level convolutional layers are in the deeper layers of the model, this means that the weights of the first few layers will be fixed (frozen) while the remainding layers will undergo the training phase. \n",
    "\n",
    "The following is an example code to demonstrate the logic behind selective freezing. For example here, the weights of the 3 first macro-blocks (these macro blocks were mentioned in Notebook 2 Section 3.2.5) will be fixed, where the rest of the layers will be fine tuned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in resnest50_featureExtractor.layers:\n",
    "    if 'res5' in layer.names:\n",
    "        # The devs of Keras have explicitly named the layers in the ResNet model, \n",
    "        # to identify which macro-block (and blocks) in each layer belongs to.\n",
    "        # Here, if the loop reaches the layer named 'res5' for 'resnet5', it would mean that\n",
    "        # the fourth macro-block was reached, passing the 3 first macro-blocks. \n",
    "        break\n",
    "    \n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 - Meta-Iterating to discover the best Transfer Learning Solution:\n",
    "\n",
    "It should be noted that in practice, __the decision in which layers will be fine-tuned or not, will require several iterations of training__. This is to compare the performance of each instantiaed model in each loop. \n",
    "\n",
    "This section will dive into the testing of different __Macro-Configurations__, where the freezing of a variable number of layers will be conducted, then the performance of the resulting network will be evaluated during the training phase. The exercise here will go through freezing macro-blocks from 0 to all 4 of them in the ResNet feeature extractor model.\n",
    "\n",
    "__Warning:__ This part of will or may take up to days of training. It is recommended to run these in parallel. Also note that \"ModelCheckpoint()\" callback can be used to restore the models if the training process were to be interrupted. This can be implemented by the following: \"reset50_finetine.load_weights(filename_latest_checkpoint)\" before launching the training phase again. Further, the Keras logs will be switched off for this section as it can overload the print out and make the notebook unreadable.\n",
    "\n",
    "#### Number of macro blocks to freeze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the number of macro blocks to freeze:\n",
    "nb_macroblocks_to_freeze = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta-Iterating of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = dict()\n",
    "histories['freeze all'] = history_freeze\n",
    "\n",
    "runing_total_time = 0\n",
    "\n",
    "# Loop over the training with different macro blocks:\n",
    "for freeze_nb in nb_macroblocks_to_freeze:\n",
    "    \n",
    "    print('{1}{2} >> {3}ResNet-50 with {0} macro-blocks(s) frozen{4}:'.format(\n",
    "        freeze_nb, log_begin_green, log_begin_bold, log_begin_underline, log_end_format))\n",
    "    \n",
    "    # STAGE 1 ---------------------------------------------------------------------------\n",
    "    # Instantiate a new ResNet model (classifier) each time:\n",
    "    resnet50_feature_extractor = tf.keras.applications.resnet50.Resnet50(include_top = False,\n",
    "                                                                         weights = 'imagenet',\n",
    "                                                                         input_shape = input_shape,\n",
    "                                                                         classes = nb_classes)\n",
    "    \n",
    "    # Add the final prediction layers:\n",
    "    features = resnet50_feature_extractor.ouput\n",
    "    avg_pool = GlobalAveragePooling2D(data_format = 'channels_last')(features)\n",
    "    predictions = Dense(nb_classes, \n",
    "                        activation='softmax',\n",
    "                       )(avg_pool)\n",
    "    \n",
    "    # Complete the model instantiation:\n",
    "    resnet50_finetune = Model(resnet50_feature_extractor.input, predictions)\n",
    "    \n",
    "    # STAGE 2 ---------------------------------------------------------------------------\n",
    "    # Freeze the desired layers:\n",
    "    break_layer_name = 'res{}'.format(freeze_nb + 2) if freeze_nb > 0 else 'conv1'\n",
    "    \n",
    "    frozen_layers = []\n",
    "    \n",
    "    # Loop over the layers to determine which is frozen or trainable:\n",
    "    for layer in resnet50_finetune.layers:\n",
    "        if break_layer_name in layer.name:\n",
    "            break\n",
    "        \n",
    "        if isinstance(layer,, tf.keras.layers.Conv2D):\n",
    "            # Check if the layer is a Convolution, and that it isn't after the 1st layer, so that it is not trained.\n",
    "            layer.trainable = False\n",
    "            frozen_layers.append(layer.name)\n",
    "            \n",
    "    print('\\t> {2}Layers frozen:{4} {0} ({3}total = {1}{4}).'.format(\n",
    "        frozen_layers, len(frozen_layers), log_begin_red, log_begin_bold, log_end_format))\n",
    "    \n",
    "    # STAGE 3 ---------------------------------------------------------------------------\n",
    "    # Reinstantiate the Input Pipelines: Begining with the data iteration.  \n",
    "    # Training Dataset:\n",
    "    train_set_cifar = DataPrepCIFAR_utility.get_dataset(phase='train',\n",
    "                                                        batch_size= batch_size,\n",
    "                                                        nb_epochs= nb_epochs,\n",
    "                                                        shuffle=True,\n",
    "                                                        input_shape= input_shape,\n",
    "                                                        seed= Seed_nb)\n",
    "\n",
    "    # Validation Dataset:\n",
    "    val_set_cifar = DataPrepCIFAR_utility.get_dataset(phase='test',\n",
    "                                                      batch_size= batch_size,\n",
    "                                                      nb_epochs= 1,\n",
    "                                                      shuffle= False,\n",
    "                                                      input_shape= input_shape,\n",
    "                                                      seed= Seed_nb)\n",
    "    \n",
    "    # STAGE 4 ---------------------------------------------------------------------------\n",
    "    # Set up the training process:\n",
    "    \n",
    "    # Define the model optimiser:\n",
    "    optimiser = tf.keras.optimizers.SGD(momentum=0.9,\n",
    "                                        nesterov=True)\n",
    "\n",
    "    # Define the accuracy metric:\n",
    "    accuracy_metric = tf.metrics.SparseCategoricalAccuracy(name = 'acc')\n",
    "\n",
    "    # Define the top-5 accuracy metric: a.k.a Top-K\n",
    "    top5_acc_metric = tf.metrics.SparseTopKCategoricalAccuracy(k = 5,\n",
    "                                                               name = 'top5_acc')\n",
    "    \n",
    "    \n",
    "    # Define the model directtory:\n",
    "    model_dir = '.\\models\\ResNet50_freeze_{}_macroblock'.format(freeze_nb)\n",
    "\n",
    "    # Define the Callbacks:\n",
    "    callbacks = [\n",
    "\n",
    "        # Callback to interrupt the training if the validation loss/metric stops imrpoving for some amount of epochs:\n",
    "        tf.keras.callbacks.EarlyStopping(monitor ='val_acc',\n",
    "                                        patience =8,\n",
    "                                        restore_best_weights = True\n",
    "                                        ),\n",
    "\n",
    "        # Callback to log the graph, Losses and Metrics into TensorBoard:\n",
    "        tf.keras.callbacks.TensorBoard(log_dir = model_dir,\n",
    "                                        histogram_freq = 0,\n",
    "                                        write_graph = True\n",
    "                                      ),\n",
    "\n",
    "        # Callback to save the model: at every 5 epochs and specifies the poch and val-loss in the filename.\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(model_dir, 'weights-epoch{epoch:02}-loss{val_loss:.2f}.h5'), period = 5\n",
    "                                        )\n",
    "\n",
    "    ]\n",
    "    \n",
    "    # Compile the Model:\n",
    "    resnet50_finetune.compile(optimizer = optimiser,\n",
    "                              loss = 'sparse_categorical_crossentropy',\n",
    "                              metrics = [accuracy_metric, top5_acc_metric]\n",
    "                             )\n",
    "    \n",
    "    # Train the model:\n",
    "    print('\\t> Training - {0}start{1} (logs = off)'.format(log_begin_red, log_end_format))\n",
    "    \n",
    "    # running time calculation\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    # Run the model through the training:\n",
    "    history_resnet50_finetune = resnet50_finetune.fit(train_set_cifar,\n",
    "                                                      epochs = nb_epochs,\n",
    "                                                      steps_per_epoch= train_steps_per_epoch,\n",
    "                                                      validation_data= (val_set_cifar),\n",
    "                                                      validation_steps= valid_steps_per_epoch,\n",
    "                                                      verbose=0,\n",
    "                                                      callbacks= callbacks)\n",
    "    \n",
    "    print('\\t> Training - {0}over{1}'.format(log_begin_green, log_end_format))\n",
    "    \n",
    "    # Stop the timer:\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time: {} Minutes'.format(round((stop - start)/60, 2)))\n",
    "    print('Time: {} hours'.format(round((stop - start)/3600, 2)))\n",
    "    \n",
    "    # Save to total time taken:\n",
    "    runing_total_time += round((stop - start)/3600, 2))\n",
    "    \n",
    "    print('Current Running Time Taken: {}'.format(runing_total_time))\n",
    "    \n",
    "    # STAGE 5 ---------------------------------------------------------------------------\n",
    "    # Print the results/metrics of each of the Tested Models:\n",
    "    \n",
    "    acc = history_resnet50_finetune.history['acc'][-1] * 100\n",
    "    top5 = history_resnet50_finetune.history['top5_acc'][-1] * 100\n",
    "    val_acc = history_resnet50_finetune.history['val_acc'][-1] * 100\n",
    "    val_top5 =history_resnet50_finetune.history['val_top5_acc'][-1] * 100\n",
    "    epochs = len(history_resnet50_finetune.history['val_loss'])\n",
    "    \n",
    "    print(\"\\t> Results after {5}{0}{6} epochs:\\t{5}acc = {1:.2f}%; top5 = {2:.2f}%; val_acc = {3:.2f}%; val_top5 = {4:.2f}%{6}\".format(\n",
    "        epochs, acc, top5, val_acc, val_top5, log_begin_bold, log_end_format))\n",
    "    \n",
    "    # STAGE 6 ---------------------------------------------------------------------------\n",
    "    # Update the Histories Dict:\n",
    "    histories['freeze {}'.format(freeze_num)] = history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Inspect TensorBoard:\n",
    "\n",
    "These can also be found in TensorBoard with the input in terminal: \"tensorboard --logdir ./models\" or in this case, \"tensorboard --logdir .\\models\\ResNet50_freeze_/////////NUMBER//////_macroblock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Tensorboard screenshot - joint results](./Description\\ Images/ResNet50_freezeAll_run1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Plot of the Model's Performance:\n",
    "\n",
    "NOTE: the \"history\" object returned from the \"model.fit()\" method also provides data for plotting the training metrics. The following till plot these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 10), sharex='col') # add parameter `sharey='row'` for a more direct comparison\n",
    "ax[0, 0].set_title(\"loss\")\n",
    "ax[0, 1].set_title(\"val-loss\")\n",
    "ax[1, 0].set_title(\"acc\")\n",
    "ax[1, 1].set_title(\"val-acc\")\n",
    "ax[2, 0].set_title(\"top5-acc\")\n",
    "ax[2, 1].set_title(\"val-top5-acc\")\n",
    "\n",
    "lines, labels = [], []\n",
    "\n",
    "for config_name in histories:\n",
    "    history = histories[config_name]\n",
    "\n",
    "    ax[0, 0].plot(history.history['loss'])\n",
    "    ax[0, 1].plot(history.history['val_loss'])\n",
    "    ax[1, 0].plot(history.history['acc'])\n",
    "    ax[1, 1].plot(history.history['val_acc'])\n",
    "    ax[2, 0].plot(history.history['top5_acc'])\n",
    "    ax[2, 1].plot(history.history['val_top5_acc'])\n",
    "    \n",
    "    line = ax[2, 1].plot(history.history['val_top5_acc'])\n",
    "    lines.append(line[0])\n",
    "    labels.append(config_name)\n",
    "    \n",
    "fig.legend(lines, labels, loc='center right', borderaxespad = 0.1)\n",
    "plt.subplots_adjust(right=0.87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing scores:\n",
    "best_val_accuracy = max(history_resnet50_freeze.history['val_acc']) * 100\n",
    "best_val_top5accuracy = max(history_resnet50_freeze.history['val_top5_acc']) * 100\n",
    "\n",
    "# Print out the results:\n",
    "print(\"Best Validation Accuracy: {:2.2f}%\".format(best_val_accuracy))\n",
    "print(\"Best Validation Top 5 Accuracy: {:2.2f}%\".format(best_val_top5accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "\n",
    "## 10 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "\n",
    "\n",
    "## Summary:\n",
    "\n",
    "Although this is not the end of this project, it does conclude the 1st notebook relevant to the theory of these advanced deep learning models. Please go to check out __Notebook 7__ for the ____."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
