{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Influential Classification Models (and Tools)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Notebook (2) will continue on from the previous sections. This notebook will go through the process of building the __ResNet__ from scratch. \n",
    "\n",
    "## Dataset:\n",
    "\n",
    "For this part of the project, the CIFAR-100 dataset will be used, it is a collection of of 60,000 32x32 images that have 100 classes. CIFAR-100 was originally collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. It iss also a subset of the 80 million tiny images dataset. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses.\n",
    "\n",
    "Source: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Further, the TensorFlow team offers a python package called \"tensorflow_datasets\" that provides the helper function to download tthis dataset as well as other more common ones. For the purposes of this project, the CIFAR-100 dataset will be download with this package.\n",
    "\n",
    "Source: https://www.tensorflow.org/datasets/catalog/cifar100\n",
    "\n",
    "## Requirements:\n",
    "- Tensorflow 2.0 (GPU is better)\n",
    "- Tensorflow-Hub\n",
    "- Keras (GPU is better)\n",
    "\n",
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 9372556792854543533),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 6588305899, 5947880554449152520)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Preparation:\n",
    "\n",
    "To use the TensorFlow package, please ensure to install it: \"pip install tensorflow-datasets\" or use the Anaconda Navigator.\n",
    "\n",
    "## 1.1 - Download the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'aflw2k3d',\n",
       " 'amazon_us_reviews',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bigearthnet',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_corrupted',\n",
       " 'clevr',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco2014',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'downsampled_imagenet',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'dummy_dataset_shared_generator',\n",
       " 'dummy_mnist',\n",
       " 'emnist',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'gap',\n",
       " 'glue',\n",
       " 'groove',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'image_label_folder',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imdb_reviews',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lfw',\n",
       " 'lm1b',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'moving_mnist',\n",
       " 'multi_nli',\n",
       " 'nsynth',\n",
       " 'omniglot',\n",
       " 'open_images_v4',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'pet_finder',\n",
       " 'quickdraw_bitmap',\n",
       " 'resisc45',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 'scene_parse150',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tf_flowers',\n",
       " 'titanic',\n",
       " 'trivia_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc2007',\n",
       " 'wikipedia',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'xnli']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the different types of datasets available:\n",
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='cifar100',\n",
      "    version=1.3.1,\n",
      "    description='This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).',\n",
      "    urls=['https://www.cs.toronto.edu/~kriz/cifar.html'],\n",
      "    features=FeaturesDict({\n",
      "        'coarse_label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=100),\n",
      "    }),\n",
      "    total_num_examples=60000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 50000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the CIFAR-100 dataset:\n",
    "cifar_builder = tfds.builder(\"cifar100\")\n",
    "cifar_builder.download_and_prepare()\n",
    "\n",
    "print(cifar_builder.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above printed text, it also shows some useful information such as the number of training and testing samples, the key-names and so on.\n",
    "\n",
    "## 1.2 - Examine the class labels of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
      "The number of classes are: 100\n"
     ]
    }
   ],
   "source": [
    "# Check out the labels:\n",
    "print(cifar_builder.info.features[\"label\"].names)\n",
    "\n",
    "print('The number of classes are: {}'.format(len(cifar_builder.info.features[\"label\"].names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_and_vegetables', 'household_electrical_devices', 'household_furniture', 'insects', 'large_carnivores', 'large_man-made_outdoor_things', 'large_natural_outdoor_scenes', 'large_omnivores_and_herbivores', 'medium_mammals', 'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals', 'trees', 'vehicles_1', 'vehicles_2']\n",
      "The number of classes are: 20\n"
     ]
    }
   ],
   "source": [
    "# Check out the coarse labels:\n",
    "print(cifar_builder.info.features[\"coarse_label\"].names)\n",
    "\n",
    "print('The number of classes are: {}'.format(len(cifar_builder.info.features[\"coarse_label\"].names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Define the Input Pipeline:\n",
    "\n",
    "As the dataset have been downloaded with the help of the package, this section will then define the input pipeline for the models to be trained. These are variables that will be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hyperparameters:\n",
    "input_shape = [224, 224, 3]\n",
    "batch_size = 512\n",
    "nb_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# Divide the dataset into training and validation sets:\n",
    "train_set_cifar = cifar_builder.as_dataset(split = tfds.Split.TRAIN)\n",
    "test_set_cifar = cifar_builder.as_dataset(split = tfds.Split.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of classes:\n",
    "nb_classes = cifar_builder.info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of images for training and validation:\n",
    "nb_train_imgs = cifar_builder.info.splits['train'].num_examples\n",
    "nb_valid_imgs = cifar_builder.info.splits['test'].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Check out the dataset instances:\n",
    "\n",
    "This should provide some additional details about the dataset, such as its dtype and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training set instance: <_OptionsDataset shapes: {coarse_label: (), image: (32, 32, 3), label: ()}, types: {coarse_label: tf.int64, image: tf.uint8, label: tf.int64}>\n"
     ]
    }
   ],
   "source": [
    "print(\"The Training set instance: {}\".format(train_set_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Testing set instance: <_OptionsDataset shapes: {coarse_label: (), image: (32, 32, 3), label: ()}, types: {coarse_label: tf.int64, image: tf.uint8, label: tf.int64}>\n"
     ]
    }
   ],
   "source": [
    "print(\"The Testing set instance: {}\".format(test_set_cifar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Preprocessing:\n",
    "\n",
    "## 2.1 - Shuffling the dataset:\n",
    "\n",
    "The notion behind shuffling the dataset before being used for training is so that the model is able to learn from majority of the data in the training set, where unshuffled, the training data can be in the format order of A-R while the testing set is S-Z, so during training the model only learns from what is available from A-R but will later perform badly in the unseen testing data of S-Z. Shuffling, as mentioned, allows the model to train in a more robust manner resulting in a better generalised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the Training Dataset: 10,000 times of reshuffling.\n",
    "train_set_cifar = train_set_cifar.repeat(nb_epochs).shuffle(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Set up the dataset for compatibility with Keras API (non-estimator) and Image Augmentation:\n",
    "\n",
    "It should be noted that Tensorflow-Dataset returns batches as feature dictionaries, which is expected by Estimators. Therefore, to train with Keras models, it is better to return the batch content as tuples. A user defined funtion will be needed to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_data_func(features, input_shape, augment=False):\n",
    "    \"\"\" This builds a pre-processing function to resize the image into the expected dimenions and allows for an\n",
    "        optional transformation of the images such as augmentations.\n",
    "    Parameters:\n",
    "        - features, is the input data.\n",
    "        - input_shape, is the expected shape for model by resizing.\n",
    "        - augment, is a Flag to apply random transformations to the input images.\n",
    "    Returns:\n",
    "        - returns Augmented images, Labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the input images into tensors:\n",
    "    input_shape = tf.convert_to_tensor(input_shape)\n",
    "    \n",
    "    # Convert TF-dataset feature dictionaries into Tuples:\n",
    "    image = features['image']\n",
    "    label = features['label']\n",
    "    \n",
    "    # Convert the image data type to \"float32\" and normalise the data to a range between \"0 and 1\":\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # Data Image Augmentation:\n",
    "    if augment:\n",
    "        # Apply random horizontal flip::\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "        # Apply Brightness and Saturation changes:\n",
    "        image = tf.image.random_brightness(image, max_delta = 0.1)\n",
    "        image = tf.image.random_saturation(image, lower = 0.5, upper = 1.5)\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0) # this keeps the pixel values in check.\n",
    "        \n",
    "        # Apply random resizing and cropping back to expected image sizes:\n",
    "        random_scale_factor = tf.random.uniform([1], minval = 1., maxval = 1.4, dtype = tf.float32)\n",
    "        \n",
    "        scaled_height = tf.cast(tf.cast(input_shape[0], tf.float32) * random_scale_factor, tf.int32)\n",
    "        scaled_width = tf.cast(tf.cast(input_shape[1], tf.float32) * random_scale_factor, tf.int32)\n",
    "        scaled_shape = tf.squeeze(tf.stack([scaled_height, scaled_width]))\n",
    "        \n",
    "        image = tf.image.resize(image, scaled_shape)\n",
    "        image = tf.image.random_crop(image, input_shape)\n",
    "    else:\n",
    "        image = tf.image.resize(image, input_shape[:2])\n",
    "        \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the function above on the training and validation set:\n",
    "\n",
    "NOTE: functools.partial example -> https://stackoverflow.com/questions/15331726/how-does-functools-partial-do-what-it-does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the function for the training data preparation:\n",
    "prepare_data_func_for_trainSet = functools.partial(_prepare_data_func, input_shape = input_shape, augment = True)\n",
    "\n",
    "# Map the function to the dataset:\n",
    "train_set_cifar = train_set_cifar.map(prepare_data_func_for_trainSet, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Split the training set into batched samples:\n",
    "train_set_cifar = train_set_cifar.batch(batch_size)\n",
    "\n",
    "# Set to prefetch the data: improves the perforamnce.\n",
    "# his allows later elements to be prepared while the current element is being processed. \n",
    "# This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "train_set_cifar= train_set_cifar.prefetch(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Validation set: no augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the function for the Validation data preparation:\n",
    "prepare_data_func_for_ValidSet = functools.partial(_prepare_data_func, input_shape = input_shape, augment = False)\n",
    "\n",
    "# Map the function to the dataset:\n",
    "test_set_cifar = test_set_cifar.map(prepare_data_func_for_ValidSet, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Split the Validation set into batched samples:\n",
    "test_set_cifar = test_set_cifar.batch(batch_size)\n",
    "\n",
    "# Set to prefetch the data: improves the perforamnce.\n",
    "# his allows later elements to be prepared while the current element is being processed. \n",
    "# This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "test_set_cifar= test_set_cifar.prefetch(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Set the \"steps_per_epoch\" for Training and Validation Set:\n",
    "\n",
    "From the previous code block, the dataset is compatible with Keras methods liike \"model.fit()\", but will also require the \"steps_per_epoch\" to be specified. This is the number of batches per epoch of the Dataset objects that has to be specified for the Keras method so that it can work properly together. This is done for the training batches (\"steps_per_epoch\") and validation batches (\"validation_steps\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = math.ceil(nb_train_imgs / batch_size)\n",
    "\n",
    "valid_steps_per_epoch = math.ceil(nb_valid_imgs / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data preparation function will be placed into a \".py\" utily file, so that it can be reuse later on.\n",
    "\n",
    "## 3 - ResNet Implementation with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "Although this is not the end of this project, it does conclude the 1st notebook relevant to the theory of these advanced deep learning models. Please go to check out __Notebook 3__ for the __Keras Applications and reusing models__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
