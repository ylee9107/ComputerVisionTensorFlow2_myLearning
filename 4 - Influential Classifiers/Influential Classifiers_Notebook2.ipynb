{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Influential Classification Models (and Tools)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Notebook (2) will continue on from the previous sections. This notebook will go through the process of building the __ResNet__ from scratch. \n",
    "\n",
    "## Dataset:\n",
    "\n",
    "For this part of the project, the CIFAR-100 dataset will be used, it is a collection of of 60,000 32x32 images that have 100 classes. CIFAR-100 was originally collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. It iss also a subset of the 80 million tiny images dataset. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses.\n",
    "\n",
    "Source: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Further, the TensorFlow team offers a python package called \"tensorflow_datasets\" that provides the helper function to download tthis dataset as well as other more common ones. For the purposes of this project, the CIFAR-100 dataset will be download with this package.\n",
    "\n",
    "Source: https://www.tensorflow.org/datasets/catalog/cifar100\n",
    "\n",
    "## Requirements:\n",
    "- Tensorflow 2.0 (GPU is better)\n",
    "- Tensorflow-Hub\n",
    "- Keras (GPU is better)\n",
    "\n",
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 9372556792854543533),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 6588305899, 5947880554449152520)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Preparation:\n",
    "\n",
    "To use the TensorFlow package, please ensure to install it: \"pip install tensorflow-datasets\" or use the Anaconda Navigator.\n",
    "\n",
    "## 1.1 - Download the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'aflw2k3d',\n",
       " 'amazon_us_reviews',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bigearthnet',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_corrupted',\n",
       " 'clevr',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco2014',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'downsampled_imagenet',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'dummy_dataset_shared_generator',\n",
       " 'dummy_mnist',\n",
       " 'emnist',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'gap',\n",
       " 'glue',\n",
       " 'groove',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'image_label_folder',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imdb_reviews',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lfw',\n",
       " 'lm1b',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'moving_mnist',\n",
       " 'multi_nli',\n",
       " 'nsynth',\n",
       " 'omniglot',\n",
       " 'open_images_v4',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'pet_finder',\n",
       " 'quickdraw_bitmap',\n",
       " 'resisc45',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 'scene_parse150',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tf_flowers',\n",
       " 'titanic',\n",
       " 'trivia_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc2007',\n",
       " 'wikipedia',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'xnli']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the different types of datasets available:\n",
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='cifar100',\n",
      "    version=1.3.1,\n",
      "    description='This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).',\n",
      "    urls=['https://www.cs.toronto.edu/~kriz/cifar.html'],\n",
      "    features=FeaturesDict({\n",
      "        'coarse_label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=100),\n",
      "    }),\n",
      "    total_num_examples=60000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 50000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the CIFAR-100 dataset:\n",
    "cifar_builder = tfds.builder(\"cifar100\")\n",
    "cifar_builder.download_and_prepare()\n",
    "\n",
    "print(cifar_builder.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above printed text, it also shows some useful information such as the number of training and testing samples, the key-names and so on.\n",
    "\n",
    "## 1.2 - Examine the class labels of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
      "The number of classes are: 100\n"
     ]
    }
   ],
   "source": [
    "# Check out the labels:\n",
    "print(cifar_builder.info.features[\"label\"].names)\n",
    "\n",
    "print('The number of classes are: {}'.format(len(cifar_builder.info.features[\"label\"].names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_and_vegetables', 'household_electrical_devices', 'household_furniture', 'insects', 'large_carnivores', 'large_man-made_outdoor_things', 'large_natural_outdoor_scenes', 'large_omnivores_and_herbivores', 'medium_mammals', 'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals', 'trees', 'vehicles_1', 'vehicles_2']\n",
      "The number of classes are: 20\n"
     ]
    }
   ],
   "source": [
    "# Check out the coarse labels:\n",
    "print(cifar_builder.info.features[\"coarse_label\"].names)\n",
    "\n",
    "print('The number of classes are: {}'.format(len(cifar_builder.info.features[\"coarse_label\"].names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Define the Input Pipeline:\n",
    "\n",
    "As the dataset have been downloaded with the help of the package, this section will then define the input pipeline for the models to be trained. These are variables that will be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hyperparameters:\n",
    "input_shape = [224, 224, 3]\n",
    "batch_size = 512\n",
    "nb_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# Divide the dataset into training and validation sets:\n",
    "train_set_cifar = cifar_builder.as_dataset(split = tfds.Split.TRAIN)\n",
    "test_set_cifar = cifar_builder.as_dataset(split = tfds.Split.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of classes:\n",
    "nb_classes = cifar_builder.info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of images for training and validation:\n",
    "nb_train_imgs = cifar_builder.info.splits['train'].num_examples\n",
    "nb_valid_imgs = cifar_builder.info.splits['test'].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Check out the dataset instances:\n",
    "\n",
    "This should provide some additional details about the dataset, such as its dtype and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training set instance: <_OptionsDataset shapes: {coarse_label: (), image: (32, 32, 3), label: ()}, types: {coarse_label: tf.int64, image: tf.uint8, label: tf.int64}>\n"
     ]
    }
   ],
   "source": [
    "print(\"The Training set instance: {}\".format(train_set_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Testing set instance: <_OptionsDataset shapes: {coarse_label: (), image: (32, 32, 3), label: ()}, types: {coarse_label: tf.int64, image: tf.uint8, label: tf.int64}>\n"
     ]
    }
   ],
   "source": [
    "print(\"The Testing set instance: {}\".format(test_set_cifar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Preprocessing:\n",
    "\n",
    "## 2.1 - Shuffling the dataset:\n",
    "\n",
    "The notion behind shuffling the dataset before being used for training is so that the model is able to learn from majority of the data in the training set, where unshuffled, the training data can be in the format order of A-R while the testing set is S-Z, so during training the model only learns from what is available from A-R but will later perform badly in the unseen testing data of S-Z. Shuffling, as mentioned, allows the model to train in a more robust manner resulting in a better generalised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the Training Dataset: 10,000 times of reshuffling.\n",
    "train_set_cifar = train_set_cifar.repeat(nb_epochs).shuffle(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Set up the dataset for compatibility with Keras API (non-estimator) and Image Augmentation:\n",
    "\n",
    "It should be noted that Tensorflow-Dataset returns batches as feature dictionaries, which is expected by Estimators. Therefore, to train with Keras models, it is better to return the batch content as tuples. A user defined funtion will be needed to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_data_func(features, input_shape, augment=False):\n",
    "    \"\"\" This builds a pre-processing function to resize the image into the expected dimenions and allows for an\n",
    "        optional transformation of the images such as augmentations.\n",
    "    Parameters:\n",
    "        - features, is the input data.\n",
    "        - input_shape, is the expected shape for model by resizing.\n",
    "        - augment, is a Flag to apply random transformations to the input images.\n",
    "    Returns:\n",
    "        - returns Augmented images, Labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the input images into tensors:\n",
    "    input_shape = tf.convert_to_tensor(input_shape)\n",
    "    \n",
    "    # Convert TF-dataset feature dictionaries into Tuples:\n",
    "    image = features['image']\n",
    "    label = features['label']\n",
    "    \n",
    "    # Convert the image data type to \"float32\" and normalise the data to a range between \"0 and 1\":\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # Data Image Augmentation:\n",
    "    if augment:\n",
    "        # Apply random horizontal flip::\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "        # Apply Brightness and Saturation changes:\n",
    "        image = tf.image.random_brightness(image, max_delta = 0.1)\n",
    "        image = tf.image.random_saturation(image, lower = 0.5, upper = 1.5)\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0) # this keeps the pixel values in check.\n",
    "        \n",
    "        # Apply random resizing and cropping back to expected image sizes:\n",
    "        random_scale_factor = tf.random.uniform([1], minval = 1., maxval = 1.4, dtype = tf.float32)\n",
    "        \n",
    "        scaled_height = tf.cast(tf.cast(input_shape[0], tf.float32) * random_scale_factor, tf.int32)\n",
    "        scaled_width = tf.cast(tf.cast(input_shape[1], tf.float32) * random_scale_factor, tf.int32)\n",
    "        scaled_shape = tf.squeeze(tf.stack([scaled_height, scaled_width]))\n",
    "        \n",
    "        image = tf.image.resize(image, scaled_shape)\n",
    "        image = tf.image.random_crop(image, input_shape)\n",
    "    else:\n",
    "        image = tf.image.resize(image, input_shape[:2])\n",
    "        \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the function above on the training and validation set:\n",
    "\n",
    "NOTE: functools.partial example -> https://stackoverflow.com/questions/15331726/how-does-functools-partial-do-what-it-does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the function for the training data preparation:\n",
    "prepare_data_func_for_trainSet = functools.partial(_prepare_data_func, input_shape = input_shape, augment = True)\n",
    "\n",
    "# Map the function to the dataset:\n",
    "train_set_cifar = train_set_cifar.map(prepare_data_func_for_trainSet, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Split the training set into batched samples:\n",
    "train_set_cifar = train_set_cifar.batch(batch_size)\n",
    "\n",
    "# Set to prefetch the data: improves the perforamnce.\n",
    "# his allows later elements to be prepared while the current element is being processed. \n",
    "# This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "train_set_cifar= train_set_cifar.prefetch(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Validation set: no augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the function for the Validation data preparation:\n",
    "prepare_data_func_for_ValidSet = functools.partial(_prepare_data_func, input_shape = input_shape, augment = False)\n",
    "\n",
    "# Map the function to the dataset:\n",
    "test_set_cifar = test_set_cifar.map(prepare_data_func_for_ValidSet, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Split the Validation set into batched samples:\n",
    "test_set_cifar = test_set_cifar.batch(batch_size)\n",
    "\n",
    "# Set to prefetch the data: improves the perforamnce.\n",
    "# his allows later elements to be prepared while the current element is being processed. \n",
    "# This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "test_set_cifar= test_set_cifar.prefetch(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Set the \"steps_per_epoch\" for Training and Validation Set:\n",
    "\n",
    "From the previous code block, the dataset is compatible with Keras methods liike \"model.fit()\", but will also require the \"steps_per_epoch\" to be specified. This is the number of batches per epoch of the Dataset objects that has to be specified for the Keras method so that it can work properly together. This is done for the training batches (\"steps_per_epoch\") and validation batches (\"validation_steps\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = math.ceil(nb_train_imgs / batch_size)\n",
    "\n",
    "valid_steps_per_epoch = math.ceil(nb_valid_imgs / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data preparation function will be placed into a \".py\" utily file, so that it can be reuse later on.\n",
    "\n",
    "## 3 - ResNet Implementation with Keras:\n",
    "\n",
    "This section will dive into making a ResNet builder tool, where the goal is to be able to instantiate a range of deep ResNet model such as Resnet-18 to Resnet-101 and so on. The Keras Functional API will be used to simplify the process.\n",
    "\n",
    "## 3.1 - Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Activation, Dense, Flatten, Conv2D, MaxPooling2D, \n",
    "                                     GlobalAveragePooling2D, AveragePooling2D, BatchNormalization, add)\n",
    "import tensorflow.keras.regularizers as regulisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Residual Blocks Implementations:\n",
    "\n",
    "For the purpose of building a ResNet model builder function that is capable of generating a network of different sizes, the implementation of ResNet therefore have to be modular as well. To do this, each of the components that make up the ResNet model will implemented separately/modular below.\n",
    "\n",
    "__Residual Blocks__ are composed of the following:\n",
    "- 1x Residual Branch; a 3x3 convolution on the input data, followed by batch normalisation and then a ReLU activation function.\n",
    "- 1x Shortcut Branch; it directly forwards the input without any modification, or that it can apply a 1x1 convolution to adapt the input volume where in the case it changes in the other branch.\n",
    "- 1x Merge Operation; consisting of an element-wise addition between the output results of the two branches.\n",
    "\n",
    "The following will then implement each of the Sub-modules of the ResNet model.\n",
    "\n",
    "### 3.2.1 - (Subblock 1) Residual Branch:\n",
    "\n",
    "Wrap the convolution, batch normalisation and ReLU activation as a stack of layers into a single function. The function below allows for the addition of 3 stacked layers (conv_bn_Relu) with just a \" x = _conv_bn_Relu(**params)(x) \" code for the residual branch.\n",
    "\n",
    "This is also called the __residual path (GREEN)__ and can be seen in the previous notebook in Section 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _res_conv(filters, kernel_size=3, padding='same', strides=1, use_relu=True, use_bias=False, name='conv_bn_Relu', \n",
    "              kernel_initialiser = 'he_normal', kernel_regulariser = regulisers.l2(1e-5)):\n",
    "    \"\"\" This builds the Residual Branch, consisting of the Convolutions, batch norm and ReLU activation.\n",
    "    Parameters:\n",
    "        - filters, is the number of filters.\n",
    "        - kernel_size, is the Kernel Size.\n",
    "        - padding, is the convolution padding.\n",
    "        - strides, is the convolution stride.\n",
    "        - use_rele, is a Flag to apply the ReLU activation function at the end.\n",
    "        - use_bias, is a Flag to use or not the bias in the Convolutional layer.\n",
    "        - name, is the Name Suffix for the layers.\n",
    "        - kernel_initialiser, is the Kernel initialisattion method name.\n",
    "        - kernel_regulariser, is the kernel regulariser.\n",
    "    Returns:\n",
    "        - returns a Callable Layer Block.\n",
    "    \"\"\"\n",
    "    def layer_func(x):\n",
    "        # Convolution:\n",
    "        conv = Conv2D(filters = filters,\n",
    "                      kernel_size = kernel_size,\n",
    "                      strides = strides,\n",
    "                      padding = padding,\n",
    "                      use_bias = use_bias,\n",
    "                      kernel_initializer = kernel_initialiser,\n",
    "                      kernel_regularizer = kernel_regulariser\n",
    "                     )(x)\n",
    "                \n",
    "        # Batch Norm:\n",
    "        residual_branch = BatchNormalization(axis = -1,\n",
    "                                             name = '_bn'\n",
    "                                            )(conv)\n",
    "        \n",
    "        # ReLU activation:\n",
    "        if use_relu:\n",
    "            residual_branch = Activation(activation = 'relu',\n",
    "                                         name = name + '_r'\n",
    "                                        )(residual_branch)\n",
    "        return residual_branch\n",
    "    \n",
    "    return layer_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - (Subblock 2) Shortcut Branch:\n",
    "\n",
    "Simplifying the __shortcut operation__ can be done by coupling it with the __merge operation__. This \"_merge_with_shortcut\" function needs to receive parameters of both the input tensor and the results of the residual branch, so that it can compare the shapes of the tensor, checking it the dimensions were changed or not. \n",
    "\n",
    "If the dimensions were changed, a 1x1 convolution will be applied to resize the input tensor before merging. If unchanged, the input tensor will be left unaltered. \n",
    "\n",
    "As this function have both the residual and shortcut results, it can merge the results together.\n",
    "\n",
    "This is also called the __mapping path (BLUE)__ and can be seen in the previous notebook in Section 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_with_shortcut(kernel_initialiser = 'he_normal', kernel_regulariser = regulisers.l2(1e-5), name='block'):\n",
    "    \"\"\" This builds the merging layer block for the input tensor and the residual branch output tensor.\n",
    "    Parameters:\n",
    "        - kernel_initialiser, is the Kernel initialisation method name.\n",
    "        - kernel_regulariser, is the Kernel regulariser.\n",
    "        - name, is the Name Suffix of this layer.\n",
    "    Returns:\n",
    "        - returns a Callable Layer Block.\n",
    "    \"\"\"\n",
    "    def layer_func(x, x_residual):\n",
    "        # Check if there are changes made to the residual branch output \"x_residual\": \n",
    "        # if changed, apply 1x1 convolutions.\n",
    "        x_shape = tf.keras.backend.int_shape(x)\n",
    "        x_residual_shape = tf.keras.backend.int_shape(x_residual)\n",
    "        \n",
    "        if (x_shape == x_residual_shape):\n",
    "            shortcut = x\n",
    "        else:\n",
    "            strides = (\n",
    "                        int(round(x_shape[1] / x_residual_shape[1])), # Vertical Stride.\n",
    "                        int(round(x_shape[2] / x_residual_shape[2]))  # horizontal Stride.\n",
    "            )\n",
    "            x_residual_chnls = x_residual_shape[3]\n",
    "            \n",
    "            shortcut = Conv2D(filters = x_residual_chnls,\n",
    "                              kernel_size = (1, 1),\n",
    "                              strides = strides,\n",
    "                              padding = \"valid\",\n",
    "                              kernel_initializer = kernel_initialiser,\n",
    "                              kernel_regularizer = kernel_regulariser,\n",
    "                              name = name '_shortcut_c'\n",
    "                             )(x)\n",
    "            \n",
    "            # Merge the shortcut and residual:\n",
    "            merge = add( [shortcut, x_residual] )\n",
    "            \n",
    "            # Return the merged output:\n",
    "            return merge\n",
    "        \n",
    "        return layer_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 - Crete a Complete Residual Block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "Although this is not the end of this project, it does conclude the 1st notebook relevant to the theory of these advanced deep learning models. Please go to check out __Notebook 3__ for the __Keras Applications and reusing models__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
