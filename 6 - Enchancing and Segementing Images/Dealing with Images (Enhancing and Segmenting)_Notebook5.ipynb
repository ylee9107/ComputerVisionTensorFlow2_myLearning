{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Images (Enhancing and Segmenting) [Notebook 5]\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project dives into Encoders-Decoders, where these models are used to edit and generate full images. How these models can be adapted for a wider range of applications such as image denoising or object and instance segmentation. The project will also introduce new concepts like Unpooling, Transposed and Atrous Convolutions layers to the network architecture and its utility for high-dimensional data. Encoders-Decoders can be used for semantic segmentation for driverless cars, where it would help in defining the objects surrounding the vehicle like, roads, other vehicles, people or trees etc. \n",
    "\n",
    "## Breakdown of this Project:\n",
    "- Introduction to Encoders-Decoders. (Notebook 1)\n",
    "- Encoders-Decoders trained for pixel-level prediction. (Notebook 1)\n",
    "- Layers such as Unpooling, Transposed and Atrous Convolutions to output high-dimensional data. (Notebook 2)\n",
    "- FCN (Notebook 3) and U-Net (Notebook 4) Architectures for semantic segmentation. \n",
    "- Instance segmentation (extension of Faster-RCNN with Mask-RCNN) (Notebook 5)\n",
    "\n",
    "## Requirements:\n",
    "1) Tensorflow 2.0 (GPU prefferably) \\\n",
    "2) CV2 (OpenCV) \\\n",
    "3) Cython \\\n",
    "4) Eigen \\\n",
    "5) PyDenseCRF\n",
    "\n",
    "For \"PyDenseCRF\" for windows, LINK: https://github.com/lucasb-eyer/pydensecrf\\\n",
    "\n",
    "It can be installed directly with the following in command prompt or terminal-equivalent: __conda install -c conda-forge pydensecrf__.\n",
    "\n",
    "If Conda-Forge __does not work__, try: \n",
    "- going to: https://www.lfd.uci.edu/~gohlke/pythonlibs/#pydensecrf\n",
    "- Download: pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "- Where \"cp37\" in the filename is the python version of 3.7, make sure you download the correct one.\n",
    "- Place the downloaded \"pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\" file in your working directory drive.\n",
    "- Open Command Prompt and type in: pip install pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "- Or if you placed it in a folder or different location: pip install <FILEPATH>\\pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "\n",
    "## Dataset:\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run on GPU:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42\n",
    "\n",
    "# Set to run or not run the code block: for code examples only. (0 = run code, and 1 = dont run code)\n",
    "dont_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 8390128233705279065),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4832671986718658171),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, -5649852988599655110)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR TENSORFLOW ONLY\n",
    "\n",
    "Automatic Mixed Precision Training in TF. Requires NVIDIA DOCKER of TensorFlow.\n",
    "\n",
    "Sources:\n",
    "- https://developer.nvidia.com/automatic-mixed-precision\n",
    "- https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#framework\n",
    "\n",
    "When enabled, automatic mixed precision will do two things:\n",
    "\n",
    "- Insert the appropriate cast operations into your TensorFlow graph to use float16 execution and storage where appropriate(this enables the use of Tensor Cores along with memory storage and bandwidth savings). \n",
    "- Turn on automatic loss scaling inside the training Optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE CODE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph-based example:\n",
    "# opt = tf.train.AdamOptimizer()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# train_op = opt.miminize(loss)\n",
    "\n",
    "# # Keras-based example:\n",
    "# opt = tf.keras.optimizers.Adam()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# model.compile(loss=loss, optimizer=opt)\n",
    "# model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR KERAS API\n",
    "\n",
    "Source:\n",
    "- https://www.tensorflow.org/guide/keras/mixed_precision\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set for MIXED PRECISION:\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "\n",
    "# print('Compute dtype: %s' % policy.compute_dtype)\n",
    "# print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run this notebook without errors, the GPU will have to be set accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# physical_devices\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Instance segmentation with Mask-RCNN (extension of Faster-RCNN):\n",
    "\n",
    "Mask-RCNN is an extension of Faster-RCNN, where instance segmentation can be achieved in two steps:\n",
    "1. Utilising a Object Detection model, like Faster-RCNN, to output the bounding boxes for each of the instance of the target classes.\n",
    "2. These instances are then passed to a semantic segmentation model to obtain the instance masks.\n",
    "\n",
    "This is an advantageous approach to solving the instance segmentation task because it assumes that the predicted bounding boxes are accurate, where the element within the bounding box will be classified pixel-wise to the belonging class. \n",
    "\n",
    "Mask-RCNN's architecture can be describe as a whole pipeline with two networks being stitched together, and is trained in an end-to-end manner. The segmentation loss is backpropagated though the common layers of the two network to ensure that the features that are extracted are useful for both the detection and segmentation tasks.\n",
    "\n",
    "##### Below shows the Mask-RCNN Architecture:\n",
    "\n",
    "<img src=\"Description Images/Mask_RCNN.PNG\" width=\"450\">\n",
    "\n",
    "Image Ref -> https://www.researchgate.net/figure/The-structure-of-the-Mask-R-CNN-architecture_fig2_337795870\n",
    "\n",
    "As mentioned, Mask-RCNN is based on the Faster-RCNN model, where it is composed of a Region Proposal Network (RPN), that is followed by two branches that predicts the class the box offset for each of the proposed region. To achieve segmentation with masks, this model has a third branch that outputs the binary mask for the element in each region. It should be noted that this branch is composed of a couple standard and transposed convolutions. Overall, this model achieves instance segmentation by processing everything in parallel rather than sequential. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Implementing Mask-RCNN with Tensorflow Object Detection API:\n",
    "\n",
    "### 2.1 - TensorFlow Object Detection API:\n",
    "\n",
    "Using the API allows the access to the most up to date implementation and the model would be pre-trained. \n",
    "\n",
    "The following code follows the step-by-step guide from the link: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html\n",
    "\n",
    "### 2.2 - Utilising a Pretrained Model:\n",
    "\n",
    "The API consists of several pre-trained models with the COCO dataset. The model themselves can vary in base architecture, such as different parameters or backbones. Depending on the choice model, it can vary in inference speed and performance. Note, as a rule of thumb, the inference time grows with the increase in mean average precision.\n",
    "\n",
    "### 2.3 - Installation Process:\n",
    "\n",
    "The step-by-step guide to set up the API from the link: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html\n",
    "\n",
    "\n",
    "## 2.4 - Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Libraries:\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Object Detection Modules from the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Downloading the Testing Images:\n",
    "\n",
    "Download the images and save them into the folder \"data/image\". The images will be from: https://github.com/tensorflow/models/tree/master/research/object_detection/test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.chdir('D:\\TensorFlow ObjDetection API')\n",
    "tf.get_logger().setLevel('ERROR')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images():\n",
    "    base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/test_images/'\n",
    "    filenames = ['image1.jpg', 'image2.jpg']\n",
    "    image_paths = []\n",
    "    for filename in filenames:\n",
    "        image_path = tf.keras.utils.get_file(fname=filename,\n",
    "                                            origin=base_url + filename,\n",
    "                                            untar=False)\n",
    "        image_path = pathlib.Path(image_path)\n",
    "        image_paths.append(str(image_path))\n",
    "    return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment, if downloading the label maps are required:\n",
    "# IMAGE_PATHS = download_images()\n",
    "# TEST_IMAGE_PATHS = IMAGE_PATHS\n",
    "# TEST_IMAGE_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('models/research/object_detection/test_images/image1.jpg'),\n",
       " WindowsPath('models/research/object_detection/test_images/image2.jpg')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PATH_TO_TEST_IMAGES_DIR = os.path.abspath('models\\research\\object_detection\\test_images')\n",
    "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('models/research/object_detection/test_images')\n",
    "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
    "TEST_IMAGE_PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Prepare the Model:\n",
    "\n",
    "#### Model Loader:\n",
    "\n",
    "List of models can be found here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL SELECTED -> Mask R-CNN Inception ResNet V2 1024x1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load in the selected model:\n",
    "\n",
    "1. From the link above, select one of the model without clicking, as this will just download it. What is needed can be seen from step 2 and onwards.\n",
    "2. Right click on the Model name of the model you would like to use.\n",
    "3. Click on \"Copy link address\" to copy the download link of the model.\n",
    "4. Paste the link in a text editor of your choice. You should observe a link similar to download.tensorflow.org/models/object_detection/tf2/YYYYYYYY/XXXXXXXXX.tar.gz.\n",
    "5. Copy the XXXXXXXXX part of the link and use it to replace the value of the MODEL_NAME variable in the code shown below.\n",
    "6. Copy the YYYYYYYY part of the link and use it to replace the value of the MODEL_DATE variable in the code shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_date):\n",
    "    \"\"\" This function loads and instantiates the requested model.\n",
    "    Parameters:\n",
    "        - model_name, is the (Str) of the model's name.\n",
    "        - model_date, is the model date.\n",
    "    Returns:\n",
    "        - returns an instantiated model.\n",
    "    Info:\n",
    "        The models can be found here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n",
    "    \"\"\"\n",
    "    # Download:\n",
    "    base_url = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "    model_file = model_name + '.tar.gz'\n",
    "    model_dir = tf.keras.utils.get_file(fname=model_name,\n",
    "                                        origin=base_url + model_date + '/' + model_file,\n",
    "                                        untar=True)\n",
    "    # Set the Path:\n",
    "    PATH_TO_MODEL_DIR = str(model_dir)\n",
    "    PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "        \n",
    "    # Instantiate the Model:\n",
    "    print('Loading model...', end='')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print('Done! Took {} seconds'.format(elapsed_time))\n",
    "    \n",
    "#     model = model.signatures['serving_default']\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://download.tensorflow.org/models/object_detection/tf2/20200711/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...Done! Took 86.36892080307007 seconds\n"
     ]
    }
   ],
   "source": [
    "detection_model = load_model(model_name = 'mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8', \n",
    "                             model_date= '20200711')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection_model = load_model(model_name = 'faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8', \n",
    "#                              model_date= '20200711')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Label Map:\n",
    "\n",
    "Downloads the labels file (.pbtxt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_labels(filename= 'mscoco_label_map.pbtxt'):\n",
    "    \"\"\" This function downloads the labels from the dataset.\n",
    "    Parameters:\n",
    "        - filename, is the (Str) of the labels file (.pbtxt).\n",
    "    Returns:\n",
    "        - returns an instantiated model.\n",
    "    Info:\n",
    "        Full list of the labels files: https://github.com/tensorflow/models/tree/master/research/object_detection/data\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "    label_dir = tf.keras.utils.get_file(fname=filename,\n",
    "                                        origin=base_url + filename,\n",
    "                                        untar=False)\n",
    "    label_dir = pathlib.Path(label_dir)\n",
    "    return str(label_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment, if downloading the label maps are required:\n",
    "# LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "# PATH_TO_LABELS = download_labels(filename=LABEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Label map data for visualisation:\n",
    "\n",
    "The label maps will correspond with the index numbers to category names, therefore when the network perdicts a \"5\", it will mean an \"airplane\". The function used below is an internal utility function, that takes in anything that will returns a dictionary mapping of the integers to the respective string labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 - Combining all the components together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <img src=\"Description Images/.png\" width=\"750\">\n",
    "\n",
    "# Image Ref -> \n",
    "\n",
    "# <img src=\"Description Images/.png\" width=\"750\">\n",
    "\n",
    "# Image Ref -> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
