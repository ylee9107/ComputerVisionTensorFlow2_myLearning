{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Images (Enhancing and Segmenting) [Notebook 3]\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project dives into Encoders-Decoders, where these models are used to edit and generate full images. How these models can be adapted for a wider range of applications such as image denoising or object and instance segmentation. The project will also introduce new concepts like Unpooling, Transposed and Atrous Convolutions layers to the network architecture and its utility for high-dimensional data. Encoders-Decoders can be used for semantic segmentation for driverless cars, where it would help in defining the objects surrounding the vehicle like, roads, other vehicles, people or trees etc. \n",
    "\n",
    "## Breakdown of this Project:\n",
    "- Introduction to Encoders-Decoders. (Notebook 1)\n",
    "- Encoders-Decoders trained for pixel-level prediction. (Notebook 1)\n",
    "- Layers such as Unpooling, Transposed and Atrous Convolutions to output high-dimensional data. (Notebook 2)\n",
    "- FCN and U-Net Architectures for semantic segmentation. (Notebook 3)\n",
    "- Instance segmentation (extension of Faster-RCNN with Mask-RCNN) (Notebook 4)\n",
    "\n",
    "## Requirements:\n",
    "1) Tensorflow 2.0 (GPU prefferably) \\\n",
    "2) CV2 (OpenCV) \\\n",
    "3) Cython \\\n",
    "4) Eigen \\\n",
    "5) PyDenseCRF\n",
    "\n",
    "For \"PyDenseCRF\" for windows, LINK: https://github.com/lucasb-eyer/pydensecrf\\\n",
    "\n",
    "It can be installed directly with the following in command prompt or terminal-equivalent: __conda install -c conda-forge pydensecrf__.\n",
    "\n",
    "If Conda-Forge __does not work__, try: \n",
    "- going to: https://www.lfd.uci.edu/~gohlke/pythonlibs/#pydensecrf\n",
    "- Download: pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "- Where \"cp37\" in the filename is the python version of 3.7, make sure you download the correct one.\n",
    "- Place the downloaded \"pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\" file in your working directory drive.\n",
    "- Open Command Prompt and type in: pip install pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "- Or if you placed it in a folder or different location: pip install <FILEPATH>\\pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "\n",
    "## Dataset:\n",
    "    \n",
    "The dataset can be obtain from the link: https://www.cityscapes-dataset.com/dataset-overview/.\n",
    "\n",
    "Quoted from the website: \"The Cityscapes Dataset focuses on semantic understanding of urban street scenes.\" It consists of >5,000 images with fine-grained semantic labels, 20,000 images with coarser annotations that were shot from the view point of driving a car around different cities in Germany. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run on GPU:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" \n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42\n",
    "\n",
    "# Set to run or not run the code block: for code examples only. (0 = run code, and 1 = dont run code)\n",
    "dont_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "# devices = sess.list_devices()\n",
    "# devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR TENSORFLOW ONLY\n",
    "\n",
    "Automatic Mixed Precision Training in TF. Requires NVIDIA DOCKER of TensorFlow.\n",
    "\n",
    "Sources:\n",
    "- https://developer.nvidia.com/automatic-mixed-precision\n",
    "- https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#framework\n",
    "\n",
    "When enabled, automatic mixed precision will do two things:\n",
    "\n",
    "- Insert the appropriate cast operations into your TensorFlow graph to use float16 execution and storage where appropriate(this enables the use of Tensor Cores along with memory storage and bandwidth savings). \n",
    "- Turn on automatic loss scaling inside the training Optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE CODE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph-based example:\n",
    "# opt = tf.train.AdamOptimizer()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# train_op = opt.miminize(loss)\n",
    "\n",
    "# # Keras-based example:\n",
    "# opt = tf.keras.optimizers.Adam()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# model.compile(loss=loss, optimizer=opt)\n",
    "# model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR KERAS API\n",
    "\n",
    "Source:\n",
    "- https://www.tensorflow.org/guide/keras/mixed_precision\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set for MIXED PRECISION:\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "\n",
    "# print('Compute dtype: %s' % policy.compute_dtype)\n",
    "# print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run this notebook without errors, the GPU will have to be set accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# physical_devices\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - What is Semantic Segmentation?:\n",
    "\n",
    "__Semantic Segmentation__ is the task of segmenting images into meaningful parts, it covers segmentation of both objects and instances. This task is different comapred to image classification or object detection tasks, where more fundamentally, it requires the return of a pixel-level dense predictions whereby it assigns a label to each of the pixel of the input images.\n",
    "\n",
    "## 2 - Encoders-Decoders for Object Segmentation:\n",
    "\n",
    "Segmenting objects in a scene of an image can be described as the mapping of images from a colour domain to a class domain. It assigns one of the target classes to each pixel and returns a label map of the same height and width. To perform this kind of operation with an Encoder-Decoder will require further considerations as it is not as straightforward.\n",
    "\n",
    "## 2.1 - Decoding as label maps:\n",
    "\n",
    "If an Encoder-Decoder network was constructed to output label maps where each of the pixel value would be a class (i.e. 1 for house or 2 for car), the model would only output very poor results. A better implementation is to directly output categorical values instead. Previously, for the task of image classification that consist of \"N\" number of categories, the final layer of the network would outtput \"N\" logits for each class, these scores were then converted to probabilities with the Softmax function and finished by picking the largest probability value with Argmax function. This mechanism can also be applied to Semantic Segmentation, where it would be at a pixel level instead rather than the overall image level. \n",
    "\n",
    "##### The image below represents the task of Image Segmentation:\n",
    "\n",
    "<img src=\"Description Images/Semantic_Segmentation_Overall.PNG\" width=\"750\">\n",
    "\n",
    "The diagram above shows the process of the Encoder-Decoder model taking an input image and outputing the predicted label maps. This process can be broken into three parts. Note that the example used here is labeled as a low-resolution prediction map, where in practice, the predicted segmentation label map should have the same resolution as the original input.\n",
    "\n",
    "##### The image below represents part 1 of Image Segmentation:\n",
    "\n",
    "<img src=\"Description Images/Semantic_Segmentation_1.PNG\" width=\"750\">\n",
    "\n",
    "Image Ref -> https://www.jeremyjordan.me/semantic-segmentation/\n",
    "\n",
    "First, the main goal of the model would be to take an input such as a RGB coloured image tensor with the shape of (Height x Width x 3) or greyscale tensor with the shape of (Height x Width x 1), and to output a segmentation label map, where each of the pixel would have a class label that is represented as an integer (Height x Width x 1). \n",
    "\n",
    "##### The image below represents part 2 of Image Segmentation:\n",
    "\n",
    "<img src=\"Description Images/Semantic_Segmentation_2.PNG\" width=\"750\">\n",
    "\n",
    "Image Ref -> https://www.jeremyjordan.me/semantic-segmentation/\n",
    "\n",
    "Second, the following shows an intermediate stage composing of individual masks for each of the class labels. By setting the number of output channels equal to the number of classes, the Encoder-Decoder model can obtain the output tensor. In doing so, it also means that it can be trained as a classifier. The loss is computed with cross-entropy loss, where it compares the softmax values with the one-hot-encoded ground truth label maps. These (Height x Width x N) prediction can be transformed into per-pixel labels by selecting the highest value along the channel axis. This essentially means that an output prediction image can be formed by collapsing the segmenation map by taking the argmax value along the channel axis (or depth-wise pixel vector).\n",
    "\n",
    "##### The image below represents part 3 of Image Segmentation:\n",
    "\n",
    "<img src=\"Description Images/Semantic_Segmentation_3.PNG\" width=\"750\">\n",
    "\n",
    "Image Ref -> https://www.jeremyjordan.me/semantic-segmentation/\n",
    "\n",
    "Third, by overlaying these predictions into a single channel will form the target prediction image, that is refered to as the __mask__, where each of the specific class is highlighted over regions of the image.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Training the model with segmentation losses and metrics:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <img src=\"Description Images/.png\" width=\"750\">\n",
    "\n",
    "# Image Ref -> \n",
    "\n",
    "# <img src=\"Description Images/.png\" width=\"750\">\n",
    "\n",
    "# Image Ref -> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
