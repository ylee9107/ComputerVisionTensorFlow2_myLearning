{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Images (Enhancing and Segmenting)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project dives into Encoders-Decoders, where these models are used to edit and generate full images. How these models can be adapted for a wider range of applications such as image denoising or object and instance segmentation. The project will also introduce new concepts like Unpooling, Transposed and Atrous Convolutions layers to the network architecture and its utility for high-dimensional data. Encoders-Decoders can be used for semantic segmentation for driverless cars, where it would help in defining the objects surrounding the vehicle like, roads, other vehicles, people or trees etc. \n",
    "\n",
    "## Breakdown of this Notebook:\n",
    "- Introduction to Encoders-Decoders.\n",
    "- Encoders-Decoders trained for pixel-level prediction.\n",
    "- Layers such as Unpooling, Transposed and Atrous Convolutions to output high-dimensional data.\n",
    "- FCN and U-Net Architectures for semantic segmentation.\n",
    "- Instance segmentation (extension of Faster-RCNN with Mask-RCNN)\n",
    "\n",
    "## Requirements:\n",
    "1) Tensorflow 2.0 (GPU prefferably) \\\n",
    "2) CV2 (OpenCV) \\\n",
    "3) Cython \\\n",
    "4) Eigen \\\n",
    "5) PyDenseCRF\n",
    "\n",
    "For \"PyDenseCRF\" for windows, LINK: https://github.com/lucasb-eyer/pydensecrf\\\n",
    "\n",
    "It can be installed directly with the following in command prompt or terminal-equivalent: __conda install -c conda-forge pydensecrf__.\n",
    "\n",
    "If Conda-Forge __does not work__, try: \n",
    "- going to: https://www.lfd.uci.edu/~gohlke/pythonlibs/#pydensecrf\n",
    "- Download: pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "- Where \"cp37\" in the filename is the python version of 3.7, make sure you download the correct one.\n",
    "- Place the downloaded \"pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\" file in your working directory drive.\n",
    "- Open Command Prompt and type in: pip install pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "- Or if you placed it in a folder or different location: pip install <FILEPATH>\\pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "\n",
    "\n",
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42\n",
    "\n",
    "# Set to run or not run the code block: for code examples only. (0 = run code, and 1 = dont run code)\n",
    "dont_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 1884025239219475482),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 6586313605, 13382654521099684224)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR TENSORFLOW ONLY\n",
    "\n",
    "Automatic Mixed Precision Training in TF. Requires NVIDIA DOCKER of TensorFlow.\n",
    "\n",
    "Sources:\n",
    "- https://developer.nvidia.com/automatic-mixed-precision\n",
    "- https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#framework\n",
    "\n",
    "When enabled, automatic mixed precision will do two things:\n",
    "\n",
    "- Insert the appropriate cast operations into your TensorFlow graph to use float16 execution and storage where appropriate(this enables the use of Tensor Cores along with memory storage and bandwidth savings). \n",
    "- Turn on automatic loss scaling inside the training Optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE CODE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph-based example:\n",
    "# opt = tf.train.AdamOptimizer()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# train_op = opt.miminize(loss)\n",
    "\n",
    "# # Keras-based example:\n",
    "# opt = tf.keras.optimizers.Adam()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# model.compile(loss=loss, optimizer=opt)\n",
    "# model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR KERAS API\n",
    "\n",
    "Source:\n",
    "- https://www.tensorflow.org/guide/keras/mixed_precision\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set for MIXED PRECISION:\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "\n",
    "# print('Compute dtype: %s' % policy.compute_dtype)\n",
    "# print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introduction to Encoders-Decoders:\n",
    "\n",
    "The Encoder-Decoder architecture is composed of, like its name suggests, an encoder at one half and a decoder at ther other half. The Encoder is essentially a function that will map the input data into a latent space. This latent space is composed of a structured set of values that is defined by the encoder. The Decoder takes the mapped elements from the latent space and maps them into the predefined target domain. Typically, there are many applications for Ecoders-Decoders, such as communications (transmiters and receivers), electronics, and so on. It largely serves the purpose as an information converter. For the purposes of Machine Learning, these kinds of network were used for text translation, whereby the Encoder network would ingest a source language input (like spanish) and learns to project the data into a latent space and have the language's meaning encoded as a feature vector (a.k.a codes), this is then followed by the Decoder network (trained alongside the Encoder) that would convert these encoded feautre vectors into the target language (like english).\n",
    "\n",
    "Below shows an example of the Ecoder-Decoder Network:\n",
    "\n",
    "<img src=\"Description Images/EncoderDecoderNetwork.png\" width=\"750\">\n",
    "\n",
    "Image Ref -> https://www.pyimagesearch.com/2020/02/24/denoising-autoencoders-with-keras-tensorflow-and-deep-learning/ and https://www.researchgate.net/figure/t-SNE-visualizations-for-clustering-on-MNIST-dataset-in-a-Original-pixel-space-b_fig1_322674846\n",
    "\n",
    "Here, the Encoder can be seen to be trained on the MNIST Digits dataset, where it converts the 28x28 images into the latent space of vectors (codes) of 32 values. The Decoder is then trained to recover the images from the latent space of vectors. By plotting (using the t-SNE method) the codes with the class labels, it can be seen that it shows the similarities or its structure inherent of the dataset. These are the semantic information that was referred to earlier. Encoders serves to extract these semantic information that are inherent within the dataset and then it is decoded (by the Decoder) to decompress the information accordingly.\n",
    "\n",
    "## 2 - Auto-Encoding:\n",
    "\n",
    "These Auto-Encoders are a special kind of Encoder-Decoder, where for such a task that the input and target domains presents to be the same, like images, the Auto-Encoder should properly encode and decode these images without having any impact on the quality. It should perform all of these despite its inherent bottleneck in desgin. Overall, the process begins with the inputs being converted into a compressed representation into the latent space by the Encoder network, where these feature vectors (compressed representations) will be reconstructed by the Decoder network. The distance between the input and output data is usually the loss calculation that would be minimised. The training process for Auto-Encoders are simpler than others largely due to no requirements for ground truth labels, this is because the input images themselves are used as the ground truth. These kinds of model are typically called __Self-Supervised Models__.\n",
    "\n",
    "One good example of this is JPEG tools and that these are also AutoEncoders. Similar to the mentioned function above, these JPEG tools will encode the images firstly then proceeds to decode them while retaining as much quality as possible. For images, the loss computation is the distance as cross-entropy loss or L1 (Manhattan) or L2 (Euclidean) loss between the input and the target images.\n",
    "\n",
    "## 2.1 - Usage of Auto-Encoders:\n",
    "\n",
    "List of Auto-Encoder usage:\n",
    "\n",
    "1) __Depth Regression__: It can be used to estimate the distance between the camera and the image content of interests at a pixel level. These kinds are important operations for applications such as Augmented-Reality, which allows for the construction of 3D representation of the surroundings and its interactions with the environment.\n",
    "\n",
    "2) __Semantic Segmentation__: This is one of the more common use cases, where the model will be trained to return the estimated class for each of the pixels in the iamge. This is a highly important example as it can be used to define objects of interests such as trees, people, vehicles and so on for driverless cars.\n",
    "\n",
    "3) __Artistic Tasks__: These kinds of Encoder-Decoders can be used for transforming images into pseudo-realistic output images, like estimating objects under the sea without the blue colour of the sea water, or estimating the day time representation of a scene with photos taken at night.\n",
    "\n",
    "4) __Generative Tasks__: AutoEncoders can also be structurally configured for generative tasks, where the latent space can be structured in such a way during training that when the feature vector is selected it can be decoded to form a picture. This will lead to more advanced models like Generative Adversarial Networks (GANS).\n",
    "\n",
    "5) __Denoising AEs__: These types of AutoEncoders can transform lossy inputs to return the original versions. Since the input is lossy, the models will then be trained to cancel the lossy operation in order to recover the missing information that will ouptut the original images. The applications for these networks will be for upscaling or super-resolution of images, where it has the added benefits of removing artifacts that are traditionally caused by bilinear interpolation.\n",
    "\n",
    "## 2.2 - Image Denoising Example:\n",
    "\n",
    "This section will demonstrate the ease of constructing an example of Encoder-Decoder network with Keras. The architecture here is a symmetrical form of the Encoders-Decoders with the lower dimensional bottlenecks. Both the inputs and targets are set as \"x_train\". The Sigmoid function is also used in the last layer to output values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    \n",
    "    # Example Code of Encoder-Decoder:\n",
    "    inputs = Input( shape = [img_height * img_width] )\n",
    "    \n",
    "    # Encoder layers:\n",
    "    encoder_1 = Dense(units = 128, activation = 'relu')(inputs)\n",
    "    code = Dense(units = 64, activation = 'reult')(encoder_1)\n",
    "    \n",
    "    # Decoder Layers:\n",
    "    decoder_1 = Dense(units = 64, activation = 'relu')(code)\n",
    "    preds = Dense(units = 128, activation = 'sigmoid')(decoder_1)\n",
    "    \n",
    "    # Model:\n",
    "    autoEncoder = Model(inputs, preds)\n",
    "    \n",
    "    # Training phase: notice, the x_train are both input and targets.\n",
    "    autoEncoder.compile(loss = 'binary_crossentropy')\n",
    "    autoEncoder.fit(x_train, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of denoising an image, the model above can be trained by passsing in a noisy copy of the training images through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    # Noisy input images:\n",
    "    x_noisy = x_train + np.random.normal(loc = .0,\n",
    "                                         scale = .5,\n",
    "                                         size = x_train.shape)\n",
    "\n",
    "    # Fit the model:\n",
    "    autoEncoder.fit(x_noisy, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
