{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Images (Enhancing and Segmenting)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project dives into Encoders-Decoders, where these models are used to edit and generate full images. How these models can be adapted for a wider range of applications such as image denoising or object and instance segmentation. The project will also introduce new concepts like Unpooling, Transposed and Atrous Convolutions layers to the network architecture and its utility for high-dimensional data. Encoders-Decoders can be used for semantic segmentation for driverless cars, where it would help in defining the objects surrounding the vehicle like, roads, other vehicles, people or trees etc. \n",
    "\n",
    "## Breakdown of this Notebook:\n",
    "- Introduction to Encoders-Decoders.\n",
    "- Encoders-Decoders trained for pixel-level prediction.\n",
    "- Layers such as Unpooling, Transposed and Atrous Convolutions to output high-dimensional data.\n",
    "- FCN and U-Net Architectures for semantic segmentation.\n",
    "- Instance segmentation (extension of Faster-RCNN with Mask-RCNN)\n",
    "\n",
    "## Requirements:\n",
    "1) Tensorflow 2.0 (GPU prefferably) \\\n",
    "2) CV2 (OpenCV) \\\n",
    "3) Cython \\\n",
    "4) Eigen \\\n",
    "5) PyDenseCRF\n",
    "\n",
    "For \"PyDenseCRF\" for windows, LINK: https://github.com/lucasb-eyer/pydensecrf\\\n",
    "\n",
    "It can be installed directly with the following in command prompt or terminal-equivalent: __conda install -c conda-forge pydensecrf__.\n",
    "\n",
    "If Conda-Forge __does not work__, try: \n",
    "- going to: https://www.lfd.uci.edu/~gohlke/pythonlibs/#pydensecrf\n",
    "- Download: pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "- Where \"cp37\" in the filename is the python version of 3.7, make sure you download the correct one.\n",
    "- Place the downloaded \"pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\" file in your working directory drive.\n",
    "- Open Command Prompt and type in: pip install pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "- Or if you placed it in a folder or different location: pip install <FILEPATH>\\pydensecrf-1.0rc2-cp37-cp37m-win_amd64.whl\n",
    "\n",
    "## Dataset:\n",
    "    \n",
    "The dataset can be obtain from the link: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "The MNIST Digits dataset contains 70,000 greyscale images that have 28 x 28 pixels for each of the image. This dataset has been a reference set over the last few years to test and improve methods for this recognition task. The Input vector for the network works out to be 28 x 28 = 784 values and it has an output of 10 values (where there are 10 different digits ranging from 0 to 9). Further, the number of hidden layers for this network will be up to the modeller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import time\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random set seed number: for reproducibility.\n",
    "Seed_nb = 42\n",
    "\n",
    "# Set to run or not run the code block: for code examples only. (0 = run code, and 1 = dont run code)\n",
    "dont_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 10090435510479448473),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 6586313605, 4376042280996619867)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR TENSORFLOW ONLY\n",
    "\n",
    "Automatic Mixed Precision Training in TF. Requires NVIDIA DOCKER of TensorFlow.\n",
    "\n",
    "Sources:\n",
    "- https://developer.nvidia.com/automatic-mixed-precision\n",
    "- https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#framework\n",
    "\n",
    "When enabled, automatic mixed precision will do two things:\n",
    "\n",
    "- Insert the appropriate cast operations into your TensorFlow graph to use float16 execution and storage where appropriate(this enables the use of Tensor Cores along with memory storage and bandwidth savings). \n",
    "- Turn on automatic loss scaling inside the training Optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE CODE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph-based example:\n",
    "# opt = tf.train.AdamOptimizer()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# train_op = opt.miminize(loss)\n",
    "\n",
    "# # Keras-based example:\n",
    "# opt = tf.keras.optimizers.Adam()\n",
    "# opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
    "# model.compile(loss=loss, optimizer=opt)\n",
    "# model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RTX_GPU Tensor Cores for faster compute: FOR KERAS API\n",
    "\n",
    "Source:\n",
    "- https://www.tensorflow.org/guide/keras/mixed_precision\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set for MIXED PRECISION:\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "\n",
    "# print('Compute dtype: %s' % policy.compute_dtype)\n",
    "# print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introduction to Encoders-Decoders:\n",
    "\n",
    "The Encoder-Decoder architecture is composed of, like its name suggests, an encoder at one half and a decoder at ther other half. The Encoder is essentially a function that will map the input data into a latent space. This latent space is composed of a structured set of values that is defined by the encoder. The Decoder takes the mapped elements from the latent space and maps them into the predefined target domain. Typically, there are many applications for Ecoders-Decoders, such as communications (transmiters and receivers), electronics, and so on. It largely serves the purpose as an information converter. For the purposes of Machine Learning, these kinds of network were used for text translation, whereby the Encoder network would ingest a source language input (like spanish) and learns to project the data into a latent space and have the language's meaning encoded as a feature vector (a.k.a codes), this is then followed by the Decoder network (trained alongside the Encoder) that would convert these encoded feautre vectors into the target language (like english).\n",
    "\n",
    "Below shows an example of the Ecoder-Decoder Network:\n",
    "\n",
    "<img src=\"Description Images/EncoderDecoderNetwork.png\" width=\"750\">\n",
    "\n",
    "Image Ref -> https://www.pyimagesearch.com/2020/02/24/denoising-autoencoders-with-keras-tensorflow-and-deep-learning/ and https://www.researchgate.net/figure/t-SNE-visualizations-for-clustering-on-MNIST-dataset-in-a-Original-pixel-space-b_fig1_322674846\n",
    "\n",
    "Here, the Encoder can be seen to be trained on the MNIST Digits dataset, where it converts the 28x28 images into the latent space of vectors (codes) of 32 values. The Decoder is then trained to recover the images from the latent space of vectors. By plotting (using the t-SNE method) the codes with the class labels, it can be seen that it shows the similarities or its structure inherent of the dataset. These are the semantic information that was referred to earlier. Encoders serves to extract these semantic information that are inherent within the dataset and then it is decoded (by the Decoder) to decompress the information accordingly.\n",
    "\n",
    "## 2 - Auto-Encoding:\n",
    "\n",
    "These Auto-Encoders are a special kind of Encoder-Decoder, where for such a task that the input and target domains presents to be the same, like images, the Auto-Encoder should properly encode and decode these images without having any impact on the quality. It should perform all of these despite its inherent bottleneck in desgin. Overall, the process begins with the inputs being converted into a compressed representation into the latent space by the Encoder network, where these feature vectors (compressed representations) will be reconstructed by the Decoder network. The distance between the input and output data is usually the loss calculation that would be minimised. The training process for Auto-Encoders are simpler than others largely due to no requirements for ground truth labels, this is because the input images themselves are used as the ground truth. These kinds of model are typically called __Self-Supervised Models__.\n",
    "\n",
    "One good example of this is JPEG tools and that these are also AutoEncoders. Similar to the mentioned function above, these JPEG tools will encode the images firstly then proceeds to decode them while retaining as much quality as possible. For images, the loss computation is the distance as cross-entropy loss or L1 (Manhattan) or L2 (Euclidean) loss between the input and the target images.\n",
    "\n",
    "## 2.1 - Usage of Auto-Encoders:\n",
    "\n",
    "List of Auto-Encoder usage:\n",
    "\n",
    "1) __Depth Regression__: It can be used to estimate the distance between the camera and the image content of interests at a pixel level. These kinds are important operations for applications such as Augmented-Reality, which allows for the construction of 3D representation of the surroundings and its interactions with the environment.\n",
    "\n",
    "2) __Semantic Segmentation__: This is one of the more common use cases, where the model will be trained to return the estimated class for each of the pixels in the iamge. This is a highly important example as it can be used to define objects of interests such as trees, people, vehicles and so on for driverless cars.\n",
    "\n",
    "3) __Artistic Tasks__: These kinds of Encoder-Decoders can be used for transforming images into pseudo-realistic output images, like estimating objects under the sea without the blue colour of the sea water, or estimating the day time representation of a scene with photos taken at night.\n",
    "\n",
    "4) __Generative Tasks__: AutoEncoders can also be structurally configured for generative tasks, where the latent space can be structured in such a way during training that when the feature vector is selected it can be decoded to form a picture. This will lead to more advanced models like Generative Adversarial Networks (GANS).\n",
    "\n",
    "5) __Denoising AEs__: These types of AutoEncoders can transform lossy inputs to return the original versions. Since the input is lossy, the models will then be trained to cancel the lossy operation in order to recover the missing information that will ouptut the original images. The applications for these networks will be for upscaling or super-resolution of images, where it has the added benefits of removing artifacts that are traditionally caused by bilinear interpolation.\n",
    "\n",
    "## 2.2 - Image Denoising Example:\n",
    "\n",
    "This section will demonstrate the ease of constructing an example of Encoder-Decoder network with Keras. The architecture here is a symmetrical form of the Encoders-Decoders with the lower dimensional bottlenecks. Both the inputs and targets are set as \"x_train\". The Sigmoid function is also used in the last layer to output values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    \n",
    "    # Example Code of Encoder-Decoder:\n",
    "    inputs = Input( shape = [img_height * img_width] )\n",
    "    \n",
    "    # Encoder layers:\n",
    "    encoder_1 = Dense(units = 128, activation = 'relu')(inputs)\n",
    "    code = Dense(units = 64, activation = 'reult')(encoder_1)\n",
    "    \n",
    "    # Decoder Layers:\n",
    "    decoder_1 = Dense(units = 64, activation = 'relu')(code)\n",
    "    preds = Dense(units = 128, activation = 'sigmoid')(decoder_1)\n",
    "    \n",
    "    # Model:\n",
    "    autoEncoder = Model(inputs, preds)\n",
    "    \n",
    "    # Training phase: notice, the x_train are both input and targets.\n",
    "    autoEncoder.compile(loss = 'binary_crossentropy')\n",
    "    autoEncoder.fit(x_train, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of denoising an image, the model above can be trained by passsing in a noisy copy of the training images through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dont_run == 1:\n",
    "    # Noisy input images:\n",
    "    x_noisy = x_train + np.random.normal(loc = .0,\n",
    "                                         scale = .5,\n",
    "                                         size = x_train.shape)\n",
    "\n",
    "    # Fit the model:\n",
    "    autoEncoder.fit(x_noisy, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building a Fully Connected Encoder-Decoder:\n",
    "\n",
    "This section will explore the process of building a Fully Connected Encoder-Decoder network that will be applied to the MNIST Digits dataset. \n",
    "\n",
    "### 3.1 - Defining the Hyper-Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "nb_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Load in Dataset: MNIST Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries:\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    version=1.0.0,\n",
      "    description='The MNIST database of handwritten digits.',\n",
      "    urls=['https://storage.googleapis.com/cvdf-datasets/mnist/'],\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    total_num_examples=70000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 60000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the Dataset:\n",
    "\n",
    "# Instantiate the dataset builder:\n",
    "mnist_builder = tfds.builder(\"mnist\")\n",
    "\n",
    "# Download and load in the dataset:\n",
    "mnist_builder.download_and_prepare()\n",
    "\n",
    "# Check out the dataset information to confirm:\n",
    "mnist_info = mnist_builder.info\n",
    "print(mnist_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Dataset Pre-processing:\n",
    "\n",
    "This section will perform transformations to the original downloaded dataset where it will resize the images to the correct dimensions, split and shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required library:\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_data_func(features, target = 'label', flatten = True, return_batch_as_tuple = True, seed = None):\n",
    "    \"\"\" This builds a function that will resize the images to the expected dimensions, \n",
    "        with the added option to apply random transformations. \n",
    "    Parameters:\n",
    "        - features, is the input data (Images, or others).\n",
    "        - target, is the Ground Truth or Target data to be returned with the images. \n",
    "            (set as 'label' for categorical labels, 'images' for images, otherwise it will be 'None')\n",
    "        - flatten, is the Flag to flatten the images into a 1D array (28, 28, 1) to (784, ).\n",
    "        - return_batch_as_tuple, is the Flag to return the batch data as a Tuple rather than Dict.\n",
    "        - seed, is the seed number to be set for reproducibility.\n",
    "    Returns:\n",
    "        - returns the processed dataset.\n",
    "        \n",
    "    NOTE: \n",
    "        - Tensorflow-Dataset returns batches as feature dictionaries, that are expected for Estimators.\n",
    "        - To train for Keras Models, it is better to return the batch data as Tuples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the images:\n",
    "    image = features['image']\n",
    "    \n",
    "    # Convert the images to float type, and scale them from [0, 255] value to [0., 1.] values:\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # If Flag for Flatten is set to True, perform the resizing of the images:\n",
    "    if flatten:\n",
    "        is_batched = len(image.shape) > 3\n",
    "        flattened_shape = (-1, 784) if is_batched else (784, )\n",
    "        image = tf.reshape(image, flattened_shape)\n",
    "        \n",
    "    # \n",
    "    if target is None:\n",
    "        return image if return_batch_as_tuple else {'image': image}\n",
    "    else:\n",
    "        features['image'] = image\n",
    "        return (image, features[target]) if return_batch_as_tuple else features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dataset(phase = 'train', target = 'label', batch_size = 32, nb_epochs = None, shuffle = True, flatten = True, return_batch_as_tuple = True, seed = None):\n",
    "    \"\"\" This builds a function to process and get the dataset. Dataset is from MNIST Digits.\n",
    "     Parameters:\n",
    "         - phase, is the current phase of data processing, either 'train' or 'test'.\n",
    "         - target, is the Ground Truth or Target data to be returned with the images. \n",
    "            (set as 'label' for categorical labels, 'images' for images, otherwise it will be 'None')\n",
    "         - batch_size, is the batch_size.\n",
    "         - nb_epochs, is the number of epochs.\n",
    "         - shuffle, is a FLag to shuffle the dataset (default=True).\n",
    "         - flatten, is the Flag to flatten the images into a 1D array (28, 28, 1) to (784, ).\n",
    "         - return_batch_as_tuple, is a Flag to return the batched data as a tuple rather than a dict.\n",
    "         - seed, is the seed number for random operations, allows for reproducibility.\n",
    "    :return:\n",
    "        - returns an Iterable Dataset.\n",
    "    \"\"\"\n",
    "    #\n",
    "    assert(phase == 'train' or phase == 'test')\n",
    "    \n",
    "    # Instantiate the data preparation function:\n",
    "    prepare_data_func = functools.partial(_prepare_data_func, \n",
    "                                          return_batch_as_tuple = return_batch_as_tuple,\n",
    "                                          target = target,\n",
    "                                          flatten = flatten,\n",
    "                                          seed = seed)\n",
    "    \n",
    "    mnist_data = mnist_builder.as_dataset(split=tfds.Split.TRAIN if phase == 'train' else tfds.Split.TEST)\n",
    "    mnist_data = mnist_data.repeat(nb_epochs)\n",
    "    \n",
    "    # Data Shuffling:\n",
    "    if shuffle:\n",
    "        mnist_data = mnist_data.shuffle(10000, seed = seed)\n",
    "    \n",
    "    # Split the dataset into batched images:\n",
    "    mnist_data = mnist_data.batch(batch_size)\n",
    "    \n",
    "    # Apply the data prep func to the dataset:\n",
    "    mnist_data = mnist_data.map(prepare_data_func, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Set to prefetch the data: for improved performance.\n",
    "    mnist_data = mnist_data.prefetch(1)\n",
    "    \n",
    "    return mnist_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the above defined functions to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# Define the number of classes in this dataset:\n",
    "nb_classes = mnist_info.features['label'].num_classes\n",
    "\n",
    "# Define the number of images in this dataset:\n",
    "nb_train_imgs = mnist_info.splits['train'].num_examples\n",
    "nb_val_imgs = mnist_info.splits['test'].num_examples\n",
    "\n",
    "# Define the trainig and validation steps per epoch:\n",
    "train_steps_per_epoch = math.ceil(nb_train_imgs / batch_size)\n",
    "val_steps_per_epoch = math.ceil(nb_val_imgs / batch_size)\n",
    "\n",
    "# Apply the above functions to obtain the processed dataset:\n",
    "train_mnist_dataset = get_mnist_dataset(phase='train',\n",
    "                                        target='image',\n",
    "                                        batch_size=batch_size,\n",
    "                                        nb_epochs=nb_epochs,\n",
    "                                        shuffle=True,\n",
    "                                        flatten=True,\n",
    "                                        return_batch_as_tuple=True,\n",
    "                                        seed=Seed_nb)\n",
    "\n",
    "val_mnist_dataset = get_mnist_dataset(phase='test',\n",
    "                                      target='image',\n",
    "                                      batch_size=batch_size,\n",
    "                                      nb_epochs=1,\n",
    "                                      shuffle=False,\n",
    "                                      flatten=True,\n",
    "                                      return_batch_as_tuple=True,\n",
    "                                      seed=Seed_nb)\n",
    "\n",
    "# Define the input shape of the images:\n",
    "input_shape = mnist_info.features['image'].shape\n",
    "\n",
    "# Define the input shape when flattened:\n",
    "flattened_input_shape = [np.prod(input_shape)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Building a Simple Auto-Encoder:\n",
    "\n",
    "The model will be composed of two parts/modules:\n",
    "\n",
    "1) Is the __Encoder__ module, where it will convert the input images into codes (or the latent space), these are tensors of lower dimensionality. \\\n",
    "2) Is the __Decode__ module, wher it will take the codes as the input and will try to recover and rebuild the original images.\n",
    "\n",
    "#### Defining the size of the Latent Space or Codes layer:\n",
    "\n",
    "Here the codes layer wwill be defined as a 32-dimensional vectors. Comparing this to the input vectors, MNIST images are 28 x 28 -> 784-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be constructed with __3 Dense Layers for the Encoder__, then in the subsequent layers, the layers will be decreased slowly down to the dimensionality of the __Code Size__. Once reaching the Codes layer, the __Decoder__ layers will be upsampled back to the __Original Dimensions__ once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Libraries:\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "enc_dense1 (Dense)           (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "enc_dense2 (Dense)           (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "enc_dense3 (Dense)           (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dec_dense1 (Dense)           (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dec_dense2 (Dense)           (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dec_dense3 (Dense)           (None, 784)               101136    \n",
      "=================================================================\n",
      "Total params: 222,384\n",
      "Trainable params: 222,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Design the AutoEncoder model:\n",
    "\n",
    "inputs = Input(shape= flattened_input_shape,\n",
    "               name = 'input')\n",
    "\n",
    "# Encoder - encoding layers:\n",
    "encoder_1 = Dense(units=128, activation='relu', name = 'enc_dense1')(inputs)\n",
    "encoder_2 = Dense(units=64, activation='relu', name = 'enc_dense2')(encoder_1)\n",
    "\n",
    "# Code Layer:\n",
    "code = Dense(units=code_size, activation='relu', name = 'enc_dense3')(encoder_2)\n",
    "\n",
    "# Decoder - decoding layers:\n",
    "decoder_1 = Dense(units=64, activation='relu', name = 'dec_dense1')(code)\n",
    "decoder_2 = Dense(units=128, activation='relu', name = 'dec_dense2')(decoder_1)\n",
    "decoded = Dense(units = flattened_input_shape[0], activation='sigmoid', name = 'dec_dense3')(decoder_2)\n",
    "\n",
    "# Instantiate the AutoEncoder Model:\n",
    "autoEncoder = Model(inputs, decoded)\n",
    "autoEncoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above represents the complete model of the Auto-Encoder based on the meantioned that the Encoder and Decoder layers will be trained together. \n",
    "\n",
    "__However__, it is more convenient to split the modules apart, where each would be defined as a separate model with each having their respective layers being wrapped.\n",
    "\n",
    "#### For the Encoder, the layers from the input to the code layer will be wrapped into a model, seen with the following:\n",
    "\n",
    "This Encoder model will take in images as its inputs and return the codes for storing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "enc_dense1 (Dense)           (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "enc_dense2 (Dense)           (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "enc_dense3 (Dense)           (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 110,816\n",
      "Trainable params: 110,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the wrapped encoder model:\n",
    "encoder = Model(inputs, code)\n",
    "\n",
    "# Model summary:\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the Decoder, the model will be composed of the decoding layers of the AutoEncoder defined above, where it would take in the __input codes__ (from the encoder) and then proceeds to return the output images.\n",
    "\n",
    "There is a problem here, that is the AutoEncoder model's decoding layers are linked to the encoding layers, this will be decoupled and it will instead be connected to a __new__ \"Input\". This Input will represent the \"codes\" from the encoder model. To do this, there is a need to detch the decoding layers and then build a new graph based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NEW Input: code layer.\n",
    "input_code = Input(shape = (code_size,), \n",
    "                   name = 'input_code')\n",
    "\n",
    "# Rebuild a New Decoder that is based off the AutoEncoder's decoding layers:\n",
    "\n",
    "# Count the number of decoding layers: Should be 3 in this case.\n",
    "nb_decoder_layer = 0\n",
    "\n",
    "for layer in autoEncoder.layers:\n",
    "    if 'dec_dense' in layer.name:\n",
    "        nb_decoder_layer += 1\n",
    "\n",
    "# Apply each layer to the new data to construct a new graph:\n",
    "dec_i = input_code\n",
    "\n",
    "# Loop to iterate from the 2 to 0:\n",
    "for i in range(nb_decoder_layer, 0, -1):\n",
    "    # Get the decoder layers from the AutoEncoder model, one at a time:\n",
    "    decoder_layer = autoEncoder.layers[-i]\n",
    "    \n",
    "    # Construct the new graph, with same parameters:\n",
    "    dec_i = decoder_layer(dec_i)\n",
    "    \n",
    "# Instantiate the Decoder model based on the newly graphed layers:\n",
    "decoder = Model(input_code, dec_i)\n",
    "\n",
    "# Model summary:\n",
    "decoder.summary()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
